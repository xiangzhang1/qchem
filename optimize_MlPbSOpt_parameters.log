global_load called
parsing cur master.PbS QD.bare qd testing.Q0 Reproduce.Pb108S108.g opt
parsing complete.
parsing cur master.PbS QD.bare qd testing.Q0 Reproduce.Pb68S68.g opt
parsing complete.
parsing cur master.PbS QD.bare qd testing.Q0 Reproduce.Pb63S62.g opt
parsing complete.
parsing cur master.PbS QD.bare qd testing.Q0 Reproduce.Pb55S38.g opt
parsing complete.
parsing cur master.PbS QD.bare qd testing.Q0 Test convergence.Pb55S38.start - regular grid | end - -0_02 | omg i forgot isym0
parsing complete.
parsing cur master.PbS QD.bare qd testing.crunchit.3 opt
parsing complete.
parsing cur master.PbS QD.bare qd testing.crunchit.4 opt
parsing complete.
parsing cur master.PbS QD.bare qd testing.crunchit.5 opt
parsing complete.
parsing cur master.PbS QD.bare qd testing.crunchit.6 opt
parsing complete.
parsing cur master.PbS QD.bare qd testing.crunchit.7 opt
parsing complete.
----------------------------
epoch 0, loss 1.07299
epoch 128, loss 0.650798
epoch 256, loss 0.608904
epoch 384, loss 0.730641
epoch 512, loss 0.622485
epoch 640, loss 0.767349
epoch 768, loss 0.549823
epoch 896, loss 0.624953
epoch 1024, loss 0.619216
epoch 1152, loss 0.888642
epoch 1280, loss 0.704856
epoch 1408, loss 0.697931
epoch 1536, loss 0.594158
epoch 1664, loss 0.755001
epoch 1792, loss 0.732052
epoch 1920, loss 0.697566
epoch 2048, loss 0.774326
epoch 2176, loss 0.637804
epoch 2304, loss 0.531628
epoch 2432, loss 0.673625
epoch 2560, loss 0.58491
epoch 2688, loss 0.840389
epoch 2816, loss 0.571049
epoch 2944, loss 0.617818
epoch 3072, loss 0.712455
epoch 3200, loss 0.57355
epoch 3328, loss 0.556396
epoch 3456, loss 0.551379
epoch 3584, loss 0.611903
epoch 3712, loss 0.567308
epoch 3840, loss 0.479151
epoch 3968, loss 0.699321
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0192074 0.0595496
0.0594374 0.0588075
0.0626106 0.0588129
9.99101e-07 0.00356173
0.0964307 0.0591132
0.0594399 0.0616249
0.0417454 0.000274141
-0.0152556 0.00801138
-0.00553123 0.0519819
-0.0734426 0.00226884
0.0416368 0.00188983
0.0477956 0.0606144
-0.063123 -0.00140367
0.0367961 0.01496
0.0188596 -0.00132732
0.0207377 -2.88702e-05
0.0375434 0.0594104
0.0272988 0.00964438
0.0739227 0.0640638
-0.0161473 0.0566491
0.0191272 0.0524509
0.00863092 0.0546792
0.000834798 0.00924337
0.0964307 0.0581114
-0.0247813 -0.00170444
0.0223972 0.0560879
0.0194912 0.00628706
0.0037846 0.00258038
0.0904958 0.0554842
0.0907409 0.0614017
0.0205116 0.0553499
0.09274 0.05857
0.000835126 0.00939993
-0.0188593 0.0157037
0.042448 0.0584189
0.12945 0.00324745
-0.0494953 0.00753006
0.0335493 0.0569008
0.0599612 0.058749
0.088282 0.0554885
-0.0131855 0.000261218
0.125873 0.060214
0.0860428 0.0114952
0.000796249 0.00939514
0.0385437 -0.000396798
0.132184 0.0687995
-0.0152556 0.00425842
-0.0188599 0.0130058
0.101441 0.0586359
-0.0365701 0.00153964
-0.0462049 0.00566236
-0.027288 0.00707739
0.0318766 6.92949e-05
0.0305923 0.0560752
0.0222152 0.0559302
0.0599029 0.0614686
0.0417489 0.00400489
0.0509553 0.0565857
0.0330953 0.0531988
-4.4075e-07 0.0105267
0.0594396 0.060765
0.118684 0.0626716
0.125873 0.0559915
0.0594397 0.0564492
0.00894166 0.00511464
0.0281391 0.0597387
1.74905e-07 0.00550811
0.0727409 0.0585128
0.0317699 0.0544697
0.0611271 -0.0018057
0.0594396 0.0599958
0.0321586 0.0608171
0.0964766 0.0571219
0.0385437 0.0119838
0.0870049 0.0567461
-0.0118764 0.0612113
-0.0468045 0.00528246
0.0461382 0.0606674
0.0631207 0.00167625
-2.15513e-06 0.014267
0.0134225 0.067694
0.00596587 0.0620951
-0.0132848 0.0681485
-0.129449 -0.000600222
0.0271185 0.00321397
-0.0300731 0.00159993
0.0318695 0.0568573
0.0878299 0.0586078
0.0362234 0.00451891
0.0667826 0.0591611
-0.0023636 0.0595546
0.0168394 0.0546246
0.102034 0.0545938
0.0305923 0.0588632
0.0599029 0.0583134
0.0593133 0.0553352
0.0643397 0.0533233
0.0449751 0.0620701
0.0477957 0.0584396
0.0867333 0.0573403
-0.0468045 0.00377626
0.0327315 0.060859
0.0223968 0.0520079
0.112932 0.0626703
0.0594399 0.0550203
0.0626054 0.0563998
-0.00856301 0.000414287
-0.0272977 0.00664757
0.0496682 0.0609706
0.0594494 0.0674245
-0.0417573 -0.00149637
-0.0248165 0.0090235
0.0594376 0.0574398
0.0191276 0.0565972
0.0362237 0.000715629
0.00891968 0.00358109
-0.0417792 0.000836507
0.0594396 0.0583516
0.046209 0.00573634
-0.0056768 0.0570837
0.0449639 0.00363679
0.0853245 0.0588838
0.104819 0.056958
0.0867067 0.05764
-0.0111732 0.0541957
0.0594395 0.0562604
0.0691072 0.0572372
0.0271163 0.00993566
parameters: [ 9.   1.   2.   1.2  4. ]. error: 1759988050.6.
----------------------------
epoch 0, loss 1.23147
epoch 128, loss 0.633076
epoch 256, loss 0.640263
epoch 384, loss 0.695436
epoch 512, loss 0.801721
epoch 640, loss 0.551839
epoch 768, loss 0.686165
epoch 896, loss 0.772599
epoch 1024, loss 0.873324
epoch 1152, loss 0.73668
epoch 1280, loss 0.639258
epoch 1408, loss 0.723499
epoch 1536, loss 0.555002
epoch 1664, loss 0.762831
epoch 1792, loss 0.697621
epoch 1920, loss 0.704837
epoch 2048, loss 0.640943
epoch 2176, loss 0.553524
epoch 2304, loss 0.594346
epoch 2432, loss 0.652181
epoch 2560, loss 0.502593
epoch 2688, loss 0.643597
epoch 2816, loss 0.659871
epoch 2944, loss 0.644645
epoch 3072, loss 0.61172
epoch 3200, loss 0.634837
epoch 3328, loss 0.679879
epoch 3456, loss 0.607638
epoch 3584, loss 0.620649
epoch 3712, loss 0.697447
epoch 3840, loss 0.699397
epoch 3968, loss 0.738051
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.090741 0.06031
-0.0416401 -0.0108347
-0.0267398 0.00262706
0.0477959 0.0584661
0.05937 0.0545005
0.0871096 0.0574889
-0.0532643 -0.0100786
-0.0611177 -0.00245507
0.0207193 0.0576603
0.0594397 0.0624927
0.0882823 0.0514709
0.0188086 -0.00151455
0.0106903 -0.00762354
0.00894003 0.00783395
0.0582651 0.0536113
0.0464004 0.0510325
0.109566 0.0619866
0.121243 0.0622258
0.0589763 0.0560762
-0.0707068 -0.00472117
0.125873 0.0631455
0.0871096 0.0626318
0.0319947 0.056986
0.0197009 0.0601725
0.0346353 0.000499369
1.13647e-06 0.00670807
0.0927397 0.0507442
-0.00131975 0.00209761
-0.0131858 -0.00827651
-0.0495799 -0.00352372
0.0170933 0.0633664
0.132184 0.0768182
-0.0132848 0.0695659
-0.0662833 -0.00478953
0.0582236 0.0513391
-0.041769 -0.00907392
0.0492512 -0.00742011
0.0920969 0.0590884
0.0870049 0.0547901
0.0867333 0.0523956
0.0271204 -0.00567188
0.0867201 0.061722
-0.0621272 -0.00599424
-2.15513e-06 0.00537643
0.0283783 0.0552645
0.0927397 0.0559943
-0.129453 0.00518767
0.0594396 0.0556649
0.0182077 0.0338972
-4.4075e-07 0.0065692
0.0882823 0.0483714
0.0871096 0.0547227
0.0662934 0.000657056
0.097887 0.0709739
0.0594396 0.0630726
0.00378707 0.00858441
0.0113515 0.0054209
0.0131845 0.0069118
2.9023e-05 0.00293894
0.076422 0.0568466
0.0220539 0.0487466
-0.0106876 -0.00768334
0.0330953 0.0458152
0.0739227 0.067908
0.0621339 -0.00753452
0.129916 0.053434
0.044975 0.0649797
0.0496683 0.0521538
0.000786701 0.00489346
0.00932522 0.0551307
0.0461382 0.0511671
5.31452e-07 0.00650845
-0.0234744 0.000607812
-0.0449623 -0.00805478
0.0174339 0.0506942
0.059435 0.0563089
0.110244 0.0536212
0.0416434 -0.0114554
0.0327313 0.059969
0.0813321 0.0538004
0.0458115 0.0617523
0.0191272 0.0488356
0.0868851 0.066574
0.0267393 0.00531197
3.5013e-07 0.00241568
0.0716781 -0.00939207
0.072741 0.0539525
0.129925 0.052695
0.0882822 0.0596664
0.0394799 0.00940004
0.0589765 0.0545379
-0.0857218 -0.00937436
0.0952318 0.0595662
-0.00628221 0.0433286
0.0492474 -0.000322132
0.110244 0.0467678
0.0477958 0.0552761
0.0321468 0.0484601
0.0477959 0.063648
0.0611271 -0.000438216
0.0189892 -0.00426659
0.0527577 0.0659085
0.0937752 0.0707312
0.0261393 0.0497096
0.0281392 0.05182
0.0527578 0.0528303
0.105503 0.0768006
0.0730674 0.0562225
-0.0860431 0.00420802
-0.0394792 0.00430326
-6.0681e-07 -0.00523372
0.0268143 0.0539144
0.032158 0.0528125
0.0920961 0.0590077
0.0594324 0.0551722
3.40452e-06 -0.00542274
-0.0161572 0.0459501
-0.129449 -0.0082556
0.0373484 0.0566883
-0.0860447 0.00704188
0.0989675 0.0688512
0.0301504 0.0557932
0.0248076 -0.0114641
0.000782366 0.00494205
-0.0194897 -0.0073903
0.0529729 -0.00573809
0.0966596 0.0540174
0.000782366 0.00616838
parameters: [ 9.   1.   2.   1.2  4. ]. error: 165972620.928.
----------------------------
epoch 0, loss 1.26794
epoch 128, loss 0.684928
epoch 256, loss 0.637699
epoch 384, loss 0.800181
epoch 512, loss 0.613238
epoch 640, loss 0.726817
epoch 768, loss 0.800439
epoch 896, loss 0.67049
epoch 1024, loss 0.615797
epoch 1152, loss 0.823435
epoch 1280, loss 0.674129
epoch 1408, loss 0.623664
epoch 1536, loss 0.538361
epoch 1664, loss 0.685856
epoch 1792, loss 0.784716
epoch 1920, loss 0.709589
epoch 2048, loss 0.625418
epoch 2176, loss 0.61019
epoch 2304, loss 0.621514
epoch 2432, loss 0.631791
epoch 2560, loss 0.72073
epoch 2688, loss 0.509969
epoch 2816, loss 0.662291
epoch 2944, loss 0.609711
epoch 3072, loss 0.669247
epoch 3200, loss 0.667258
epoch 3328, loss 0.684881
epoch 3456, loss 0.753335
epoch 3584, loss 0.615089
epoch 3712, loss 0.604304
epoch 3840, loss 0.663793
epoch 3968, loss 0.595386
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.00863032 0.0486355
0.020735 -0.00268138
0.0964305 0.0596909
0.059435 0.0569606
0.0562851 0.0597635
0.0594473 0.0568977
0.0236481 0.0641896
0.0781223 0.0527671
0.0122194 0.05096
0.100115 0.00347466
-0.0204101 0.00527075
0.0907411 0.0653195
-0.0295746 0.0066617
0.00856342 -0.00442656
0.038792 0.0564855
0.028139 0.062143
0.0122452 0.0609855
0.0662801 -0.000445816
0.0952316 0.0597014
0.0385291 0.000608601
0.0222151 0.0611408
0.029557 0.0151943
0.0295687 -0.000760927
0.00932522 0.0527739
-0.0385437 0.00035593
0.102034 0.0535688
-3.40966e-05 0.00722083
0.0496683 0.063211
1.74905e-07 0.00765579
0.0205116 0.0485877
0.0333261 0.0629541
-1.48505e-07 0.00645072
-0.0161572 0.0451105
0.0217933 0.0625209
0.0532557 -0.00295095
0.0594396 0.0626804
0.0562849 0.059878
0.0776855 0.0234344
0.0562851 0.0605132
-0.0367964 0.00886378
0.0871097 0.0672643
0.125873 0.0528946
0.0237987 0.0602813
0.12432 0.0627591
0.0375434 0.0535419
0.0964772 0.0628509
0.0170933 0.0585724
0.124354 0.0579372
0.0861486 0.0594907
0.0710836 0.0617573
-0.036223 0.00289151
-0.000807624 -0.00427851
-0.00331175 0.0523544
0.0495796 -0.00398634
0.092056 0.0532103
0.0327313 0.0633192
-0.0776845 0.00881833
0.0113515 -0.00156394
-0.0394792 0.003196
0.0267369 -0.00149336
0.049499 0.0112786
0.0449618 0.00066365
-0.000835377 -0.00198933
-0.0707068 0.0040492
0.0764328 0.053997
0.0322983 0.0606688
3.40452e-06 0.000139088
-0.053266 0.000491475
0.0734439 -0.00070859
0.0248169 0.0155842
-0.049248 0.00052238
0.0337182 0.0667268
0.0477959 0.0666345
-0.0468045 0.00648693
-2.23337e-05 -0.000304065
0.0373484 0.0614079
0.0310444 0.0553552
0.129925 0.0584309
0.0496682 0.0623579
0.0594399 0.0580438
-0.041769 0.00471477
0.0211183 0.0589502
-0.00731719 0.0475309
0.0867335 0.0633904
0.059181 0.0523908
-0.0113495 -9.26776e-07
-0.0207367 -0.000931918
-0.00131656 -0.00453685
0.028139 0.065564
0.0140549 0.0596036
-0.0056768 0.0535014
-0.0109489 0.0112233
0.000794682 0.00678642
-0.0300731 -0.00293691
0.0964305 0.054493
-1.32922e-07 0.0067056
0.0882822 0.0575643
0.05943 0.0570925
0.0626054 0.0558477
0.0248169 0.000189823
0.0461382 0.0563794
0.0208321 0.000360192
0.0248169 0.00962157
0.0752422 0.0580561
0.0594396 0.0612427
0.062595 0.0597806
0.00378707 -0.00230388
0.0594396 0.065327
0.0593665 0.0512603
0.0322983 0.0643852
0.0997468 0.0630476
0.126171 0.0612856
0.0691984 0.0648847
0.0271289 -0.00145144
0.0661224 0.0584943
0.0594376 0.0581047
0.0594398 0.0645776
0.0384877 0.0618654
-0.0207342 0.00519032
0.0496681 0.059215
-0.0860431 0.00904894
0.0952318 0.0652651
0.032933 0.0588251
-0.0362233 -0.0023803
-0.0189848 0.00589444
0.109549 0.0510003
6.7959e-07 -0.00350244
0.0319943 0.0576625
parameters: [ 10.    1.    2.    1.2   4. ]. error: 4174326024.02.
----------------------------
epoch 0, loss 1.08037
epoch 128, loss 0.770576
epoch 256, loss 0.679591
epoch 384, loss 0.615043
epoch 512, loss 0.485292
epoch 640, loss 0.690169
epoch 768, loss 0.703349
epoch 896, loss 0.601827
epoch 1024, loss 0.690942
epoch 1152, loss 0.672451
epoch 1280, loss 0.592741
epoch 1408, loss 0.800326
epoch 1536, loss 0.667742
epoch 1664, loss 0.596094
epoch 1792, loss 0.756981
epoch 1920, loss 0.667424
epoch 2048, loss 0.550383
epoch 2176, loss 0.716493
epoch 2304, loss 0.663248
epoch 2432, loss 0.474166
epoch 2560, loss 0.597827
epoch 2688, loss 0.497258
epoch 2816, loss 0.536031
epoch 2944, loss 0.565145
epoch 3072, loss 0.800486
epoch 3200, loss 0.615512
epoch 3328, loss 0.653776
epoch 3456, loss 0.625898
epoch 3584, loss 0.74461
epoch 3712, loss 0.603912
epoch 3840, loss 0.613918
epoch 3968, loss 0.58114
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0396776 0.0558945
0.0273016 0.0126161
0.101441 0.0557598
0.0323163 -0.0019347
0.036223 0.00181252
-0.0734361 0.0114124
0.088729 0.0606359
-0.000807624 -0.000638909
-0.0776729 0.00263166
-0.0468001 -0.00031096
-0.013282 0.0689523
-0.0207342 0.00197042
0.0605996 0.055138
0.0327313 0.0565269
-0.00628221 0.0550523
0.0122452 0.0561791
0.130005 0.0549551
0.134985 0.0605745
0.0278231 0.0579736
0.00894166 0.00333899
0.0562648 0.0561058
0.0648671 0.0556869
0.0497672 0.0569598
0.0642209 0.0561116
0.05937 0.0572032
0.033718 0.0566724
0.0910569 0.0608068
-0.0734426 0.00860085
0.0734322 0.00647166
0.0384676 0.0548394
-0.0807993 0.00570945
0.143948 0.06279
0.0991788 0.0622574
0.0211031 0.00039525
0.0283785 0.0510589
0.0950803 0.0604486
0.0477959 0.0602853
0.0594397 0.060514
0.143948 0.0611322
0.0594562 0.0574044
0.0897923 0.06235
-0.0177194 0.00421958
0.0271163 0.00268082
0.0594398 0.0561673
0.0532535 0.0558587
0.0981549 0.0571511
0.0707072 0.00510716
-0.0248069 -0.000211983
0.0237988 0.0578677
0.106624 0.0594283
0.0318766 0.01159
0.0964772 0.0585028
0.0991789 0.0599968
0.0599029 0.0582466
-0.0390966 0.000620414
0.0541227 0.0581902
-0.0161473 0.060509
0.0237987 0.0608549
0.0267393 0.0112322
0.0859466 0.0576238
-0.0295746 -0.00322426
0.0611271 -0.00270729
2.27454e-05 0.0033764
0.0800821 0.056447
0.0144859 0.00999588
0.0093143 0.054312
0.0594397 0.0594625
0.0813321 0.0597116
0.0191272 0.0570577
-0.0326435 -0.000510219
-0.0529722 0.00158791
-0.0209824 0.0133577
-1.1614e-09 0.00699888
-0.00142113 0.00221126
0.0562851 0.0563447
-0.0247813 0.00307253
0.0730675 0.0548293
0.0417484 0.0078112
-0.0295746 -0.00322426
0.110244 0.0582383
0.028139 0.0606034
-0.0449623 0.00283665
0.0267369 0.00248452
0.061289 0.0693907
0.0594395 0.0555417
0.0661223 0.0578829
0.01684 0.0519075
0.0859466 0.0622799
0.0394802 -0.00127548
-0.0209824 0.00578648
0.0945151 0.0594687
0.0859466 0.0622768
0.0496682 0.0583292
0.0412752 0.00636156
0.059407 0.0571973
-0.0860447 0.00678005
6.39156e-06 0.0036602
-0.049248 0.000860241
0.0477957 0.0570984
0.0191276 0.0556783
0.0945151 0.0549607
-0.0113495 -0.00254618
0.022215 0.0549485
0.0364958 0.0547758
0.0327313 0.0548231
0.0594396 0.0570421
0.0144704 0.00801715
-0.0662877 0.000371081
-0.0776729 -0.00256636
0.0362237 0.0137729
-0.000781954 -0.00110512
0.0927398 0.0575349
0.110244 0.053365
0.0461383 0.0561493
0.0860587 8.18492e-05
0.0966595 0.051246
-0.0106876 0.000673671
0.0650733 0.0580936
0.0286795 0.0536669
0.090741 0.0591413
0.0861488 0.0615807
0.0368959 0.055566
-1.99995e-06 0.00565271
0.106624 0.0597119
-0.0416468 0.00386907
0.059435 0.0689092
0.0394803 0.0067496
0.0267369 -0.000181685
parameters: [ 7.382  1.     2.     1.2    4.   ]. error: 436946.500275.
----------------------------
epoch 0, loss 0.997256
epoch 128, loss 0.682932
epoch 256, loss 0.546419
epoch 384, loss 0.693969
epoch 512, loss 0.587296
epoch 640, loss 0.575259
epoch 768, loss 0.555274
epoch 896, loss 0.598478
epoch 1024, loss 0.657532
epoch 1152, loss 0.656257
epoch 1280, loss 0.650718
epoch 1408, loss 0.743666
epoch 1536, loss 0.690309
epoch 1664, loss 0.662543
epoch 1792, loss 0.741256
epoch 1920, loss 0.602923
epoch 2048, loss 0.633933
epoch 2176, loss 0.530255
epoch 2304, loss 0.661553
epoch 2432, loss 0.481798
epoch 2560, loss 0.654284
epoch 2688, loss 0.662855
epoch 2816, loss 0.569419
epoch 2944, loss 0.608867
epoch 3072, loss 0.706098
epoch 3200, loss 0.674836
epoch 3328, loss 0.63621
epoch 3456, loss 0.670205
epoch 3584, loss 0.724628
epoch 3712, loss 0.705599
epoch 3840, loss 0.80782
epoch 3968, loss 0.539556
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0234754 0.0110913
1.95996e-07 -0.00486693
-0.0390959 -0.0032532
0.0857795 0.0518729
-0.0207316 -0.00646567
-0.0248165 -0.00503444
0.0594398 0.0580182
0.0977799 0.0667656
-4.71368e-07 -0.00288612
1.96444e-05 0.00666649
0.0594397 0.0653801
-2.64683e-07 0.00527691
0.0373475 0.0543068
0.0283783 0.047968
0.0541426 0.0573617
0.076422 0.060106
-0.00737805 0.0459416
0.0271115 -0.00201948
-1.48505e-07 0.00825934
0.0281393 0.0549963
0.0532657 0.0159723
0.0318766 -0.00402775
0.0594396 0.0531134
-0.0113373 -0.00456591
-0.0295559 -0.003809
0.0188597 -0.00201758
0.0477959 0.0606565
-0.00700272 0.0477751
0.121243 0.0610514
-0.0152606 -0.0053388
-0.00236368 0.0375723
0.0594473 0.0556896
0.0527576 0.0580378
-0.0631101 -0.0055802
0.0594424 0.05067
0.032158 0.0484055
0.0122194 0.0423103
0.0194981 -0.0037614
0.0710836 0.0592009
0.0131845 -0.00588275
-0.014471 -0.00410922
-0.0827278 -0.0041381
0.13219 0.0816515
0.0650734 0.0544416
0.0397474 0.0489569
0.0730675 0.0651304
0.0594452 0.0736734
0.0237986 0.0543016
-0.0131848 0.00584528
-0.0390966 -0.00279963
0.0301505 0.0463394
0.112932 0.0669766
0.0396776 0.0400316
0.0113491 0.00531845
0.021525 -0.0055295
-0.0132965 0.0722314
0.0594116 0.0518637
4.07333e-06 0.0135363
0.0317696 0.0626606
0.0857794 0.0502159
0.0199307 0.0600299
0.059435 0.0568048
0.0300757 0.0010591
1.95996e-07 0.00868723
0.0907409 0.0545776
0.0538009 0.0467582
0.0211182 0.0641999
0.0907411 0.0572464
0.101441 0.057997
0.0860364 -0.00441254
-0.02083 -0.00571044
0.043921 0.0471377
0.0365698 -0.0037051
-0.032317 -0.00227302
0.0387924 0.0532878
0.129925 0.0526547
-0.0385427 -0.0061635
0.0859865 0.0668483
0.0251233 0.0679205
0.0487559 0.0553258
0.0188597 0.0143821
0.0468012 -0.00477302
-0.00236343 0.0425039
0.0131845 0.00253044
0.0204028 -0.0057752
0.0134225 0.0711954
0.109566 0.0534832
-0.0152556 -0.00533325
0.0496682 0.0614341
-0.0385433 -0.00392007
0.0853245 0.0478774
0.0800819 0.0592723
0.0594397 0.0609409
0.0562851 0.0506009
0.0706155 0.0601609
0.0394799 0.0112602
0.101441 0.0631645
0.0234704 0.00680761
-6.15975e-07 -0.00400869
0.0734394 0.0061408
0.0592247 0.0553114
0.093775 0.0674444
-0.0188062 -0.00428215
0.0631163 -0.00736931
0.0861487 0.0541405
0.0594397 0.0560251
0.0330542 0.0520398
-0.129445 -0.00586555
-0.129453 0.00416463
0.0131848 0.00919963
0.0527576 0.0535681
0.0781227 0.0480687
0.0267781 0.0384093
-0.0385437 0.0138027
0.0283785 0.0424791
0.0318693 0.040463
0.0261393 0.0543613
2.9023e-05 -0.00207573
0.0373475 0.0538454
0.0538009 0.038803
0.0236481 0.0651384
0.129445 0.00847096
0.0385434 -0.00301999
0.0594397 0.0583543
-0.0300731 -0.00356228
0.090741 0.0623911
-0.0716682 -0.00394593
0.0482638 0.0599032
parameters: [ 8.157  1.     2.     1.2    4.   ]. error: 4136048289.88.
----------------------------
epoch 0, loss 1.07981
epoch 128, loss 0.792396
epoch 256, loss 0.791764
epoch 384, loss 0.694497
epoch 512, loss 0.685724
epoch 640, loss 0.824363
epoch 768, loss 0.601246
epoch 896, loss 0.716884
epoch 1024, loss 0.594454
epoch 1152, loss 0.598004
epoch 1280, loss 0.718924
epoch 1408, loss 0.657067
epoch 1536, loss 0.673542
epoch 1664, loss 0.619475
epoch 1792, loss 0.534296
epoch 1920, loss 0.612494
epoch 2048, loss 0.565805
epoch 2176, loss 0.544522
epoch 2304, loss 0.62601
epoch 2432, loss 0.63011
epoch 2560, loss 0.646671
epoch 2688, loss 0.72298
epoch 2816, loss 0.530226
epoch 2944, loss 0.635623
epoch 3072, loss 0.618319
epoch 3200, loss 0.553384
epoch 3328, loss 0.601971
epoch 3456, loss 0.562957
epoch 3584, loss 0.623809
epoch 3712, loss 0.599939
epoch 3840, loss 0.569166
epoch 3968, loss 0.655367
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0997462 0.0453552
0.101814 0.0702042
0.0295774 -0.00770807
0.0749536 0.0566626
0.0462156 0.0045892
0.0390991 -0.00719818
0.0329332 0.0525118
0.0527578 0.0441268
0.129453 0.00528615
0.0396778 0.0432792
0.0859865 0.0685859
0.0593758 0.0541087
0.059435 0.0568633
0.0594397 0.0460674
0.0589133 0.0466836
0.0800821 0.0560136
0.0384777 0.0580803
0.0594397 0.0608045
0.0920613 0.0583463
0.0223968 0.0508312
0.0529729 0.00790406
0.0405236 0.0537349
0.0385437 -0.0081704
-0.00378521 -0.00794202
0.0174339 0.0359568
-0.0188023 -0.00743418
0.125863 0.0609179
-0.0662833 -0.00189223
0.0174335 0.0402916
0.0494968 0.00379559
0.0131855 0.00276623
0.0791961 0.0404179
0.0867199 0.0414138
-0.0188023 -0.00788745
0.0706155 0.0506637
-0.0621346 -0.00183126
0.0981549 0.0580725
0.0482639 0.0484079
-0.10012 -0.00859872
0.0497673 0.0461578
0.0871096 0.0549798
0.0170834 0.0657597
0.0861486 0.0592649
-0.0271157 -0.00737754
-0.0144713 0.0090288
-0.0272977 -0.0071282
-0.0204028 -0.00876053
0.0458117 0.0453886
0.0691984 0.044609
0.132184 0.0778184
-0.0776729 -0.00229874
0.0791268 0.0498937
0.0283785 0.0501791
0.0251234 0.0717666
0.0626106 0.0513297
0.112932 0.0710214
0.0093143 0.0551686
0.0416368 -0.00799652
0.0710838 0.0610588
0.0594452 0.0808245
0.0267419 -0.00788749
0.0594301 0.0504831
0.0482639 0.0526701
-0.129457 0.00177725
0.0281392 0.040772
-0.0189848 0.002975
2.96104e-08 0.0077796
0.0199307 0.0596973
0.0599031 0.0394283
-0.0144836 -0.0065834
-0.0208232 -0.00819237
0.0424369 0.0619158
0.000835111 -0.00775308
0.105506 0.0797358
0.0217933 0.070488
0.000807032 0.0011413
0.13219 0.0794621
0.01684 0.0369901
0.0706156 0.0518235
0.0691068 0.0469565
0.0396778 0.0384854
0.0189892 -0.00675023
0.0321466 0.0366459
0.032933 0.0377948
0.0277564 0.0557429
0.0648671 0.056394
-0.00484737 0.0505916
0.0594396 0.058594
0.110244 0.0540299
0.0907408 0.0624252
0.0267369 0.000919537
0.0964772 0.0457603
0.0605996 0.0568353
0.0661223 0.0641297
2.9023e-05 -0.00580047
0.0211031 0.00355157
0.0861486 0.0435481
0.0870049 0.044569
0.0752422 0.0554956
0.0248076 0.000363209
0.130005 0.0594814
-0.00856301 0.00306924
0.0295774 -0.00770807
-0.0707068 -0.00807576
0.0317699 0.0381977
0.0753057 0.0614026
0.124354 0.0606326
0.0251231 0.0715831
0.0964772 0.0424689
0.043921 0.053923
-0.0394796 -0.0056196
0.00596587 0.0635515
0.0691986 0.0473213
-0.0188605 0.00164301
0.100668 0.0460765
-1.48505e-07 0.00525631
-0.0152606 0.00559488
0.0417629 -0.00858399
0.0492512 -0.00738935
0.0562851 0.0532316
0.0424268 0.0545
-0.0131858 0.0020627
-0.00236368 0.0615901
-0.0412711 -0.00674441
0.059367 0.0555784
0.104819 0.0509641
-0.036223 -0.00713245
9.99101e-07 -0.00762246
parameters: [ 9.   1.   2.   1.2  4. ]. error: 353819726627.0.
----------------------------
epoch 0, loss 1.28848
epoch 128, loss 0.872866
epoch 256, loss 0.730542
epoch 384, loss 0.741439
epoch 512, loss 0.761983
epoch 640, loss 0.684721
epoch 768, loss 0.517231
epoch 896, loss 0.681879
epoch 1024, loss 0.72653
epoch 1152, loss 0.819543
epoch 1280, loss 0.481638
epoch 1408, loss 0.789731
epoch 1536, loss 0.687929
epoch 1664, loss 0.547805
epoch 1792, loss 0.798734
epoch 1920, loss 0.513603
epoch 2048, loss 0.522753
epoch 2176, loss 0.590913
epoch 2304, loss 0.607401
epoch 2432, loss 0.55168
epoch 2560, loss 0.513481
epoch 2688, loss 0.581718
epoch 2816, loss 0.539842
epoch 2944, loss 0.58639
epoch 3072, loss 0.614084
epoch 3200, loss 0.569714
epoch 3328, loss 0.652965
epoch 3456, loss 0.606261
epoch 3584, loss 0.67363
epoch 3712, loss 0.456583
epoch 3840, loss 0.713479
epoch 3968, loss 0.649643
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0606162 0.0506619
0.0131849 -0.000705927
-0.016115 0.0358392
0.0364958 0.0502573
-0.0385421 0.00596228
-0.0362233 0.0081888
0.0592543 0.0543942
0.081797 0.0604922
0.0199307 0.055582
0.0192074 0.0599849
-0.00733491 0.04681
0.129993 0.0654274
0.0541227 0.0550986
-0.00142113 0.0109565
0.0901952 0.0610377
0.0857789 0.047865
0.110244 0.062511
0.0859467 0.0652555
-0.0131848 -0.000906232
0.0716711 -0.000948823
-0.0271126 -0.00151233
0.132184 0.0867907
0.0106905 0.00455029
0.0189828 -0.000503152
0.022444 0.0490119
0.0989677 0.0638193
0.0199307 0.0611269
0.102035 0.0525049
-0.000835063 0.0050599
-0.0204122 -0.000570509
-0.0023636 0.0389833
0.134995 0.0614723
-0.053266 0.00531365
0.0189892 -0.000733215
0.126222 0.0564144
0.0599612 0.0456036
-3.6833e-05 0.00854547
0.10664 0.0470363
0.0188086 -0.00171411
0.0861486 0.0680427
0.000213748 0.0570983
0.021525 0.00603113
0.0412725 0.00762671
0.0691984 0.0625971
0.0927398 0.0763564
0.0337178 0.0602702
0.0210915 -0.000446345
-0.0326437 -0.000861819
-6.66928e-06 0.0209381
-0.0462148 0.0136951
0.0211185 0.0603162
-0.0189887 -0.000158276
-1.48505e-07 -0.0008003
0.0321586 0.0556019
0.0387924 0.0459557
0.0803921 0.0605422
4.57767e-08 -0.000358655
0.0710836 0.0583536
0.0868849 0.0637937
0.0594919 0.0453778
0.0710838 0.0592989
0.0247806 0.00868145
-0.0662867 -0.00194236
9.99101e-07 -0.000193983
0.0333261 0.062324
0.036223 -0.000587518
0.0730675 0.0616931
0.0878299 0.0570614
0.0662803 -0.000969417
0.106655 0.0578801
0.0209719 0.00631862
-0.0827278 0.012713
0.0236481 0.0444684
0.0592247 0.0542362
-0.0532643 0.00671215
0.05943 0.0474309
0.0217931 0.0515057
-0.0247793 0.00422648
3.40452e-06 0.00566683
0.0860587 -0.000714659
0.0867067 0.0611348
0.129916 0.0596272
0.0508582 0.0571768
-0.0131841 0.0187981
-0.0631232 -0.00156677
0.0330953 0.0524143
0.0977799 0.0644355
0.0594116 0.0486647
0.0333261 0.0573024
0.01134 0.00584039
0.130757 0.0615145
0.0037846 -0.000444356
0.0188023 0.00731861
0.0322983 0.0574459
0.0727409 0.0543057
-0.036223 -0.000283095
-0.0207316 -0.00133017
0.0281391 0.0477291
-0.0211014 0.00994056
0.0278231 0.0645301
0.085161 0.0559501
0.0495796 -0.00119695
0.0492512 -0.00116471
-0.00628321 0.0271735
0.0624794 0.0625002
-0.0318771 -0.000657897
0.126222 0.0610557
0.0710835 0.0572844
0.0532535 0.0590804
0.0594397 0.0549065
0.0286795 0.0481594
0.0301504 0.0538107
0.0191272 0.0327694
0.0412776 0.0137388
0.0983621 0.0476984
0.0220538 0.0566426
-0.0860447 0.00668976
0.112932 0.0751276
-0.0248165 0.000393798
0.000807032 -0.000471561
0.0589765 0.0653191
0.0177216 0.00533561
0.0217933 0.0694528
0.0237988 0.0442606
0.0364958 0.0489563
0.0631196 -0.000932495
1.13647e-06 -0.00104329
0.0964772 0.0592559
parameters: [ 9.382  1.     2.     1.2    4.   ]. error: 21321392.3008.
----------------------------
epoch 0, loss 1.18509
epoch 128, loss 0.74846
epoch 256, loss 0.782671
epoch 384, loss 0.665654
epoch 512, loss 0.665214
epoch 640, loss 0.705261
epoch 768, loss 0.599664
epoch 896, loss 0.672084
epoch 1024, loss 0.802508
epoch 1152, loss 0.510146
epoch 1280, loss 0.711028
epoch 1408, loss 0.585809
epoch 1536, loss 0.680421
epoch 1664, loss 0.533659
epoch 1792, loss 0.578685
epoch 1920, loss 0.721491
epoch 2048, loss 0.754918
epoch 2176, loss 0.653725
epoch 2304, loss 0.583445
epoch 2432, loss 0.711217
epoch 2560, loss 0.7606
epoch 2688, loss 0.704453
epoch 2816, loss 0.748839
epoch 2944, loss 0.80555
epoch 3072, loss 0.600229
epoch 3200, loss 0.736141
epoch 3328, loss 0.663294
epoch 3456, loss 0.5109
epoch 3584, loss 0.583153
epoch 3712, loss 0.629985
epoch 3840, loss 0.556457
epoch 3968, loss 0.648063
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0122452 0.0584968
0.0385437 0.00717285
0.0594116 0.0536598
-0.0177194 -0.00743579
-0.014471 0.00323964
-0.0109842 0.00227843
0.0497674 0.0534873
4.57767e-08 -0.00788474
-0.0611267 -0.00160843
0.0268142 0.0473834
-0.00249422 -0.0094964
0.106634 0.0614532
0.0791969 0.0596488
-0.0131845 -0.00620333
0.0594399 0.0564094
0.0950802 0.053833
0.0691986 0.0523337
0.0037846 0.00539762
0.100097 0.0621622
-0.0414059 0.00605527
-0.0144856 0.0113942
0.0857794 0.0498469
-0.0117783 -0.00871434
0.0424369 0.0526803
0.0989675 0.0672046
0.0989678 0.0699496
0.0273016 -0.00887847
0.0626002 0.0614803
-0.0109774 -0.00802689
0.0477957 0.0474346
0.0174335 0.0390181
0.0910567 0.0615197
-0.0207316 -0.0070799
0.101441 0.0522826
0.0477957 0.0491705
0.0495801 -0.00927367
0.0461383 0.0499258
0.0764481 0.0565865
0.0327314 0.0592135
-2.64683e-07 -0.00778147
0.0724734 0.0484655
0.0593312 0.0570511
0.0594397 0.0631133
0.0897923 0.0590507
0.0594467 0.0748245
0.0594398 0.0617782
0.0131849 -0.00681242
0.0209719 -0.00756881
0.0859465 0.0567044
0.0621302 -0.00124032
0.0412702 0.00535872
-0.0611272 -0.00943882
-0.0621274 -0.00981379
0.09274 0.0558617
0.0267779 0.0548857
0.0594397 0.0553735
0.0300726 -0.00936671
0.0194912 -0.00862049
0.0739226 0.0697389
0.000802697 -0.00682356
0.129925 0.0610412
0.0495801 -0.00975154
0.0859465 0.0579496
-0.0152583 -0.00809122
-0.0807974 -0.00820649
-0.0621274 -0.0035233
-0.000835377 0.00937926
-0.041769 -0.00845731
0.0764328 0.0527901
0.0594396 0.0580566
-0.000781954 0.00865184
0.109566 0.063984
4.07609e-05 -0.00694961
0.0827282 0.0184433
-0.000807624 0.00824591
-0.0188599 -0.00715471
-0.0532547 0.00337799
-0.00726943 0.048997
0.0496682 0.0559771
1.74905e-07 -0.00736166
-0.000781954 0.0060263
0.0301505 0.0584508
0.0625951 0.06255
0.0897923 0.054108
0.000834782 -0.00733264
-0.0152583 0.00426813
0.0594494 0.0737978
0.00894003 0.00646438
0.061289 0.0713135
-0.0417488 -0.00155429
0.0330948 0.0492462
0.0375382 0.0499558
0.0594162 0.0552221
0.076417 0.053309
0.121243 0.0680648
0.0346447 -0.00121106
0.0327313 0.0526304
0.0335493 0.0475711
0.046209 0.00250481
-0.0132965 0.066652
0.0283783 0.0543335
0.118684 0.0702234
-0.013282 0.0671037
0.0599031 0.0604692
0.0941706 0.061182
0.0385431 -0.00756618
0.0329021 0.0597558
-0.0161473 0.0461013
-0.0529722 -0.0080927
0.10664 0.059664
0.0529709 0.00825999
0.0290868 0.0480452
0.0594396 0.0523559
-0.0271278 0.00854678
0.00378707 -0.00718076
0.126171 0.0604251
-0.0340329 0.00437642
0.021525 -0.0079461
0.0867332 0.0596893
0.102034 0.0612202
-0.000805986 -0.00789722
-0.032317 -0.00264791
0.0897923 0.0604381
0.0538297 0.0515036
0.126127 0.0610327
0.0781227 0.0590554
0.0950804 0.0570775
-0.0631101 -0.00287479
parameters: [ 9.618  1.     2.     1.2    4.   ]. error: 1481965870.42.
----------------------------
epoch 0, loss 1.3359
epoch 128, loss 0.662519
epoch 256, loss 0.92828
epoch 384, loss 0.854233
epoch 512, loss 0.600347
epoch 640, loss 0.640229
epoch 768, loss 0.529783
epoch 896, loss 0.841604
epoch 1024, loss 0.693234
epoch 1152, loss 0.674491
epoch 1280, loss 0.53036
epoch 1408, loss 0.59184
epoch 1536, loss 0.500783
epoch 1664, loss 0.678382
epoch 1792, loss 0.474297
epoch 1920, loss 0.53856
epoch 2048, loss 0.680385
epoch 2176, loss 0.65719
epoch 2304, loss 0.74847
epoch 2432, loss 0.693044
epoch 2560, loss 0.64189
epoch 2688, loss 0.58583
epoch 2816, loss 0.637275
epoch 2944, loss 0.744446
epoch 3072, loss 0.545911
epoch 3200, loss 0.52009
epoch 3328, loss 0.636569
epoch 3456, loss 0.535675
epoch 3584, loss 0.555968
epoch 3712, loss 0.731264
epoch 3840, loss 0.612338
epoch 3968, loss 0.727554
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0496682 0.0632166
-0.00250418 0.00556536
0.0870049 0.0598393
0.0346452 -0.000167239
0.0305919 0.0589724
0.0272988 -0.000340133
0.0234704 0.0174221
0.0207304 0.0104778
0.032195 0.0602716
0.0199306 0.063834
0.0373475 0.0579311
0.0462156 0.00808323
0.0329331 0.0598754
-0.0417573 0.00815475
-0.0462076 0.00458963
0.102035 0.058909
-0.0113495 0.00141752
0.0131851 0.00184898
0.000213351 0.0648346
-0.036223 0.00247329
-0.0247793 0.0110833
1.74905e-07 0.00487753
0.0866799 0.0616282
-0.0023637 0.058983
0.0449751 0.0650233
0.056285 0.0622297
-0.0144843 0.00166929
0.092056 0.0613335
0.0144859 0.0179208
-0.000835048 0.00412627
0.0920969 0.0582887
-0.0340329 0.000284557
0.109566 0.0605693
-0.0532547 0.015792
0.0901949 0.0581689
0.0859865 0.0646498
-0.00726943 0.0546698
-0.129453 4.60268e-05
0.0582589 0.0573221
0.0562851 0.0621745
0.0853245 0.0602471
0.0199307 0.0644882
0.0594204 0.0583576
-0.0417792 0.00815588
0.0882823 0.0583448
0.0207193 0.059175
-0.0152583 0.0040723
0.043628 0.0605692
0.0706156 0.0630414
-0.00378521 0.0102262
0.0851613 0.0634254
0.0599031 0.062739
0.0983621 0.0613511
-0.0529659 0.0044397
0.0272988 -0.000340133
-0.0210969 0.00323776
0.104819 0.0608273
0.0966595 0.0592959
0.0594398 0.0636258
0.0234754 0.00389895
-0.0414014 0.000270256
0.0865811 0.0625312
0.0562849 0.0605686
0.0248097 0.0149409
-0.0449623 0.00950062
0.0910569 0.0633815
0.0677309 0.0564115
0.0594562 0.0575261
0.0997468 0.0610996
0.0261393 0.0603837
0.094504 0.0577795
0.0529666 0.00549879
0.0626054 0.0619134
-0.00894125 0.00891243
0.0477959 0.0613264
-0.0295746 0.0106508
0.126169 0.0575213
-0.0734426 0.00030461
0.0643397 0.0599683
5.31452e-07 0.00275803
0.0144859 0.00673089
-0.014471 0.00594577
0.0868851 0.0616998
0.0871095 0.0608734
0.0497674 0.0596741
-0.0414014 0.00127936
0.0716711 0.00359183
-0.0318748 0.0116773
-0.00553123 0.0553096
0.059435 0.0600948
0.0321632 0.0604053
0.0611204 0.00550705
-0.0131841 0.00496642
-0.0144856 0.0131283
0.0385441 0.00511685
0.049499 0.00658464
0.0752321 0.0605575
0.0594027 0.055793
0.0286789 0.0578578
0.0317699 0.0636754
-0.0326437 -3.28302e-05
0.020735 0.00306864
0.129458 0.00153962
0.0866747 0.0609577
0.0594397 0.0609249
-0.0272995 0.00165514
0.0710838 0.0624136
0.080392 0.0604299
-0.00701261 0.0610698
0.100097 0.0638508
-0.041769 -0.000546347
-0.00142797 0.00641869
-0.0300728 0.00105804
-0.00249422 0.000624792
0.027823 0.0636629
0.0271204 0.00111852
0.01684 0.0588558
-0.0267352 0.0157277
-0.00728242 0.0553427
0.0661224 0.0629828
0.0326439 -0.000515624
0.0329331 0.059729
0.0897923 0.0616556
0.0538297 0.0579326
0.0390991 -0.000105302
0.0661223 0.0604484
0.01684 0.0582372
0.0317696 0.0597729
parameters: [ 9.498  1.     2.     1.2    4.   ]. error: 1298966918.0.
----------------------------
epoch 0, loss 1.23972
epoch 128, loss 0.982174
epoch 256, loss 0.779561
epoch 384, loss 0.679462
epoch 512, loss 0.73412
epoch 640, loss 0.677195
epoch 768, loss 0.717298
epoch 896, loss 0.60922
epoch 1024, loss 0.602651
epoch 1152, loss 0.54471
epoch 1280, loss 0.51557
epoch 1408, loss 0.668912
epoch 1536, loss 0.600901
epoch 1664, loss 0.585131
epoch 1792, loss 0.565458
epoch 1920, loss 0.502316
epoch 2048, loss 0.62111
epoch 2176, loss 0.671148
epoch 2304, loss 0.792561
epoch 2432, loss 0.636182
epoch 2560, loss 0.63902
epoch 2688, loss 0.582445
epoch 2816, loss 0.606504
epoch 2944, loss 0.493409
epoch 3072, loss 0.738393
epoch 3200, loss 0.622982
epoch 3328, loss 0.628069
epoch 3456, loss 0.656204
epoch 3584, loss 0.558875
epoch 3712, loss 0.61627
epoch 3840, loss 0.762726
epoch 3968, loss 0.771351
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0907408 0.0699931
0.10665 0.0644593
-0.10011 0.00412681
0.0373475 0.0459352
0.0414025 0.013879
0.0716781 0.00376832
0.10664 0.0648737
0.121243 0.070689
-0.00378297 0.00460035
0.0188019 0.0607462
0.0882822 0.0569444
0.0317697 0.0685358
0.0321468 0.0535779
0.10955 0.0550898
0.0562648 0.0567213
0.0330948 0.0470576
0.0208245 -0.00321394
0.0396776 0.045849
0.0168394 0.0547397
0.0131845 -0.00231692
0.059435 0.0525454
0.0594251 0.0486646
0.0621308 -0.00229848
0.0267369 -0.00249068
0.072741 0.0676796
0.0321466 0.064268
0.0676335 0.0518408
0.0322982 0.0648924
-0.0495799 -0.00268339
-0.00628321 0.0574821
0.0208321 0.0230493
0.0911183 0.056631
0.0593346 0.0525495
0.000796249 -0.00257152
0.097887 0.0705517
0.0496682 0.0636903
8.40829e-06 -0.000333934
0.0384776 0.0563842
0.0223968 0.0433364
0.01134 0.00498341
0.0532578 0.018894
-0.0362233 0.008598
0.0710835 0.0644088
-3.11621e-07 -0.00193901
0.0964766 0.0510199
0.0964307 0.0560074
-0.00236343 0.0485966
0.0871095 0.056141
0.0188603 0.0125924
0.0920508 0.0575505
0.0133878 0.0775013
0.0562851 0.070603
0.0477956 0.0633735
-0.0132848 0.0701399
0.110244 0.0594923
-0.0631101 -0.000131722
-0.014471 -0.00149736
0.0648671 0.0588653
0.0170933 0.060628
-0.0207367 0.0104096
0.0191272 0.0388827
-6.15975e-07 -0.00162956
0.125146 0.0656049
0.0966595 0.059981
0.0272894 0.0063996
0.0330953 0.0505663
0.0676335 0.0585664
0.10546 0.0840966
0.0857183 0.00363614
0.0594204 0.0519691
-0.0111692 0.0523578
0.0205116 0.0508525
0.0594395 0.0654838
0.0337182 0.0557266
0.0477958 0.0601557
0.0989678 0.0752356
0.0626054 0.0635186
0.0286795 0.0616063
0.126086 0.0694996
0.0582389 0.0602999
0.0593665 0.0542662
0.0211183 0.0697754
0.060558 0.061261
0.0321468 0.0560446
0.0589764 0.0732027
-0.0144843 0.00603557
0.129453 0.0458655
0.0950805 0.0523861
0.0807983 -0.00127367
0.0594399 0.0616373
-0.0267398 -0.00173232
0.0691985 0.0575561
-0.0204101 0.00693098
0.0867119 0.0607584
0.0177216 -0.00132839
0.0122194 0.0585334
-3.91322e-05 0.0125038
0.0335492 0.050373
0.025123 0.0581795
0.0860587 0.0184818
-0.000807624 0.0224841
0.0594398 0.0665143
0.0209719 0.00709119
0.109566 0.0743337
-0.0492483 -0.00360873
0.0783558 0.0625562
0.059435 0.0491322
0.0662803 0.00335609
-0.0529659 0.000590694
-1.92327e-05 -0.00126863
0.125883 0.0687161
0.0594398 0.07063
0.0871096 0.0740779
0.0122295 0.0640694
-0.0495796 0.00144065
0.0131849 -0.00198817
0.0122352 0.0541236
0.00891968 0.0212691
0.0594398 0.0587531
0.0625949 0.0709474
0.0286795 0.0592446
0.0243594 0.0516052
-0.0113495 -0.00300784
0.0594273 0.0616997
0.0882822 0.0618988
-0.0412731 -0.00249443
0.0710835 0.0654301
0.0977799 0.073906
parameters: [ 9.236  1.     2.     1.2    4.   ]. error: 30591033.4818.
----------------------------
epoch 0, loss 1.53297
epoch 128, loss 0.827077
epoch 256, loss 0.79195
epoch 384, loss 0.696947
epoch 512, loss 0.550386
epoch 640, loss 0.655761
epoch 768, loss 0.773028
epoch 896, loss 0.819626
epoch 1024, loss 0.767529
epoch 1152, loss 0.556557
epoch 1280, loss 0.479531
epoch 1408, loss 0.646824
epoch 1536, loss 0.65308
epoch 1664, loss 0.609422
epoch 1792, loss 0.575888
epoch 1920, loss 0.568336
epoch 2048, loss 0.681783
epoch 2176, loss 0.648299
epoch 2304, loss 0.564516
epoch 2432, loss 0.631544
epoch 2560, loss 0.492503
epoch 2688, loss 0.830411
epoch 2816, loss 0.530927
epoch 2944, loss 0.586006
epoch 3072, loss 0.658208
epoch 3200, loss 0.717607
epoch 3328, loss 0.836711
epoch 3456, loss 0.698831
epoch 3584, loss 0.601544
epoch 3712, loss 0.790922
epoch 3840, loss 0.597432
epoch 3968, loss 0.62905
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.028139 0.0621451
0.0706156 0.0627015
0.000802697 0.0117702
2.05834e-07 0.0123592
-0.0131841 0.0166522
0.0461385 0.0627772
0.0144853 0.0168605
0.0301504 0.0611738
0.0223968 0.0568933
-0.0152533 -0.00105981
0.0106905 0.00790549
-0.0118765 0.0623585
-0.0248092 0.000390676
0.0337179 0.0620926
0.0594398 0.0613134
0.0927399 0.0624796
0.102034 0.0592467
-0.000835377 0.0152487
0.0362233 0.0177289
0.125873 0.0625101
0.0710838 0.060548
-0.0495737 -0.00215919
3.36628e-05 -0.000673934
0.118684 0.0661164
0.0495796 -0.00153787
0.0449751 0.0646062
0.0375382 0.0588999
0.0710837 0.0637199
0.0594397 0.06263
0.0122295 0.0600224
0.0435741 0.0576996
0.049499 0.000680388
0.0220538 0.0617266
0.0968257 0.0655783
0.121243 0.0612956
0.0322983 0.0627469
0.0897923 0.0618437
0.0562597 0.061014
0.0945153 0.0561011
0.0594397 0.0628443
0.0707062 0.0172115
-0.0662877 -0.00363702
0.0606162 0.0558486
-0.0132965 0.0664541
-0.0117783 0.00652097
0.0594301 0.0602539
-0.0132904 0.0659592
0.0710835 0.0605391
0.0278229 0.0603623
0.0211182 0.0637068
0.0407513 0.0579644
-0.0394796 -0.00172425
0.0319944 0.0600503
-0.0394796 -0.000693235
0.0562849 0.0616563
0.0867199 0.0650574
-0.0413988 0.0105999
-0.000801651 0.000105631
-2.86114e-05 0.0168697
0.038792 0.0570767
0.0333262 0.0645442
0.0594325 0.0603299
0.0321466 0.0632885
-0.00701261 0.0586638
-0.0118764 0.0621481
0.121243 0.0620866
-0.0215356 0.0139887
0.0971043 0.0659816
0.130756 0.0640107
-0.0144713 -0.000403985
-0.0207316 0.0160153
0.0188015 0.0622677
0.0286795 0.0594362
-0.0132904 0.0669683
-0.0494977 0.014329
0.0346447 -0.00113719
0.121243 0.0628006
-0.000835048 0.0167541
-0.0144856 0.00150757
-0.000805986 0.00109881
0.09274 0.0641783
0.0188023 -0.00171745
0.00863029 0.0564672
0.094504 0.0602963
0.0461382 0.0619225
-0.0318748 0.0111552
0.0131845 0.0016049
0.046209 0.0157577
-0.0271111 0.0120952
0.0492474 0.0106123
0.0859465 0.0623718
0.0604514 0.0526295
3.82209e-05 -0.000721227
0.101441 0.0607163
-0.00726943 0.0548032
0.0224432 0.060314
0.074953 0.0612232
0.014471 0.0192522
0.0853255 0.058803
0.0329331 0.062178
0.0271187 -0.00192354
0.0248076 0.0158238
0.0691986 0.0645714
0.0764328 0.0593159
0.0346447 0.00839717
0.0870053 0.0592731
0.0986862 0.0575629
0.0385441 0.0205534
-2.99285e-06 0.0135823
0.0595095 0.0548788
0.076422 0.05773
0.0417484 -4.94114e-05
0.0412752 0.0168879
-0.0295823 0.0118661
0.0340225 -0.00115808
0.0317698 0.0611579
0.0867202 0.0644998
0.0594376 0.0632546
0.0857794 0.0578944
0.0532535 0.0532457
0.05937 0.0559272
0.0878299 0.0597761
0.0920508 0.0582304
0.0868848 0.0621066
-0.080798 0.0141387
0.101814 0.0642935
-0.0023637 0.0594795
0.0978866 0.0643647
parameters: [ 9.31  1.    2.    1.2   4.  ]. error: 8834489918.1.
----------------------------
epoch 0, loss 0.676415
epoch 128, loss 0.910548
epoch 256, loss 0.936965
epoch 384, loss 0.710742
epoch 512, loss 0.677364
epoch 640, loss 0.694086
epoch 768, loss 0.753363
epoch 896, loss 0.638879
epoch 1024, loss 0.551605
epoch 1152, loss 0.766142
epoch 1280, loss 0.777306
epoch 1408, loss 0.637137
epoch 1536, loss 0.607814
epoch 1664, loss 0.622359
epoch 1792, loss 0.41732
epoch 1920, loss 0.732075
epoch 2048, loss 0.56089
epoch 2176, loss 0.63834
epoch 2304, loss 0.787722
epoch 2432, loss 0.697709
epoch 2560, loss 0.685857
epoch 2688, loss 0.554838
epoch 2816, loss 0.691934
epoch 2944, loss 0.692694
epoch 3072, loss 0.648688
epoch 3200, loss 0.579729
epoch 3328, loss 0.588143
epoch 3456, loss 0.606301
epoch 3584, loss 0.592417
epoch 3712, loss 0.642359
epoch 3840, loss 0.596406
epoch 3968, loss 0.571323
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.133575 0.062924
0.0673482 0.059842
0.110244 0.0600215
0.0122243 0.0562807
-0.0412711 -0.00260894
0.022444 0.0541343
0.0496683 0.060651
0.0122295 0.0517517
-0.0132904 0.0684147
0.0991789 0.0605608
0.0396776 0.0510587
0.0321467 0.0589265
0.0290868 0.0584387
0.023648 0.0639318
0.032933 0.0593786
-0.0131851 0.00872451
-0.000792737 0.00944781
0.0594397 0.0621544
0.109566 0.061852
0.05627 0.0565135
0.0594426 0.0541218
0.0904958 0.0577713
0.0261392 0.0620705
0.00596587 0.0615694
0.0317697 0.0587566
0.0384777 0.0523025
0.000835126 0.0115552
-0.0365701 0.0104257
0.0204143 -0.00180634
0.0594449 0.0594766
0.0659505 -0.00198027
-0.0207342 -0.00258198
0.0396776 0.0541349
0.0707062 0.0123106
0.0691985 0.0588622
0.0323166 -0.00262026
0.0144707 -0.00118956
-0.0131858 -0.000998615
0.0855721 0.0627012
0.0650734 0.058794
0.109566 0.0579637
0.0820225 0.0601615
0.0278228 0.0599821
0.0861486 0.0609095
0.0594466 0.0670264
0.0131845 0.0105329
-0.00893962 -0.00180575
0.0859467 0.0585885
0.0194912 0.00683307
0.0122194 0.0552913
0.0920969 0.0581411
0.0594376 0.0559222
0.027823 0.0561287
0.0594397 0.0625053
-0.0346424 -0.00207527
0.0396776 0.0568542
0.118684 0.0639765
0.0964766 0.0591823
0.0562851 0.055608
0.0851609 0.058883
0.0659615 0.0119808
0.135027 0.0584157
0.0626106 0.0541797
-0.000835392 0.00933562
0.0594204 0.0552532
0.0983621 0.0570403
0.0281391 0.0598945
0.0594426 0.0574757
-0.10011 0.0043273
0.0739226 0.0610691
0.0631207 -0.00380304
-1.99995e-06 0.00888333
0.0482638 0.0609983
0.0871098 0.0573791
0.0532677 -0.00301919
0.0920613 0.0561111
0.104819 0.0593055
-0.0776729 -0.00379374
-0.0131854 -0.0013263
0.100116 -0.00235035
0.0210212 0.0606051
-0.0109566 -0.00248472
0.0989678 0.0628243
-0.0152606 0.00698712
0.0390889 0.00301674
-0.0621343 -0.00355159
2.5668e-06 0.0109483
3.5013e-07 -0.000656209
-0.0248069 0.0108637
0.0322983 0.0581754
0.0734439 0.0116546
0.0337178 0.0623044
0.0594424 0.0579617
0.0435741 0.0534709
-0.034636 -0.00302594
0.0716781 -0.00297519
0.0237989 0.0557404
0.0375434 0.0566697
0.0290868 0.0606359
0.0589763 0.0575081
0.059435 0.0577938
0.0375434 0.0551726
-0.0857226 -0.00283387
0.0210212 0.0598967
0.0327312 0.0608092
0.0937752 0.0645669
0.0305923 0.0576054
-0.0271193 -0.00145874
0.10546 0.0681448
0.100097 0.0649563
0.0904958 0.0568516
0.0594426 0.0525676
-0.000835048 -0.000975452
0.0317697 0.0595862
0.0813321 0.0585954
0.126206 0.060396
0.0467944 -0.00208224
0.0631173 0.00452858
-0.0118767 0.0628334
0.0321468 0.0584129
0.00459996 -0.00355655
0.0385434 0.011185
0.0243585 0.0530935
0.027823 0.0596417
0.0724741 0.0587011
0.080402 0.0534035
0.0691985 0.063553
0.0945151 0.0560637
parameters: [ 9.426  1.     2.     1.2    4.   ]. error: 11881925.0465.
----------------------------
epoch 0, loss 1.13885
epoch 128, loss 0.72792
epoch 256, loss 0.781396
epoch 384, loss 0.769864
epoch 512, loss 0.714581
epoch 640, loss 0.66631
epoch 768, loss 0.707199
epoch 896, loss 0.629301
epoch 1024, loss 0.581302
epoch 1152, loss 0.595237
epoch 1280, loss 0.65468
epoch 1408, loss 0.765031
epoch 1536, loss 0.78484
epoch 1664, loss 0.757607
epoch 1792, loss 0.493123
epoch 1920, loss 0.671469
epoch 2048, loss 0.560735
epoch 2176, loss 0.654918
epoch 2304, loss 0.66805
epoch 2432, loss 0.539102
epoch 2560, loss 0.647338
epoch 2688, loss 0.540852
epoch 2816, loss 0.683148
epoch 2944, loss 0.648117
epoch 3072, loss 0.752937
epoch 3200, loss 0.534916
epoch 3328, loss 0.347617
epoch 3456, loss 0.642174
epoch 3584, loss 0.47682
epoch 3712, loss 0.487468
epoch 3840, loss 0.707778
epoch 3968, loss 0.628225
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0776735 -0.00698573
0.0223968 0.041453
0.0764328 0.0548013
0.0730674 0.0655588
0.00856342 -0.00606594
0.0859466 0.0677407
0.0417629 -0.00590825
0.0337182 0.058
0.00142251 -0.00568437
0.0295774 -0.00626341
-3.40966e-05 0.0040674
0.104819 0.0586561
0.0871096 0.0689231
-3.91322e-05 0.0034303
0.000834798 0.0039703
0.0631207 -0.00228544
0.0373475 0.0501911
0.0541426 0.0519934
0.0286795 0.0439849
0.0867333 0.0697504
-0.0295746 -0.00150255
0.0109781 0.00405902
0.0978866 0.0682698
0.0134225 0.0786585
0.110244 0.0566712
0.0865811 0.0727737
0.0268143 0.0473044
0.0716773 -0.000970419
0.0482639 0.0672547
0.130756 0.0630894
0.0594395 0.0665778
0.0594482 0.0796681
0.0594398 0.0691323
0.080392 0.0513416
0.0305919 0.0494465
0.0261392 0.0643722
0.0385441 -0.00600172
0.0318766 0.00034063
-0.0271126 -0.00765379
0.0594399 0.0450194
1.01848e-06 -0.00855171
0.0417454 -0.00195729
0.0582389 0.0527783
0.0177216 -0.00799167
-0.0860366 -0.000547069
-0.049248 -0.00720546
-0.0414014 0.00773285
0.0373484 0.0467598
-1.1614e-09 0.0112446
0.0385437 -0.00594479
0.088282 0.0568096
0.0461382 0.0652808
-0.0210969 -0.00712559
-0.0207316 0.000567057
0.0593312 0.0502088
0.0321466 0.0600918
0.0966587 0.0598412
0.085161 0.0671739
0.101441 0.0522113
0.0594399 0.0640344
0.0978866 0.0666939
0.0330953 0.0479038
0.0861487 0.0703986
-0.0209708 0.00636351
0.0109586 0.00436927
0.0625951 0.0674228
-0.0494977 -0.00140029
0.0937753 0.0658401
0.0594116 0.0395287
0.0857788 0.0496991
0.0329021 0.0575747
0.0870053 0.0581658
0.0329332 0.0583979
0.0964766 0.0536265
-2.86114e-05 -0.00508723
0.0211185 0.0644802
0.0385441 0.0125687
0.0859865 0.0645973
0.0223968 0.0445238
0.0529666 -0.00676476
0.0174339 0.0441387
0.0318766 0.00581801
0.0375381 0.0482396
-0.0194973 0.00116657
-0.00131975 -0.00794291
0.0477959 0.0652985
-0.0417792 -0.00124102
7.08095e-06 0.00295602
0.118684 0.0743361
4.17498e-06 -0.0069409
0.0267369 -0.00528105
-0.0161473 0.04187
0.00931426 0.0624655
0.0901948 0.0546147
0.0188609 -0.00519154
-0.0210969 0.00257379
-0.0621272 -0.00278752
0.0319944 0.0617047
0.124311 0.0673512
0.0317697 0.06659
0.0724741 0.050821
0.0882822 0.0529043
0.0710837 0.0662755
0.0710837 0.066611
0.0317698 0.0659189
0.126127 0.0669728
0.0322983 0.0660163
0.0851609 0.0683513
-0.0860382 -0.00633144
0.0594376 0.0537174
0.0462066 0.00337917
0.0594397 0.0702829
-0.0210947 0.00608049
-0.0188057 0.000824177
0.0268143 0.0470962
0.0267393 -0.00505333
0.0267393 -0.00704523
0.088729 0.0714752
0.0983621 0.0570517
0.0594397 0.0714475
0.0322983 0.0660163
-0.0707068 0.00384172
0.022444 0.0468164
-0.0462076 -0.00666086
0.0281392 0.0616205
0.000835111 -0.00581436
0.00250079 -0.00145882
0.0541426 0.051053
parameters: [ 9.454  1.     2.     1.2    4.   ]. error: 38709.0579874.
----------------------------
epoch 0, loss 1.18847
epoch 128, loss 0.908423
epoch 256, loss 0.806341
epoch 384, loss 0.612567
epoch 512, loss 0.693232
epoch 640, loss 0.55126
epoch 768, loss 0.611669
epoch 896, loss 0.660305
epoch 1024, loss 0.646155
epoch 1152, loss 0.77318
epoch 1280, loss 0.749788
epoch 1408, loss 0.631249
epoch 1536, loss 0.782632
epoch 1664, loss 0.555459
epoch 1792, loss 0.686132
epoch 1920, loss 0.559477
epoch 2048, loss 0.555946
epoch 2176, loss 0.611701
epoch 2304, loss 0.736287
epoch 2432, loss 0.596683
epoch 2560, loss 0.762663
epoch 2688, loss 0.551464
epoch 2816, loss 0.713234
epoch 2944, loss 0.489821
epoch 3072, loss 0.611431
epoch 3200, loss 0.756425
epoch 3328, loss 0.784017
epoch 3456, loss 0.664948
epoch 3584, loss 0.544256
epoch 3712, loss 0.67662
epoch 3840, loss 0.6293
epoch 3968, loss 0.737961
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0321468 0.0689882
0.0281392 0.0647106
0.0920961 0.0653026
0.0385434 0.0103308
0.0870053 0.061065
0.0971047 0.0651709
-0.0207342 -0.00110651
0.0582651 0.0591983
0.0261394 0.0681336
-0.0188593 0.0148185
0.0897923 0.0668228
-0.0529722 0.00864158
0.0859467 0.0647402
-0.00628221 0.0513027
0.0527577 0.0680787
-0.0662867 -0.00256159
0.0234681 0.0119874
0.0188019 0.0581893
0.101441 0.0629351
0.106629 0.0586687
0.0134225 0.069268
0.0594297 0.0590666
-0.0272904 0.00864935
0.0204028 -0.0010749
0.0707072 0.000635882
0.124277 0.0631484
0.038792 0.0610347
0.0424268 0.058103
0.0861486 0.0701488
0.0337179 0.062902
0.0122504 0.0591366
0.125873 0.0614384
0.0594297 0.0586509
0.0527575 0.0660766
0.0562649 0.0575629
0.0592543 0.0558853
0.0716711 0.00644261
0.0986864 0.0628519
0.0599029 0.0695925
0.0387924 0.0610418
0.0449639 0.0127764
0.0210212 0.0635683
0.0281393 0.0664882
0.044975 0.0647461
0.0594397 0.0680827
0.0477958 0.0628797
-3.66166e-06 0.00977177
-0.0131855 0.00887676
-0.0417488 0.00495087
0.126127 0.0623347
0.0267781 0.0553077
0.0626106 0.0595415
0.022444 0.0532697
0.0496681 0.066472
-0.0106876 0.00464522
0.0631173 0.00702899
0.0781227 0.059994
0.00459996 0.0035285
0.059435 0.0564521
-0.0177216 0.0117713
0.0248097 0.00071469
-0.0113495 0.00877956
0.0117782 -0.00246037
0.00460075 0.0052074
0.0593312 0.0611396
8.39808e-07 0.00134319
0.0594273 0.0583039
0.134985 0.0602079
-4.71368e-07 0.0124361
-0.0131841 0.000168291
0.0594424 0.0589495
0.125883 0.0585988
0.0248197 0.00099282
-0.00131975 0.00606355
0.0407515 0.0582166
0.0384676 0.0576446
-0.0860431 -0.00102011
-0.00735889 0.0579562
0.0599612 0.0617672
-0.00131656 -0.00148765
-0.0215356 -0.0009308
-0.0056768 0.0577577
0.0396776 0.0587838
0.0621302 -0.0021642
0.097887 0.0636459
0.0538015 0.0622715
0.12614 0.0613091
0.00931417 0.0687562
0.0495763 0.00350725
-0.0385427 -0.00160698
0.0648671 0.0598944
-1.48505e-07 -0.000315267
0.0210981 -0.000633138
0.0611197 -0.0024687
0.0337182 0.0654341
0.0594397 0.0701404
0.0424369 0.0557161
-0.0346424 0.00387784
0.0819546 0.060961
0.023648 0.0657623
0.0117785 0.00547577
0.00931426 0.0664305
0.0215366 0.0127433
-0.0385421 -0.000576536
0.0781227 0.0609421
-0.0734384 -0.00147809
0.0197009 0.0684337
0.0621302 -0.00208015
0.100115 -0.00227276
0.0122352 0.0555702
0.0594396 0.0692483
-0.0234699 0.014822
-0.0494953 -0.00124944
0.0144707 0.0135577
0.0248097 -0.000613316
0.0278228 0.0661905
0.0599031 0.0670059
0.0321632 0.0546737
0.0727411 0.0690682
0.130757 0.0675243
0.0594398 0.0668227
0.0394799 0.0105931
0.0911183 0.0591178
0.0661223 0.0683611
-0.00378297 -0.00020517
-0.000792737 -0.00024924
0.0937753 0.0668183
0.00863092 0.0553936
parameters: [ 9.471  1.     2.     1.2    4.   ]. error: 2077851.45382.
----------------------------
epoch 0, loss 1.32018
epoch 128, loss 0.544784
epoch 256, loss 0.830299
epoch 384, loss 0.826763
epoch 512, loss 0.690636
epoch 640, loss 0.55888
epoch 768, loss 0.776737
epoch 896, loss 0.700299
epoch 1024, loss 0.623913
epoch 1152, loss 0.641209
epoch 1280, loss 0.579162
epoch 1408, loss 0.614899
epoch 1536, loss 0.591092
epoch 1664, loss 0.612069
epoch 1792, loss 0.666963
epoch 1920, loss 0.686936
epoch 2048, loss 0.48176
epoch 2176, loss 0.662502
epoch 2304, loss 0.735077
epoch 2432, loss 0.716735
epoch 2560, loss 0.534901
epoch 2688, loss 0.648749
epoch 2816, loss 0.853287
epoch 2944, loss 0.559567
epoch 3072, loss 0.648376
epoch 3200, loss 0.523266
epoch 3328, loss 0.580698
epoch 3456, loss 0.70607
epoch 3584, loss 0.673559
epoch 3712, loss 0.702021
epoch 3840, loss 0.540525
epoch 3968, loss 0.837819
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0927399 0.0574378
0.0710835 0.06953
-0.0132965 0.0846229
0.133575 0.0792365
0.0730675 0.0580805
0.0197009 0.0601894
-0.0414059 0.00361409
0.12614 0.0754716
-0.0272904 -0.00980176
-0.0857218 0.000741393
0.0327313 0.058267
0.0605996 0.0600229
0.0204028 -0.0069198
0.0594482 0.0907295
0.0662803 -0.00859733
0.0131845 -0.0061874
0.020735 -0.00920534
-2.64683e-07 -0.00141458
-0.000835392 -0.00552909
0.0318766 0.0016843
0.0659505 -0.00137928
0.0927397 0.0560562
-0.0394796 -0.00751573
0.0384777 0.0541232
0.0385437 -0.0044147
0.0662931 -0.0074276
0.0807993 0.0371615
0.033718 0.0700208
-0.00236357 0.0646079
0.0267781 0.0512977
0.129458 -0.00585882
0.0968256 0.0678217
0.0859467 0.0703595
-0.0495799 -0.00284201
0.032902 0.0762565
-0.0362233 -0.00762126
0.129458 0.0421239
0.0201879 0.0396838
0.0562851 0.0629888
-0.0211014 0.00215324
-0.0449623 0.00826436
0.0496683 0.0572184
-0.00699273 0.0424496
-0.0611177 -0.0029393
0.0197009 0.0534832
-0.0662877 -0.00667477
0.0859865 0.0787308
0.0594449 0.0578237
-0.00699273 0.0596844
0.0461382 0.0606986
0.0394799 -0.00519466
0.0367961 0.00789368
-0.036223 -0.00704237
0.0589127 0.0582464
0.0625951 0.0590438
0.0337182 0.0543678
0.0268143 0.0487411
0.0964772 0.0579073
0.0861486 0.0695771
-3.91322e-05 0.000868195
0.0140549 0.0401501
-0.0449623 -0.00931813
0.0109781 -0.00772976
0.0237989 0.0371184
-0.0111732 0.0536797
0.0417629 0.00481577
0.118684 0.0887189
0.0594482 0.0889395
-0.0211014 -0.00876577
-0.0271157 -0.00277025
0.0449639 0.00164951
-0.0109566 -0.00921193
0.061289 0.0894357
0.0318693 0.0463342
1.95996e-07 0.00548463
0.072741 0.0622188
0.081797 0.0634235
0.0387924 0.0429911
0.079197 0.0653264
0.0234681 -0.00818219
-0.000807624 0.0104024
0.0594397 0.0605894
0.0277564 0.0529311
0.0290868 0.0484731
0.0897923 0.0553443
0.132184 0.0957213
0.0594349 0.0553741
0.0247038 0.052175
0.0281391 0.0529616
-0.0109774 0.00116734
-0.0131841 -0.00731754
0.0964307 0.0610843
0.0562597 0.06132
0.0492512 -0.00700748
-0.036223 -0.00772398
-0.0113495 -0.00965582
0.0496683 0.0539799
0.0594349 0.0601452
0.0329331 0.0585512
0.0861488 0.0685209
0.0337182 0.0623753
-0.0300728 0.000639421
0.0283783 0.0547139
0.0941706 0.064301
0.0462156 -0.00264771
0.0631173 0.000131965
-0.0529682 0.00140451
0.027745 0.0433789
0.0210915 -0.00855979
-0.00628321 0.0354702
0.0927399 0.0750558
0.0562649 0.0645311
0.0594376 0.0629503
0.0201879 0.0540241
0.0477959 0.0538776
0.0497674 0.0585035
-0.0152583 0.0125788
0.0691987 0.0637467
0.0589133 0.0512247
-0.0417573 -0.00873393
0.038792 0.0554719
0.0496683 0.0650246
0.0397474 0.0564448
-0.0394792 -0.00719003
0.0853245 0.0644838
0.0251234 0.076881
0.0594396 0.050721
0.0594482 0.0889395
parameters: [ 9.458  1.     2.     1.2    4.   ]. error: 1309991062.71.
----------------------------
epoch 0, loss 1.26802
epoch 128, loss 0.606037
epoch 256, loss 0.643961
epoch 384, loss 0.59264
epoch 512, loss 0.619497
epoch 640, loss 0.527153
epoch 768, loss 0.628649
epoch 896, loss 0.693903
epoch 1024, loss 0.591059
epoch 1152, loss 0.559422
epoch 1280, loss 0.600859
epoch 1408, loss 0.707845
epoch 1536, loss 0.684917
epoch 1664, loss 0.562057
epoch 1792, loss 0.538848
epoch 1920, loss 0.52661
epoch 2048, loss 0.758443
epoch 2176, loss 0.651276
epoch 2304, loss 0.656929
epoch 2432, loss 0.4869
epoch 2560, loss 0.555901
epoch 2688, loss 0.585677
epoch 2816, loss 0.544056
epoch 2944, loss 0.577329
epoch 3072, loss 0.674882
epoch 3200, loss 0.772699
epoch 3328, loss 0.455258
epoch 3456, loss 0.607602
epoch 3584, loss 0.507737
epoch 3712, loss 0.597249
epoch 3840, loss 0.668195
epoch 3968, loss 0.65073
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0271193 0.0171303
0.0310444 0.059399
0.0319943 0.067951
0.0971043 0.0598436
0.112932 0.0627485
-3.40966e-05 0.0053152
0.0217933 0.0585501
0.0594398 0.0703315
0.0595095 0.0525547
0.0626054 0.0601306
0.0122504 0.0532652
-0.00893962 0.00493814
-0.00331175 0.0448267
-0.0023637 0.0619874
0.0807987 0.0241384
0.121243 0.0725733
0.022444 0.0523688
0.0327313 0.0678992
-1.48505e-07 0.0232938
0.0482638 0.0726283
0.125157 0.0627762
0.0337179 0.0677715
0.0234704 0.0316278
0.0416418 0.00379238
0.0662803 0.0089539
0.0281391 0.0688588
0.062595 0.0701525
0.0327314 0.0702957
0.000213351 0.0576843
0.0305919 0.0517583
0.0594397 0.0704358
0.104819 0.0640789
-0.039089 0.00994194
0.0910569 0.0718342
0.0192074 0.0681508
-0.00236357 0.0620502
-0.0271153 0.011769
0.0318766 0.0347448
0.036223 0.0322874
-2.99285e-06 0.0120639
0.0968255 0.0731381
0.038792 0.0601552
0.121243 0.0713519
0.0937753 0.0630515
0.0326439 0.00357355
-0.0113495 0.0107815
0.0140549 0.0550801
0.10664 0.0617865
0.0286795 0.0571131
0.0538015 0.0601773
0.0321467 0.067922
0.0594562 0.049903
-0.0106908 0.00393864
0.0340225 0.00351162
0.0971043 0.0609635
-0.0494977 0.00297126
0.0237987 0.0630126
-0.0271121 0.0126015
0.0122243 0.048325
-0.014471 0.00654871
0.0407515 0.0599321
0.0122352 0.0514035
-0.00856301 0.00500429
0.0611197 0.0116658
0.0439209 0.0528476
0.0594398 0.0717367
0.0323163 0.00966525
0.0605534 0.053423
0.0208321 0.0277512
0.10665 0.0621092
0.0594396 0.0685155
0.0278231 0.0680274
0.0599029 0.072664
-0.0250779 0.0504436
0.106645 0.0606141
-0.0207316 0.00367053
0.0271187 0.00936254
0.0865811 0.0731132
0.0730674 0.0706359
0.0197008 0.067425
0.0416368 0.0162385
-0.0204122 0.0279813
-0.032317 0.00256851
-0.0529722 0.0192034
0.0189892 0.0038417
0.0224432 0.0575054
0.0370775 0.0559952
0.0667826 0.0706384
0.0495801 0.00309636
-0.0207292 0.0190997
0.0188016 0.0575436
0.0247829 0.0134009
-0.0362226 0.00404949
0.0599029 0.0722786
0.0188603 0.0248996
0.0734322 0.0252577
0.0989677 0.0631142
0.0267781 0.0525127
0.0210209 0.0571636
0.0867201 0.073049
0.0661224 0.0682324
0.0384777 0.0546439
0.000782366 0.00561109
0.0989678 0.0624716
-0.0207342 0.0164147
0.0248097 0.0137354
-0.00131975 0.00468367
-6.30661e-07 0.00376288
0.12432 0.058496
-0.0144836 0.00345608
0.0594467 0.0612113
0.088729 0.0729218
0.0997468 0.0601465
0.0385431 0.00554849
0.100097 0.0631918
0.0867202 0.073106
-0.0394792 0.00502195
0.0964766 0.0632656
-0.0111821 0.0469539
0.0267781 0.0555863
0.0978866 0.0611569
0.0317696 0.0666468
0.0691985 0.0693619
0.0201879 0.0571909
0.0529729 0.0115818
0.0237988 0.0648694
0.0950803 0.0670469
0.0968255 0.0731381
parameters: [ 9.449  1.     2.     1.2    4.   ]. error: 12030.2795245.
----------------------------
epoch 0, loss 1.00722
epoch 128, loss 0.930913
epoch 256, loss 0.61534
epoch 384, loss 0.568552
epoch 512, loss 0.553637
epoch 640, loss 0.638848
epoch 768, loss 0.627063
epoch 896, loss 0.597694
epoch 1024, loss 0.640885
epoch 1152, loss 0.623547
epoch 1280, loss 0.632461
epoch 1408, loss 0.602863
epoch 1536, loss 0.525507
epoch 1664, loss 0.517654
epoch 1792, loss 0.707729
epoch 1920, loss 0.71245
epoch 2048, loss 0.588937
epoch 2176, loss 0.742745
epoch 2304, loss 0.597636
epoch 2432, loss 0.713976
epoch 2560, loss 0.570203
epoch 2688, loss 0.794058
epoch 2816, loss 0.609983
epoch 2944, loss 0.616469
epoch 3072, loss 0.560224
epoch 3200, loss 0.717601
epoch 3328, loss 0.55985
epoch 3456, loss 0.498251
epoch 3584, loss 0.49706
epoch 3712, loss 0.612082
epoch 3840, loss 0.688007
epoch 3968, loss 0.645885
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.00628221 0.0518395
0.0267443 0.000260052
0.0860428 0.015366
0.100668 0.059634
-0.0132848 0.067027
0.085161 0.066659
-0.0621343 0.00511224
-0.0734312 -0.00145556
0.134995 0.0631429
0.0248197 0.000461218
0.0594204 0.05999
0.0236481 0.0595873
0.080402 0.0633414
0.038792 0.0561586
0.0594397 0.0721128
0.0267781 0.0590503
0.0191276 0.049809
0.0611263 0.00720492
0.0968257 0.0700543
0.0983621 0.0654229
-0.0857218 0.00465827
0.0495796 -0.000930204
-0.0248069 0.0219729
0.0701141 0.062987
0.112932 0.0715841
0.059435 0.0586107
0.0594324 0.0622393
0.0301505 0.0684858
0.0851609 0.0695293
0.0675205 0.0576619
0.0920969 0.0669484
0.0594397 0.0679399
0.0144853 0.0106683
0.0594398 0.0670927
-0.0462169 -0.00194775
-0.0210896 0.00781309
0.0283783 0.0539032
0.000796249 0.0114482
0.0997462 0.0572221
0.0370769 0.0592283
0.0208321 0.0257047
-0.0412711 -0.00166718
0.0236481 0.0590615
0.126171 0.065767
0.0346447 -0.0021509
0.0208321 -0.00144181
-0.00894125 0.0154075
0.0857789 0.0558243
0.0199309 0.0614863
0.032933 0.0600544
0.0416418 -2.70895e-05
0.0106903 0.00812501
0.0385437 0.0134515
0.0594396 0.0690627
0.0868851 0.069299
0.0283783 0.0601978
-0.0346416 -0.00258614
0.0327313 0.0669035
0.081527 0.0604443
0.0971047 0.0695824
0.0208245 0.0185272
0.059435 0.0752
0.0594162 0.0582439
-0.00236357 0.0590798
0.0981543 0.0612748
0.0594396 0.0685984
1.74905e-07 -0.00166682
0.0857183 -0.00142593
-0.0271278 0.0189999
0.124311 0.0710126
0.0859466 0.0664492
0.0624794 0.0556183
-0.0662843 -0.00300407
0.019701 0.0618934
0.0385437 0.00843535
0.0461384 0.0630191
0.0532535 0.0550381
0.00894166 -0.00129906
0.0897923 0.0646723
-0.0495796 0.00206306
-0.0365701 -0.000766083
1.74905e-07 -0.00221521
0.0317699 0.062982
0.0867334 0.0702461
-4.6184e-07 0.0236875
0.0870049 0.0653689
0.0396777 0.0615529
0.0462066 -0.000277731
-0.0271157 0.00853095
0.0927396 0.0631324
0.0865811 0.0718916
-0.0716787 -0.00225609
-6.0681e-07 0.00884179
-0.0118767 0.0559789
0.106624 0.0695129
0.0691986 0.066353
0.00894003 0.0146849
0.0205126 0.0605764
0.0267393 0.0111996
0.0966596 0.0624699
0.0855722 0.0657279
0.0589765 0.072328
0.0871096 0.0677063
-0.0707068 -0.00219962
0.105503 0.0763072
0.0904958 0.0629774
0.00250001 0.00410925
0.0217933 0.0622287
-0.0113473 0.0105195
-0.00856301 -0.000922668
-1.63011e-07 0.000616596
0.0236479 0.0610388
0.134985 0.0654771
-0.0248182 0.000122414
0.0710838 0.0696987
0.0248197 0.0103777
-0.0860382 0.00820439
-0.0390898 -0.00237864
0.0594466 0.075287
0.0739227 0.0652552
-0.0414014 -0.00271645
0.0210958 4.42498e-05
0.0631173 0.00856504
0.126127 0.0673786
-0.0413988 0.00622887
0.0868849 0.0708289
0.0538009 0.0612528
0.130756 0.0688002
parameters: [ 9.44  1.    2.    1.2   4.  ]. error: 49634.0571605.
----------------------------
epoch 0, loss 1.13681
epoch 128, loss 0.943211
epoch 256, loss 0.697318
epoch 384, loss 0.709366
epoch 512, loss 0.731734
epoch 640, loss 0.683225
epoch 768, loss 0.654644
epoch 896, loss 0.54022
epoch 1024, loss 0.56194
epoch 1152, loss 0.714364
epoch 1280, loss 0.878415
epoch 1408, loss 0.714588
epoch 1536, loss 0.522629
epoch 1664, loss 0.677421
epoch 1792, loss 0.625594
epoch 1920, loss 0.582165
epoch 2048, loss 0.650679
epoch 2176, loss 0.731475
epoch 2304, loss 0.631479
epoch 2432, loss 0.576569
epoch 2560, loss 0.638209
epoch 2688, loss 0.793414
epoch 2816, loss 0.568804
epoch 2944, loss 0.742659
epoch 3072, loss 0.767841
epoch 3200, loss 0.793069
epoch 3328, loss 0.554321
epoch 3456, loss 0.748672
epoch 3584, loss 0.567338
epoch 3712, loss 0.747645
epoch 3840, loss 0.629073
epoch 3968, loss 0.574254
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0326435 -0.00115409
0.0589765 0.0598771
0.028139 0.060082
-0.0204101 0.00755365
0.0329021 0.0628818
0.080382 0.0576867
0.0964772 0.0595168
-0.0385431 0.0147619
0.0318787 0.00726938
0.0271115 0.00424188
-0.0271152 -0.00143043
0.080392 0.0545929
0.0329332 0.057395
0.0594398 0.0615546
0.0234754 0.00205574
0.0865811 0.0613692
0.129993 0.0572271
0.0301504 0.0564532
-0.0106876 0.00355898
0.0220539 0.0615226
0.0134225 0.0694891
0.0321466 0.0593717
0.0152591 -0.000389504
-0.0118764 0.0556966
0.0764118 0.0574487
0.0857789 0.0568922
-0.0106908 -0.0017627
0.0861486 0.0611463
0.0375434 0.0535805
0.0920561 0.0598236
-0.0131855 0.00178783
0.0739226 0.0622635
0.112932 0.0637603
0.0611271 0.00524965
-0.0113473 0.00561608
0.0710837 0.0624217
-0.0113473 -0.000435463
0.0492512 -0.000220048
-0.0449623 0.00814216
0.0417484 0.00503756
0.0281391 0.0555457
0.018802 0.00139133
-0.0416423 -0.000696506
0.0407513 0.0557618
0.0375381 0.0553244
-0.0152533 0.00831318
1.01848e-06 0.00564401
-0.129453 0.00358947
-0.0210896 0.0151682
0.0791961 0.0535183
0.0776735 0.0126696
5.31452e-07 0.00014376
0.0716711 -0.000265814
-0.0271111 0.00841116
0.0106905 -0.00224008
0.0716711 -0.000768698
0.0468012 0.000139625
0.0511643 0.056599
-0.0417488 -0.000918902
0.0317699 0.0566892
0.130757 0.0582221
0.0996717 0.0622974
-0.00735889 0.0490536
0.022444 0.0552967
0.0301504 0.0597329
0.0710835 0.0593668
-0.0109566 0.0106853
-3.91322e-05 0.00146547
0.0273016 -0.000295702
0.0134225 0.0685077
0.0867334 0.061612
-0.00236368 0.0582681
0.125873 0.0572709
0.0594396 0.0623895
0.0541426 0.056454
0.0261395 0.058846
0.0605996 0.0529813
0.0659505 0.000985653
-0.0271152 -0.00281178
0.12945 0.000964192
0.0385437 0.00327037
0.0223972 0.054598
0.0968256 0.0648724
0.0920561 0.0573494
0.0210212 0.0641433
0.0860523 0.000285906
-7.68032e-07 0.0147217
0.0117845 -0.00291604
0.0857795 0.0578478
0.0594396 0.0628532
0.0861488 0.0586364
0.100116 0.00281831
0.0815268 0.0536118
0.0807993 0.0131516
0.0223968 0.0513751
0.0631173 0.00544507
1.13647e-06 0.0138543
4.07609e-05 -0.00161955
0.0268142 0.0570173
0.0911173 0.0608373
0.0675205 0.054124
0.000213351 0.0599891
-0.0417792 -0.000525273
0.0277564 0.0553621
-0.0210896 0.00644326
0.0496681 0.0634824
-0.0340213 -0.00080601
-0.00142113 -0.000778749
0.100115 -0.00145052
3.5013e-07 0.0103822
0.0281392 0.0622755
0.0122504 0.0523754
0.0412776 0.00969504
0.0390991 0.00209019
0.0492512 0.00502662
0.0495801 -0.000820021
-0.0118765 0.0494495
-0.0413988 0.000319564
0.0582389 0.0518789
-0.00894944 0.0144245
0.0606162 0.0520193
0.000834782 0.00905451
-0.00553123 0.0452417
0.0882823 0.058092
0.0706155 0.0635027
0.0272916 0.00940439
0.046799 0.00828295
0.0210981 0.0111248
parameters: [ 9.449  1.     2.     1.2    4.   ]. error: 1514099676.73.
----------------------------
epoch 0, loss 1.33644
epoch 128, loss 1.15936
epoch 256, loss 0.786642
epoch 384, loss 0.570916
epoch 512, loss 0.842338
epoch 640, loss 0.836704
epoch 768, loss 0.540214
epoch 896, loss 0.686163
epoch 1024, loss 0.771462
epoch 1152, loss 0.481388
epoch 1280, loss 0.587225
epoch 1408, loss 0.571296
epoch 1536, loss 0.633998
epoch 1664, loss 0.590817
epoch 1792, loss 0.701631
epoch 1920, loss 0.646862
epoch 2048, loss 0.665068
epoch 2176, loss 0.738281
epoch 2304, loss 0.629453
epoch 2432, loss 0.62734
epoch 2560, loss 0.73663
epoch 2688, loss 0.587179
epoch 2816, loss 0.553556
epoch 2944, loss 0.709638
epoch 3072, loss 0.727427
epoch 3200, loss 0.646575
epoch 3328, loss 0.500777
epoch 3456, loss 0.712337
epoch 3584, loss 0.656197
epoch 3712, loss 0.685516
epoch 3840, loss 0.598809
epoch 3968, loss 0.68573
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0346352 0.0038704
-0.049248 0.000884105
0.0207377 0.00890961
0.0131848 0.00529627
-0.0188599 0.0045761
-0.00726943 0.0567802
0.0144853 0.00584135
0.0866799 0.0582112
-0.0529682 0.00493527
-4.4075e-07 0.0127609
0.0482638 0.0592746
0.0436381 0.0575486
0.0496682 0.0603613
0.0394803 0.0171277
0.0871098 0.0608038
0.01134 0.0110464
0.0631173 0.00177914
0.0496683 0.0604638
0.000794682 0.0074243
0.0412776 0.0153329
0.0593665 0.0573688
0.0662934 0.00166162
0.132184 0.0662491
0.124277 0.0576463
0.0562648 0.0579786
0.0594396 0.0611932
0.0417629 0.00924643
0.0861486 0.060169
0.0318787 0.0142296
-0.0716686 0.00815623
-0.129453 0.00886879
0.121243 0.0618815
0.0752955 0.058246
0.0224432 0.0581859
-0.0131848 0.00709078
0.102034 0.0568266
0.0140549 0.056
0.0122295 0.0569285
0.0414075 0.00180089
0.0861488 0.0613501
0.0594398 0.0587194
0.080392 0.0572695
0.0375434 0.0588979
0.0897923 0.0603924
-0.00728242 0.0564446
0.0593312 0.0578406
0.118684 0.0620406
0.0495796 0.00182455
0.0267419 0.00964358
0.0691984 0.0585137
0.0496681 0.0609528
0.0594398 0.0600033
0.0436381 0.0575486
0.0367961 0.00353528
0.0373475 0.0560392
-0.00378521 0.0027819
0.0594375 0.0587972
0.0247806 0.0142307
-0.00236368 0.0599367
-0.0611272 0.00397467
0.0188015 0.0628969
0.0621308 0.011375
0.0631163 0.00466467
-0.0414033 0.00904722
0.0594426 0.057763
0.0387924 0.0578623
0.042448 0.0596101
0.0594396 0.0596655
0.00894166 0.00627382
-0.0207342 0.00352366
0.0329331 0.0617916
0.106619 0.0583968
-0.129449 0.00321332
0.0989675 0.0619706
0.00459996 0.00246859
0.0319943 0.0609305
0.0867201 0.0612806
0.0496681 0.0595618
0.019701 0.0605018
-0.0111821 0.0556049
0.0318693 0.0567561
0.0373484 0.059007
0.0204028 0.00741976
0.130756 0.0606701
0.0859466 0.06207
0.000835126 0.00993757
0.0611263 0.00255279
0.101814 0.0628622
-0.0827278 0.00844613
-0.0189887 0.00416722
0.100097 0.0633095
0.0631196 -0.000811986
-0.0462148 0.0185523
0.0803921 0.0587207
0.0295843 0.0206762
-0.129457 0.00467043
0.0807983 0.00790013
-0.0385434 0.00737541
-0.0111732 0.0574751
0.0870053 0.0576511
-0.0495799 0.00486414
0.0385437 0.0056327
0.0271117 0.00156712
0.0477957 0.0596684
0.0691985 0.0579916
0.0871097 0.059107
0.135027 0.0584278
0.102034 0.0572778
0.0278229 0.059516
-0.0056768 0.054581
0.0952316 0.0586717
-0.0118765 0.057815
0.0867334 0.0602503
0.0964307 0.058234
0.0594397 0.0593054
0.0860523 0.00971172
4.07333e-06 0.0127658
0.0205126 0.0555289
0.026809 0.0581158
-2.64683e-07 0.0224092
-0.00701261 0.0598671
-0.0234677 0.0072337
0.0140549 0.0560126
4.17498e-06 0.0121624
-0.0113495 0.00390822
0.0234638 0.0150465
0.0594494 0.0650456
0.0261392 0.0574867
parameters: [ 9.449  2.     2.     1.2    4.   ]. error: 5498629.9217.
----------------------------
epoch 0, loss 1.28551
epoch 128, loss 0.925487
epoch 256, loss 0.981964
epoch 384, loss 1.06215
epoch 512, loss 0.833889
epoch 640, loss 0.830776
epoch 768, loss 0.704011
epoch 896, loss 0.834466
epoch 1024, loss 0.950085
epoch 1152, loss 0.803556
epoch 1280, loss 0.766606
epoch 1408, loss 0.656407
epoch 1536, loss 0.702768
epoch 1664, loss 0.601917
epoch 1792, loss 0.619935
epoch 1920, loss 0.686717
epoch 2048, loss 0.781795
epoch 2176, loss 0.54288
epoch 2304, loss 0.875616
epoch 2432, loss 0.69304
epoch 2560, loss 0.705771
epoch 2688, loss 0.677395
epoch 2816, loss 0.834671
epoch 2944, loss 0.510226
epoch 3072, loss 0.734169
epoch 3200, loss 0.713288
epoch 3328, loss 0.728165
epoch 3456, loss 0.79816
epoch 3584, loss 0.748307
epoch 3712, loss 0.722536
epoch 3840, loss 0.603543
epoch 3968, loss 0.728344
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594273 0.0514959
0.0594398 0.05515
0.015257 0.00341154
0.0594397 0.0545446
-0.0177216 0.00334605
0.0866799 0.0534995
0.0117847 -0.0049302
0.0752422 0.0522073
0.101441 0.0509358
-0.0204101 0.00292646
-1.99995e-06 0.0031438
0.0327313 0.0522824
-0.000795203 0.00308046
0.0871095 0.0553785
0.126127 0.0487426
0.0691068 0.0498737
0.000793148 -0.00302889
0.0204143 0.00586439
0.0496683 0.0541453
0.0706155 0.0548628
-0.041635 -0.00146154
0.0594379 0.0655839
0.0144704 0.00444202
-0.0189824 -0.00226595
0.0424581 0.0521053
0.0971043 0.0600855
0.094504 0.0500635
0.032195 0.0523081
0.0373475 0.0482763
-0.0385431 -0.00208628
0.0859467 0.0544334
0.0867333 0.0547924
0.000794682 -0.00271465
0.00142251 -0.00159585
0.0197009 0.0543133
0.0271187 -0.000781196
0.0868851 0.0540585
0.000834798 -0.00281857
0.046799 -0.00272045
0.0435639 0.0513755
-0.0210947 0.00202337
0.0594919 0.0478934
-0.000807624 0.00255963
0.0317699 0.0540501
0.0716719 0.000207097
0.0968255 0.0551385
0.133575 0.0487867
0.106629 0.052911
0.0131848 0.00194715
0.081797 0.0498546
0.019701 0.0537251
0.0109781 0.00545834
0.0727412 0.0557288
-0.0412757 -0.0029953
0.0268195 0.0502712
0.0791969 0.0506384
0.0215366 0.00335658
0.0859467 0.0557094
-3.11621e-07 -0.00296332
0.00931426 0.0533704
-1.63011e-07 0.00558744
-0.0416401 0.00111194
0.0927398 0.0544188
0.0243585 0.0496233
0.022444 0.0504524
0.0582389 0.0481553
0.0327313 0.0530027
0.081527 0.0518446
0.0707072 0.00440818
-0.000807624 -0.00366624
0.0910569 0.0540078
0.059435 0.0518015
0.0435741 0.051112
0.0243594 0.0487713
0.0861486 0.0546839
0.0968257 0.0558064
-0.0417792 0.00247981
0.0337178 0.053382
0.0594398 0.054666
0.0205126 0.0470086
-0.0467928 0.00318315
0.081527 0.0512588
0.0625949 0.0539326
0.0310444 0.0494133
0.0482638 0.0547081
0.0117845 -0.0055788
0.0188019 0.0583369
-0.0117822 -0.00518172
0.0248076 -0.00353365
0.0626002 0.0525944
-0.0271153 0.00554475
0.0318787 -0.002248
-0.0412731 -0.0035988
0.0330953 0.0501049
0.0594396 0.0559064
0.0730674 0.0545697
-0.0188057 -0.00540658
-0.0467928 -0.00372924
5.31452e-07 0.00235273
-0.0132904 0.0656083
2.05834e-07 -0.00220764
-0.0611177 -0.00527774
-0.0189848 0.00268665
0.0971043 0.0585174
-0.0272995 -0.00113861
0.123121 0.0503247
0.0122404 0.0492985
0.060558 0.0494002
0.028139 0.0538138
-0.0132965 0.0653177
1.13647e-06 0.00342984
0.00596587 0.0583723
0.000782366 -0.00349149
0.0781227 0.0489109
-0.0023637 0.0536277
-0.0271111 -0.0021601
0.0267781 0.0486345
-0.00131975 0.000330103
0.0373475 0.0498323
0.0168394 0.0487614
0.0188088 -0.00482808
0.0243585 0.047647
0.0593133 0.0483415
0.0871096 0.0552161
0.0247806 0.0036381
0.0458116 0.0543624
-0.0117822 -0.0052437
0.0267393 0.00600019
parameters: [ 9.449  3.618  2.     1.2    4.   ]. error: 88204007.5181.
----------------------------
epoch 0, loss 1.09768
epoch 128, loss 0.789563
epoch 256, loss 0.668322
epoch 384, loss 0.670181
epoch 512, loss 0.623072
epoch 640, loss 0.667991
epoch 768, loss 0.589571
epoch 896, loss 0.67759
epoch 1024, loss 0.772914
epoch 1152, loss 0.712433
epoch 1280, loss 0.656761
epoch 1408, loss 0.68367
epoch 1536, loss 0.569272
epoch 1664, loss 0.612938
epoch 1792, loss 0.814143
epoch 1920, loss 0.755129
epoch 2048, loss 0.830422
epoch 2176, loss 0.639396
epoch 2304, loss 0.66702
epoch 2432, loss 0.576337
epoch 2560, loss 0.574195
epoch 2688, loss 0.556343
epoch 2816, loss 0.740426
epoch 2944, loss 0.611631
epoch 3072, loss 0.5794
epoch 3200, loss 0.661029
epoch 3328, loss 0.547054
epoch 3456, loss 0.643172
epoch 3584, loss 0.607397
epoch 3712, loss 0.645322
epoch 3840, loss 0.878216
epoch 3968, loss 0.733658
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0417792 -0.000868078
0.0977799 0.0617552
0.101814 0.0613206
0.0362234 0.0156498
0.0318693 0.0531423
0.0594251 0.0559765
-0.0113401 -0.00151919
0.0989678 0.0611506
0.0716773 0.00167022
-0.014485 -7.87183e-05
-2.99285e-06 0.0100215
0.0168394 0.0549594
0.125863 0.0545335
-0.0860447 0.00230125
0.0301505 0.056888
-0.0857218 -0.000598728
0.0868848 0.0586752
-0.0621343 0.000792117
0.0911183 0.0557811
0.0594427 0.0540809
0.019701 0.0552816
-0.00378521 0.000618514
-0.000792737 0.00252117
0.0248197 0.000275136
0.0593665 0.0537154
0.076422 0.0558203
0.0594374 0.0561367
-0.0207342 -0.00317139
0.00143008 0.00499584
-0.00131975 -0.00148377
0.0188019 0.0604203
-2.95455e-07 0.00917793
5.31452e-07 0.00774838
-0.0621274 -0.00159056
0.056285 0.057422
0.059367 0.0520444
0.0387924 0.0548746
0.0310444 0.0553575
0.0301505 0.0559557
0.0529686 0.00631218
0.0626054 0.0573486
0.0286789 0.0498363
0.0920508 0.055623
-0.0207292 0.00338547
0.0871095 0.0570437
0.0529729 0.00492761
0.0237989 0.0555389
0.036794 0.0500783
0.0813321 0.0553455
0.0594397 0.0574217
-0.0215356 -0.00145473
0.046209 -0.00144178
0.0322983 0.0560353
0.0188609 0.00157603
0.0599031 0.0536712
0.121243 0.0576496
0.000213748 0.061568
0.0191272 0.0546608
0.0207193 0.0541084
0.032933 0.0574263
0.135017 0.0599705
0.0897923 0.0572939
0.0851612 0.0579982
0.0414025 -0.00123366
-0.036223 -7.62e-05
0.0468064 -0.00194279
0.0268195 0.0555013
-0.0385437 0.00615218
0.0317696 0.053677
-0.0247793 0.00589253
-0.0207316 -0.0030534
0.036794 0.0506998
0.0941706 0.0545076
0.0191272 0.0470244
-1.92327e-05 0.00668218
0.0109517 0.00587446
0.0272916 0.00417538
0.0439209 0.0542322
0.0594402 0.0561837
0.0827282 0.00308729
0.0197009 0.0563401
-0.0716686 0.00480046
0.0109517 0.00460519
0.0593665 0.0540323
0.134995 0.0585873
-0.00700272 0.0581375
0.121243 0.0596919
0.0321466 0.0579464
-6.0681e-07 0.00987002
0.0594399 0.0595519
0.0417454 0.00309819
0.0211183 0.0602334
-0.0385427 0.00313328
0.0538015 0.0513975
0.130005 0.0561134
0.0593346 0.0538563
0.0207193 0.0552475
0.0109857 0.00797976
0.0318695 0.0541522
0.0538009 0.0518231
0.0321586 0.0532275
-0.0807993 -0.00127092
0.0753057 0.0552115
-0.0189887 0.0015739
0.0857789 0.0548234
0.106634 0.0557741
-0.0300731 0.00172452
0.130756 0.0586184
0.0594395 0.0546408
0.0217931 0.0609914
0.0867067 0.0549599
1.01848e-06 -0.0017207
0.0968255 0.0575045
0.00931417 0.0571475
0.0321632 0.0566647
0.0594398 0.0574172
-0.0414033 0.00227106
0.124311 0.0522922
5.31452e-07 0.00450251
0.130005 0.0528745
0.0594402 0.0571747
-0.0144856 -0.00131877
0.000213748 0.0607248
0.10664 0.0550081
0.0322982 0.0556255
0.0871095 0.0534759
-0.0234699 0.000754547
0.0860428 0.0059536
parameters: [ 9.449  2.     2.     1.2    4.   ]. error: 256714847.633.
----------------------------
epoch 0, loss 0.98155
epoch 128, loss 0.876318
epoch 256, loss 0.994172
epoch 384, loss 0.813147
epoch 512, loss 0.767685
epoch 640, loss 0.662101
epoch 768, loss 0.776321
epoch 896, loss 0.794799
epoch 1024, loss 0.610585
epoch 1152, loss 0.774912
epoch 1280, loss 0.754633
epoch 1408, loss 0.757508
epoch 1536, loss 0.757632
epoch 1664, loss 0.70936
epoch 1792, loss 0.835992
epoch 1920, loss 0.578109
epoch 2048, loss 0.657105
epoch 2176, loss 0.714872
epoch 2304, loss 0.774778
epoch 2432, loss 0.769395
epoch 2560, loss 0.708477
epoch 2688, loss 0.777839
epoch 2816, loss 0.708622
epoch 2944, loss 0.581047
epoch 3072, loss 0.584774
epoch 3200, loss 0.572101
epoch 3328, loss 0.583062
epoch 3456, loss 0.68889
epoch 3584, loss 0.646702
epoch 3712, loss 0.756931
epoch 3840, loss 0.689186
epoch 3968, loss 0.751181
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0562649 0.0550636
0.0827282 -0.00129307
-0.0234626 -0.00145081
-0.0414059 0.00478509
0.0477959 0.0585017
0.00596546 0.0588553
-0.065961 0.00421421
0.00863032 0.0505374
-0.00236373 0.0573161
0.0106903 -0.00388252
0.0317696 0.0561163
0.0867119 0.0556327
0.101441 0.0543106
-0.0495734 -0.000140133
-1.1614e-09 -0.00255461
0.134985 0.0562112
-0.0807986 -0.00141024
0.0538009 0.0519978
0.0897923 0.0594096
0.0971047 0.0634199
0.0594452 0.0682063
0.059435 0.055423
-0.0272995 0.00625468
0.0927398 0.060098
0.0710835 0.0591584
0.132184 0.0693835
-0.000795203 -0.00330514
0.0910569 0.0604969
0.0527576 0.0580353
0.0317696 0.0579381
0.0927398 0.0585998
-0.0462148 -0.00204503
-0.0194897 0.00472144
0.046209 0.00433908
0.0387924 0.0533518
0.0599031 0.0578285
-0.0318748 -0.0025822
0.0966587 0.0543912
-0.0414059 -0.00341684
-0.0340329 -0.00129801
0.0707072 0.00473616
0.0971047 0.0618189
0.0594397 0.0586687
1.01848e-06 0.00698642
0.097887 0.0626188
0.109566 0.0602311
-0.0248092 0.00830109
0.0599612 0.0522611
0.0950804 0.0591051
-0.0161248 0.0540732
0.0247806 0.00750951
0.0205116 0.0506024
0.0390889 -0.00347373
-0.0734361 0.00437662
-0.0662843 0.000126631
0.0318787 0.00894031
0.0927399 0.0614506
0.0691986 0.0596332
6.39156e-06 0.00737253
-0.0215356 -0.00100557
-0.000835392 -0.00279292
-0.0271111 0.00639781
0.081797 0.0542232
0.0373484 0.0541727
0.0247829 -0.00280761
0.0277564 0.0547422
0.0594396 0.0596139
0.106634 0.0559687
-0.0462076 0.00529978
0.0188019 0.0607207
0.0910567 0.0585208
0.0170933 0.0608551
0.0800819 0.0550569
2.5668e-06 -0.00353025
0.0461385 0.0574691
0.0211183 0.0608533
-0.0716686 -0.000854383
0.0857794 0.053728
0.129453 -0.00245732
0.0170834 0.0602585
-0.0131851 0.0061229
0.0290868 0.0576501
0.0594452 0.0677672
-0.0267352 -0.00376006
0.0496682 0.0578041
-0.00726943 0.0481784
0.0907408 0.0583106
0.0323166 -0.000325999
1.01848e-06 0.0035653
2.05834e-07 -0.00252204
0.100097 0.0627484
0.0624794 0.0503951
0.0691985 0.0582029
0.0710837 0.0593834
0.0911183 0.0537245
0.0367961 -0.00188655
0.0594397 0.0590885
0.0594396 0.0586
0.129445 0.00933542
-0.0857226 -9.62719e-05
-0.0272977 0.00422251
0.000835126 0.00789056
-0.0390898 0.000475396
-0.039089 0.00186095
0.105506 0.0681349
0.0234638 0.00464971
0.0496681 0.0579579
0.106624 0.0556234
-0.0417488 0.000598253
0.0920969 0.0560728
0.0594301 0.0546734
-0.041769 0.00559593
0.00932522 0.0501238
-0.0271121 -0.00456232
0.0594027 0.0498685
4.07333e-06 0.00698035
-0.0209824 -0.00129968
2.96104e-08 -0.00278562
0.101814 0.063088
-0.0189848 -0.00253868
0.028139 0.0582258
0.0321466 0.0583403
0.0204045 -0.00238413
0.0281393 0.0581911
0.0859466 0.0596406
0.0594398 0.0589984
0.00863092 0.0523656
-0.0144843 0.00359857
parameters: [ 9.449  2.618  2.     1.2    4.   ]. error: 7.17552368783e+13.
----------------------------
epoch 0, loss 1.71323
epoch 128, loss 0.884764
epoch 256, loss 0.752984
epoch 384, loss 0.651724
epoch 512, loss 0.705814
epoch 640, loss 0.738074
epoch 768, loss 0.76986
epoch 896, loss 0.762397
epoch 1024, loss 0.66815
epoch 1152, loss 0.560177
epoch 1280, loss 0.509866
epoch 1408, loss 0.780636
epoch 1536, loss 0.585249
epoch 1664, loss 0.608172
epoch 1792, loss 0.698242
epoch 1920, loss 0.741473
epoch 2048, loss 0.750037
epoch 2176, loss 0.667937
epoch 2304, loss 0.636464
epoch 2432, loss 0.627854
epoch 2560, loss 0.499264
epoch 2688, loss 0.595176
epoch 2816, loss 0.74281
epoch 2944, loss 0.560919
epoch 3072, loss 0.537448
epoch 3200, loss 0.613946
epoch 3328, loss 0.62128
epoch 3456, loss 0.6783
epoch 3584, loss 0.646002
epoch 3712, loss 0.570103
epoch 3840, loss 0.623972
epoch 3968, loss 0.732583
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.12614 0.0563422
-0.0611267 -0.00131693
0.0134334 0.0666394
0.0594482 0.0673924
-0.000801651 0.0029965
0.0594396 0.0599143
-0.0417422 -0.000397041
0.0362234 0.000849211
-0.000786289 0.0123783
-0.0611171 -0.000557318
0.0594162 0.0533942
0.0625951 0.0603654
0.0966587 0.0574219
0.0594424 0.0527609
0.0194981 0.00779837
0.0340225 0.00999309
0.0611271 0.00320339
0.0209719 0.00873801
-2.64683e-07 0.0111531
0.0271109 0.00803007
0.0220538 0.059551
0.0594396 0.0600981
0.0870053 0.0560987
0.0464004 0.0581036
-7.68032e-07 0.00308514
0.0691072 0.0553825
0.0594399 0.0551968
0.0813321 0.0564994
0.0978866 0.061404
0.0662934 -0.00266451
-0.0131845 0.00969825
0.0272988 0.00226266
-0.0416401 0.00119578
0.0113491 0.00049428
0.0237988 0.0619861
0.0385434 0.0118305
0.121243 0.0603855
-0.0144713 0.0112389
0.0223972 0.0571847
0.0204045 0.00956842
0.0461385 0.0586727
0.0106903 -0.000227559
-0.036223 0.00191179
0.126206 0.0532428
0.0813321 0.0573635
0.0594325 0.0530319
-0.0412711 0.0116893
0.0813321 0.0580117
0.000793148 0.00750529
0.0941706 0.0564028
0.043921 0.0586473
0.0321467 0.0574698
0.0317699 0.0591046
0.088729 0.060144
0.0529666 0.00915325
-0.0177216 0.00951756
0.0739226 0.0590957
0.129916 0.0525497
0.0781227 0.0537506
0.080382 0.0545421
0.0532535 0.0522802
0.000782366 0.0120024
0.0599612 0.0554552
0.0194981 0.011269
0.0435639 0.0542509
0.0867067 0.0566382
0.0594424 0.052705
0.022444 0.0563872
0.0189892 0.00963829
0.0667826 0.0605047
0.0710837 0.0590695
0.0626054 0.0561024
0.0870049 0.0561303
0.101814 0.0621471
0.0194981 0.00143893
0.0594399 0.0536143
0.0201879 0.0565944
-0.0118764 0.0602128
-0.0131858 0.00158851
-0.0734361 0.00914104
-0.0394792 0.0100682
-0.0734426 0.00542575
-0.0131858 0.00376729
-0.0204001 -0.000902875
0.0278229 0.0602106
0.0859465 0.0612334
-0.0394792 0.0080965
3.5013e-07 0.0104626
-1.1614e-09 0.000653461
-0.0131854 0.0106106
0.102034 0.0569607
0.0594397 0.060876
0.018802 -0.000920142
0.0385437 0.0123433
0.062595 0.0599012
0.0152616 0.00103206
0.0333261 0.059285
0.0701141 0.0548981
0.0594397 0.0591696
0.0290868 0.059436
0.0494968 0.00157582
0.0593312 0.0526744
0.0855721 0.0606068
0.0764481 0.0592606
-0.0106908 -0.000869434
0.0477956 0.0591461
0.0710838 0.0620959
0.028139 0.057754
0.0204045 0.00610898
0.0318787 0.0162213
0.01684 0.0543629
0.109566 0.06078
0.00459996 -0.000860515
0.0594396 0.0595855
-0.00737805 0.0527144
-0.0412685 0.0106039
0.0318693 0.0532814
-1.48505e-07 0.00281641
0.0594494 0.0658698
0.125873 0.0571873
-0.00484737 0.0508269
-0.00700262 0.0524788
0.0417489 -0.00144943
-0.0707062 0.00506285
0.0340225 0.00662983
0.0261392 0.0604012
-0.0188062 -0.0025249
0.015257 0.00140626
parameters: [ 9.449  1.618  2.     1.2    4.   ]. error: 1543568570.85.
----------------------------
epoch 0, loss 1.15991
epoch 128, loss 1.07045
epoch 256, loss 0.709132
epoch 384, loss 0.65536
epoch 512, loss 0.813394
epoch 640, loss 0.653088
epoch 768, loss 0.551574
epoch 896, loss 0.652134
epoch 1024, loss 0.769917
epoch 1152, loss 0.737489
epoch 1280, loss 0.622379
epoch 1408, loss 0.737423
epoch 1536, loss 0.596925
epoch 1664, loss 0.743324
epoch 1792, loss 0.855283
epoch 1920, loss 0.712545
epoch 2048, loss 0.725145
epoch 2176, loss 0.751673
epoch 2304, loss 0.621096
epoch 2432, loss 0.553486
epoch 2560, loss 0.672842
epoch 2688, loss 0.717304
epoch 2816, loss 0.769841
epoch 2944, loss 0.69603
epoch 3072, loss 0.645692
epoch 3200, loss 0.819106
epoch 3328, loss 0.699067
epoch 3456, loss 0.553634
epoch 3584, loss 0.622007
epoch 3712, loss 0.838148
epoch 3840, loss 0.804479
epoch 3968, loss 0.647714
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0496682 0.0549573
-0.0631101 -0.00388123
0.13219 0.0673703
0.0706156 0.0554286
0.0131851 -0.00205713
-0.034636 0.00220312
0.0243594 0.0475059
-0.00331175 0.0513932
1.13647e-06 0.00984677
0.0594395 0.0526247
0.125157 0.0565124
0.0197009 0.0531305
0.0904958 0.0559405
0.038792 0.0497041
0.0177241 0.00827123
0.0278228 0.0564979
0.028139 0.0542606
-0.000835392 0.00920726
0.0177216 -0.00245133
0.0220539 0.051374
0.022215 0.0524172
6.39156e-06 -0.00235022
1.95996e-07 -0.00215495
0.0278228 0.054663
6.39156e-06 0.00890854
0.038792 0.0519906
0.0904958 0.0553246
0.097887 0.0613028
-0.0827278 0.00752108
-0.053266 0.00993937
-0.0152583 0.00839885
0.0800821 0.0519122
0.0865811 0.0598356
0.0362233 0.0088333
0.0417629 0.00920807
0.0495768 0.00361356
0.121243 0.060801
-0.0118768 0.0574646
0.0286789 0.054311
-0.0113373 -0.00164718
0.0144707 -0.00199953
0.0396777 0.0522252
0.028139 0.0570523
0.028139 0.0555402
0.000834782 0.00921103
0.0907409 0.0539855
0.0131849 0.00995701
0.0594251 0.0553608
0.0662934 0.00249889
-0.0188606 0.00977066
0.00931417 0.0559197
0.0870049 0.0566854
-0.000805986 0.00976762
0.126171 0.0565166
0.0691985 0.0531548
-0.0247813 -0.00259989
0.0857789 0.0514366
0.0435741 0.0562473
0.0497673 0.0548136
0.0461385 0.0571365
0.0527575 0.0558374
0.0416434 -0.00076815
0.0631173 -0.00329816
0.0983621 0.0553745
2.05834e-07 -0.00220414
0.0318695 0.0538346
0.0907408 0.059012
0.0467944 -0.00224448
-0.0118768 0.0574646
-0.0188605 -0.00174703
0.0964766 0.0538617
0.0468064 -0.00189313
0.0776855 -0.00189513
-0.0295746 0.00627359
0.0261391 0.0566782
0.0384676 0.0570608
0.00891968 -0.00115538
0.12945 -0.00135932
0.0261392 0.0554978
0.00894003 0.00896977
0.01684 0.0520628
0.0318695 0.0540156
0.0997462 0.0532729
0.0477959 0.0577587
0.0878299 0.0534098
0.0594395 0.0591263
0.0286795 0.053369
0.0407515 0.0536928
-0.0023637 0.0517236
-0.0394792 -0.00195717
-0.0385431 0.00963175
0.0496681 0.0550344
1.13647e-06 -0.00217685
0.0950805 0.0540225
-7.68032e-07 0.00880773
1.95996e-07 0.0105511
-0.00728242 0.0536823
0.0417489 0.00401764
0.0859465 0.0569758
-0.0023636 0.058703
0.0594402 0.055055
-0.0188023 -0.0044226
0.0268142 0.054086
0.0106905 0.00285948
0.0752321 0.0560065
-0.0631101 0.00122617
0.0594397 0.0585683
0.0261392 0.053756
0.0857788 0.0513164
-0.0611171 0.00179917
0.0417702 0.00913884
-0.000795203 0.0083925
0.0594397 0.0534314
0.0327313 0.0566718
0.0594397 0.055627
0.0594396 0.0531775
-0.0271111 -0.00170039
0.0295843 0.00809467
0.05937 0.0563878
0.074953 0.0505973
0.00080867 -0.00219097
0.0871096 0.055844
0.0117785 -0.00411009
-0.0394796 0.00780235
-0.00378521 -0.00252248
0.0867333 0.0580388
0.062595 0.05723
0.085161 0.0575008
parameters: [ 9.449  1.809  2.     1.2    4.   ]. error: 6724065534.44.
----------------------------
epoch 0, loss 0.88147
epoch 128, loss 0.702956
epoch 256, loss 0.879091
epoch 384, loss 0.791574
epoch 512, loss 0.699368
epoch 640, loss 0.81172
epoch 768, loss 0.730768
epoch 896, loss 0.77002
epoch 1024, loss 0.590002
epoch 1152, loss 0.553641
epoch 1280, loss 0.668128
epoch 1408, loss 0.61951
epoch 1536, loss 0.77148
epoch 1664, loss 0.753702
epoch 1792, loss 0.813492
epoch 1920, loss 0.690468
epoch 2048, loss 0.697985
epoch 2176, loss 0.551884
epoch 2304, loss 0.701123
epoch 2432, loss 0.679379
epoch 2560, loss 0.733867
epoch 2688, loss 0.560034
epoch 2816, loss 0.565228
epoch 2944, loss 0.694085
epoch 3072, loss 0.71081
epoch 3200, loss 0.604433
epoch 3328, loss 0.763161
epoch 3456, loss 0.654022
epoch 3584, loss 0.678704
epoch 3712, loss 0.607736
epoch 3840, loss 0.469796
epoch 3968, loss 0.73295
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0385427 -0.00278439
0.0390889 -0.0066156
-0.129449 -0.00338118
0.0964305 0.0577516
0.0562851 0.0588028
0.125146 0.0619394
0.0152545 -0.00457721
0.0803921 0.0482193
0.13219 0.0718371
0.100115 0.0012499
0.100115 -0.00592031
0.0301504 0.0591179
-0.0417624 0.00243834
0.118684 0.0656955
0.0461382 0.0624301
0.0243594 0.0509739
0.0317698 0.0618598
0.0764328 0.0529552
0.0857795 0.0590623
0.00894166 0.0109148
-0.0462148 0.0077455
-3.40966e-05 -0.00385252
0.124277 0.0638045
0.0407515 0.0520093
0.0248097 0.00554834
-0.065961 -0.00460588
0.121243 0.060983
0.132184 0.0721517
0.109566 0.0621927
0.0621334 0.000943974
1.74905e-07 -0.00373608
0.0387924 0.0536262
0.0945153 0.0528574
0.0037846 0.00478874
0.0593665 0.059732
0.0261394 0.0617616
-0.0131851 -0.00550143
0.0482639 0.0624747
0.0208245 -0.00464018
0.0117782 -0.00665244
0.0791961 0.0486103
0.0477959 0.0639201
0.0707066 -0.00329264
0.100097 0.0630518
0.0144859 -0.00336103
0.125863 0.0535454
0.0937752 0.0629588
0.105471 0.0700714
0.074953 0.0519035
0.0867199 0.0595757
0.0462066 0.00437611
0.135027 0.0542311
0.0330953 0.0512927
-0.0023636 0.0621381
0.0599031 0.0612522
0.0205126 0.047621
0.0857795 0.0566012
0.0734322 -0.00517004
-0.053257 0.00379915
-0.0144856 -0.00345511
0.029557 0.00589317
0.0593312 0.055402
0.0859467 0.0634976
0.0458116 0.0563515
0.0594919 0.0610358
0.0691985 0.0613137
0.106619 0.0531696
-0.0394796 -0.00434488
0.0691986 0.0621646
0.0424369 0.0516753
0.100116 0.00112579
-0.0144713 -0.00414488
0.0131861 -0.00459169
0.0857284 -0.000583946
-0.0247793 0.00463981
0.0205126 0.0483073
-0.0394792 0.00663826
0.0295774 -0.00349466
0.0188023 -0.00652873
0.0707066 -0.0027747
0.13219 0.0720887
0.0412776 0.00767167
0.0495768 -0.00567207
0.0204045 0.00308662
0.0326439 0.00125344
0.124277 0.0632062
0.0271117 -0.00030022
0.0417583 -0.00368836
0.0977799 0.0624829
0.0497672 0.0542127
-0.0295823 0.00502915
0.0631173 -0.00681735
0.0977799 0.061776
-0.00460347 0.000558038
-0.0272904 -0.00424187
0.0204045 0.00808686
0.0859466 0.0594313
0.01134 0.00399939
0.059181 0.0600448
0.0904958 0.0528482
-0.0323135 -4.08209e-05
0.0870049 0.0566138
0.0800819 0.0571609
0.0487559 0.0530816
-0.0417792 0.00545214
0.0662803 0.0019653
0.0594426 0.0549246
0.0716719 -0.00558199
0.0589764 0.0596003
0.0920508 0.0529723
0.0211031 0.00577593
-0.000793636 0.00713076
0.0281392 0.0566084
0.0667826 0.0616247
-0.0056768 0.0507755
0.0643397 0.0586151
0.0871095 0.0616554
-0.027288 0.00641015
0.00460075 -0.00591295
0.0716719 0.00183248
-4.71368e-07 0.00602923
-0.0734312 -0.00605631
0.0910567 0.0604554
0.0405236 0.0630264
0.0971043 0.0653231
-0.0716682 -0.000251289
0.0496681 0.060984
-0.0118764 0.0466889
parameters: [ 9.449  2.236  2.     1.2    4.   ]. error: 177656.896093.
----------------------------
epoch 0, loss 1.05937
epoch 128, loss 0.918603
epoch 256, loss 0.759997
epoch 384, loss 0.634382
epoch 512, loss 0.588204
epoch 640, loss 0.630767
epoch 768, loss 0.803115
epoch 896, loss 0.670462
epoch 1024, loss 0.672441
epoch 1152, loss 0.713898
epoch 1280, loss 0.641837
epoch 1408, loss 0.675878
epoch 1536, loss 0.714917
epoch 1664, loss 0.620709
epoch 1792, loss 0.5902
epoch 1920, loss 0.752524
epoch 2048, loss 0.647473
epoch 2176, loss 0.785865
epoch 2304, loss 0.725682
epoch 2432, loss 0.745195
epoch 2560, loss 0.701113
epoch 2688, loss 0.650399
epoch 2816, loss 0.814306
epoch 2944, loss 0.896258
epoch 3072, loss 0.581543
epoch 3200, loss 0.736654
epoch 3328, loss 0.827037
epoch 3456, loss 0.679676
epoch 3584, loss 0.59586
epoch 3712, loss 0.697281
epoch 3840, loss 0.59381
epoch 3968, loss 0.692143
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0321468 0.0535204
0.0582389 0.0541613
-0.0631101 0.0028493
0.125157 0.0593555
0.079197 0.0506289
-0.000835063 0.00956477
0.0968257 0.0606433
-0.0267425 0.00290206
2.27454e-05 0.00114169
0.0267779 0.0525254
0.0449639 0.00523059
0.0117845 0.00896086
2.41162e-06 0.0010083
-5.97989e-06 0.00944919
6.7959e-07 0.0116282
0.0122504 0.056551
0.0140549 0.0518404
-0.0110929 0.045465
0.0207304 0.00722354
0.0109586 0.000718401
0.0458117 0.0630163
0.0626054 0.0527007
-0.0111821 0.0435598
0.0210958 0.0106284
0.0300726 0.00128671
0.0691986 0.0560238
0.0477959 0.0534149
0.0815268 0.0540695
0.0211031 0.00356855
0.0626002 0.052721
0.0594396 0.0600762
0.0626054 0.056403
0.0605996 0.0540144
0.0326439 0.00648707
0.0594398 0.0608817
-0.0271157 0.00867056
0.100097 0.0590774
0.0477957 0.0560558
0.104819 0.0559079
0.0822309 0.0558484
0.0387924 0.0564919
0.0594374 0.0578727
-1.17339e-07 0.00187407
-0.0734384 0.00762289
-0.0189848 0.0136322
-0.0662833 0.00272041
0.0188086 0.000457727
0.0594396 0.0551196
-0.0385433 0.000621774
0.0458117 0.0521107
0.0710837 0.0568717
0.0199307 0.0582203
0.0220539 0.0606171
-0.0611272 0.00168929
-0.0495734 0.00123173
-0.00737805 0.0480378
-0.02083 0.000137326
0.132178 0.064305
0.0390895 0.00123715
0.130756 0.0648538
0.0251233 0.060865
0.0131845 0.0126277
0.076438 0.0549671
0.0964307 0.0499237
-0.0295823 0.0102566
-0.0177194 0.00133789
0.0211183 0.0601887
0.0950805 0.0615587
0.0594397 0.0597029
0.0710835 0.0625273
-0.000835377 0.00275718
0.0385291 0.01521
0.080402 0.0542614
0.0818743 0.0501627
0.126169 0.0556579
0.0927397 0.0622576
0.0871098 0.053459
0.0286789 0.0462238
-0.0495734 0.00372984
-0.0161248 0.0533123
0.0191272 0.0547879
0.124354 0.050278
-0.000781954 0.0120399
0.10546 0.0627704
0.0106903 0.00778615
-0.0271153 0.00404795
-0.02083 0.0015816
0.0236481 0.0607769
-0.00733491 0.0550761
0.0220537 0.060436
0.0458117 0.0621097
-0.0271121 0.00810545
0.0305919 0.0565098
0.0871098 0.0586809
0.0621302 0.00809657
0.0594396 0.0585809
0.0281391 0.0566135
0.130005 0.0561469
0.0199309 0.0576285
0.0346452 0.00528484
0.0261393 0.0589051
0.0199307 0.057807
0.09274 0.0555579
0.00931417 0.0588563
0.0482639 0.0576778
0.0174335 0.0556927
-0.00737805 0.050434
0.125157 0.0614318
-0.0631101 0.000427128
0.0625951 0.0552054
0.0321468 0.0580273
-0.0204101 0.00165109
0.0594397 0.0623624
0.0594397 0.0573566
0.0370769 0.052815
0.0606162 0.0518557
0.0707062 0.00366316
0.0710835 0.0629859
-0.0707068 0.00353754
0.0532557 0.0155355
-0.0234699 0.00100237
0.0093143 0.05309
-0.014471 0.0140729
0.0527575 0.0610896
1.13647e-06 0.00011338
0.0920508 0.0543883
0.0594399 0.0504211
0.000782366 0.001317
parameters: [ 9.449  2.265  2.     1.2    4.   ]. error: 382966329.422.
----------------------------
epoch 0, loss 1.24416
epoch 128, loss 0.799843
epoch 256, loss 0.940297
epoch 384, loss 0.663853
epoch 512, loss 0.749035
epoch 640, loss 0.706887
epoch 768, loss 0.768957
epoch 896, loss 0.608149
epoch 1024, loss 0.600148
epoch 1152, loss 0.506317
epoch 1280, loss 0.584704
epoch 1408, loss 0.794872
epoch 1536, loss 0.6335
epoch 1664, loss 0.569528
epoch 1792, loss 0.791053
epoch 1920, loss 0.67444
epoch 2048, loss 0.642258
epoch 2176, loss 0.736236
epoch 2304, loss 0.547085
epoch 2432, loss 0.529956
epoch 2560, loss 0.684638
epoch 2688, loss 0.56616
epoch 2816, loss 0.599584
epoch 2944, loss 0.69606
epoch 3072, loss 0.653661
epoch 3200, loss 0.661128
epoch 3328, loss 0.689759
epoch 3456, loss 0.743637
epoch 3584, loss 0.658977
epoch 3712, loss 0.540041
epoch 3840, loss 0.466757
epoch 3968, loss 0.451865
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0870049 0.0522764
0.0317699 0.0443104
3.36628e-05 -0.00419596
0.0866747 0.0586441
-0.0346352 -0.00511014
0.0322983 0.0411096
0.0867333 0.0526037
0.0301505 0.0404644
0.125873 0.0659421
0.0661223 0.0464416
0.0532657 0.00469899
-2.99285e-06 -0.00334447
0.0871098 0.0516297
0.0764272 0.0597292
0.0968255 0.0582594
-0.0621272 -0.000116616
-0.0414014 -0.000143512
0.0189868 -0.00451718
0.0416434 -0.00346284
-0.00894125 0.00664562
0.0268195 0.0513493
0.121243 0.0566318
0.0417629 -0.00477942
0.079197 0.0465863
-0.0132965 0.0830459
0.124311 0.0689385
0.0594398 0.0497193
0.0417583 0.00294171
0.0904958 0.0489835
0.00931426 0.0368958
0.0207377 0.00440865
-0.0117817 -0.000521622
0.000782366 0.00552282
-0.129445 -0.00434784
0.0599031 0.0465219
0.0207193 0.0423015
0.0261393 0.0386051
0.110244 0.0504582
0.0208321 0.00870094
-0.0631101 -0.00211631
0.032902 0.0717543
0.0901952 0.0513807
0.0631163 -0.00425663
0.0716773 -0.00460531
0.0710838 0.0536763
0.0122243 0.0535637
0.0424316 0.0572161
0.0301504 0.0423845
0.090741 0.0529972
0.032902 0.0710997
0.0237989 0.0430759
0.0920613 0.0616309
0.0211183 0.0699386
0.0368959 0.0566555
0.0964766 0.0537166
-0.0248069 0.00689147
0.0247806 -0.00436075
0.081527 0.0489543
0.0197008 0.0429215
0.124354 0.0677043
0.0412752 -0.00388257
0.0791961 0.0525438
0.101441 0.0541525
0.0295843 0.00482895
0.0594398 0.0485894
0.0781223 0.0503551
0.121243 0.0517337
0.0458116 0.0465543
0.0133907 0.088089
-0.0340213 0.00259173
0.0867201 0.0512911
0.0865812 0.0515924
-0.0462169 -0.00415043
0.12432 0.0643995
0.0815268 0.0501509
-0.063123 -0.000555101
0.0589765 0.0464768
0.0362233 0.00322971
0.0122404 0.0536587
0.100668 0.047341
0.0594759 0.0585876
0.0691068 0.0486628
0.0191272 0.0409248
-0.0449599 0.00275621
0.0278231 0.0397725
0.0272988 -0.00439648
0.00250079 -0.0047119
0.0482639 0.0484132
0.0626106 0.0570141
0.014484 -0.00331807
-0.0272977 0.00666391
0.000796249 0.00650177
0.0626106 0.0578292
-0.0532547 0.00197741
0.00131924 -0.0050446
0.0594397 0.047782
0.0477957 0.0492376
0.0109781 0.00403911
0.0716711 -0.000792424
0.0321467 0.046336
0.026809 0.0549094
0.0278229 0.043971
-0.0109842 0.00252846
0.0373475 0.0440567
0.0532535 0.0589474
0.0594398 0.0513118
0.0131849 -0.00359247
0.125873 0.0639196
0.0327313 0.043194
0.0532578 -0.00421609
0.0605996 0.0574903
0.0791268 0.0513012
0.0968257 0.0584896
0.129925 0.0672576
0.0117785 -0.00474637
0.00863092 0.040083
0.0691987 0.0512869
-0.0118764 0.0329312
0.0594398 0.0494956
0.102034 0.0540247
0.0373475 0.0476945
0.0329332 0.0433969
0.0368964 0.0565256
-0.0188057 -0.00461541
-0.0412731 0.00639397
0.0791268 0.0496867
0.0317697 0.0405178
0.0113491 0.00411408
parameters: [ 9.449  2.128  2.     1.2    4.   ]. error: 482961.609507.
----------------------------
epoch 0, loss 1.62399
epoch 128, loss 0.76344
epoch 256, loss 0.858753
epoch 384, loss 0.785171
epoch 512, loss 0.646912
epoch 640, loss 0.695683
epoch 768, loss 0.763871
epoch 896, loss 0.558345
epoch 1024, loss 0.70706
epoch 1152, loss 0.55912
epoch 1280, loss 0.684428
epoch 1408, loss 0.716384
epoch 1536, loss 0.568968
epoch 1664, loss 0.728783
epoch 1792, loss 0.621351
epoch 1920, loss 0.693161
epoch 2048, loss 0.623133
epoch 2176, loss 0.705609
epoch 2304, loss 0.681036
epoch 2432, loss 0.780676
epoch 2560, loss 0.728486
epoch 2688, loss 0.803992
epoch 2816, loss 0.601384
epoch 2944, loss 0.725225
epoch 3072, loss 0.586828
epoch 3200, loss 0.632352
epoch 3328, loss 0.691348
epoch 3456, loss 0.560515
epoch 3584, loss 0.605897
epoch 3712, loss 0.557129
epoch 3840, loss 0.605965
epoch 3968, loss 0.604299
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0710835 0.0595898
0.0599612 0.0570192
0.0532657 -0.000378247
-0.0621274 0.00662399
-0.0113373 -0.000641837
-0.000781954 0.0138588
0.0589764 0.0587255
-0.0272904 0.00324908
0.126222 0.0568487
0.088282 0.0547804
-0.000805986 0.00857185
0.126169 0.0543995
0.0964772 0.0590243
0.0495801 0.00356259
0.0815268 0.0589732
0.081797 0.0541779
0.0594396 0.0577996
0.0865811 0.058355
0.0730675 0.0614968
0.01684 0.0583932
0.0168394 0.0544254
0.0861486 0.0600438
0.109566 0.0574966
0.0477959 0.0558416
-0.0023637 0.0616168
0.0676335 0.0557232
-0.10011 0.00176941
-0.0188057 0.00457888
0.0594452 0.0661921
0.00460075 0.00363311
-0.000835392 0.012696
0.0330542 0.053924
0.0268142 0.0569823
0.0592247 0.0544467
0.0621302 -0.00150952
0.130756 0.0558241
0.0373475 0.0575124
0.0599031 0.0611209
0.0205126 0.0525603
0.0582589 0.0547079
-0.0152583 -0.000148284
0.0223972 0.0585021
0.0857788 0.0572444
0.0464005 0.0561598
0.05943 0.0595153
0.0861487 0.0582233
0.130756 0.0547507
0.0594399 0.0593452
0.0594397 0.0602147
0.00143008 0.00713692
0.0851511 0.0549505
-0.0271126 -0.00300451
0.0605996 0.0547473
0.014471 0.000401848
0.0983621 0.0590132
0.0482639 0.0575502
0.0807993 0.012643
-0.041769 0.00503798
0.0910567 0.0605052
0.0859467 0.0578625
0.0220539 0.0586996
0.0414025 0.00963885
-0.0529722 0.00132291
-0.0662843 2.59847e-05
0.0594116 0.0544433
-3.17974e-05 0.00214176
0.0414051 -7.60249e-05
0.028139 0.0604311
0.0791969 0.0577518
0.0319943 0.0618387
0.0966596 0.0552637
6.39156e-06 0.00463408
-0.00700272 0.0594349
0.0217931 0.0617005
-0.00628221 0.0529806
0.109566 0.0610855
4.07333e-06 -0.000678136
0.0643397 0.0550833
0.0189892 0.00195851
0.0188016 0.0620887
0.0174339 0.0544925
0.10664 0.0587952
0.0882822 0.0560598
0.0131848 0.00525437
0.12945 0.00722822
-0.0109489 0.000838128
0.0791961 0.0564061
0.0220537 0.0566918
0.0321466 0.0583643
-0.0776845 0.00697742
0.0207193 0.0570722
0.0878297 0.0576634
0.0977801 0.0623755
0.0417454 -0.00295342
-0.0106876 -0.00228132
-0.0131858 0.0083866
0.0977799 0.0618389
-0.0271152 -0.000580146
-0.0188057 -0.00178856
-0.0621346 0.00377156
0.0706155 0.0592348
0.0188603 0.00366461
0.109566 0.0584534
0.0131845 0.000457295
0.0340225 0.00487869
0.0189852 0.00489586
0.0853245 0.0588087
0.0286795 0.0545526
0.0191272 0.0550303
0.0800821 0.0562692
0.059435 0.0589381
0.0204028 0.0145655
0.0199309 0.0622966
0.0594427 0.0565869
0.0335492 0.0578167
-0.0529698 0.00212602
0.0920613 0.0581431
0.0436381 0.0573207
0.0594399 0.0579456
0.0109586 0.0146383
0.0458117 0.0599334
-0.0385434 0.00329341
0.121243 0.0610151
0.0329331 0.0570031
0.0815268 0.0559342
0.0174339 0.0555907
0.0904958 0.057589
0.0532557 0.0106033
parameters: [ 9.449  2.195  2.     1.2    4.   ]. error: 142618.526945.
----------------------------
epoch 0, loss 1.15361
epoch 128, loss 0.757076
epoch 256, loss 0.604148
epoch 384, loss 0.670962
epoch 512, loss 0.702771
epoch 640, loss 0.785787
epoch 768, loss 0.772967
epoch 896, loss 0.820277
epoch 1024, loss 0.697673
epoch 1152, loss 0.773679
epoch 1280, loss 0.642778
epoch 1408, loss 0.831641
epoch 1536, loss 0.532003
epoch 1664, loss 0.82785
epoch 1792, loss 0.666436
epoch 1920, loss 0.871585
epoch 2048, loss 0.672836
epoch 2176, loss 0.831375
epoch 2304, loss 0.629558
epoch 2432, loss 0.736326
epoch 2560, loss 0.702927
epoch 2688, loss 0.540288
epoch 2816, loss 0.470248
epoch 2944, loss 0.559179
epoch 3072, loss 0.755715
epoch 3200, loss 0.664878
epoch 3328, loss 0.66635
epoch 3456, loss 0.528944
epoch 3584, loss 0.57198
epoch 3712, loss 0.701559
epoch 3840, loss 0.521489
epoch 3968, loss 0.616866
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0321467 0.0578287
-0.0144856 -0.00223974
0.0783558 0.0656797
0.0867333 0.0595948
0.0868849 0.0632645
0.0495801 0.00377838
0.0251233 0.0540843
0.0860428 0.0145038
-0.00968441 0.0422214
0.0439209 0.0479839
0.0853255 0.0533907
0.0710835 0.0626349
0.0594397 0.0613838
0.0851613 0.0669059
3.5013e-07 -0.00115042
-0.0417573 0.0112022
0.0867334 0.0611395
0.0281392 0.0643988
0.0859467 0.0601012
0.000834798 -0.00239536
0.0589127 0.0545009
-0.0323135 0.0021932
0.0941706 0.0572294
0.0131845 -0.00192097
0.0920613 0.0468738
-2.86114e-05 0.00532685
0.0134225 0.0637199
0.0286795 0.0558906
0.0461382 0.0632772
0.0594324 0.0540573
0.105506 0.0675665
-0.000781954 -0.00153831
0.0458117 0.0583449
-0.013282 0.0600715
-0.0346424 -0.00427303
0.028139 0.0600119
-0.0111732 0.0414412
0.0449751 0.0556103
-0.0716686 -0.00376321
0.043921 0.0500658
-0.0495737 -0.00341727
0.0335493 0.041302
-0.0414014 -0.00217217
0.0625949 0.0609484
0.0562649 0.0542469
0.0631196 0.00384415
0.101441 0.0571311
-0.0152533 0.0106355
-0.0204101 -0.00272974
0.0878299 0.0547611
-0.0734312 -0.00287124
-0.0161572 0.0443584
0.0871098 0.0608523
0.0346348 -0.00294076
-3.11149e-07 -0.00149796
0.0209836 -0.00232154
0.0248197 -0.00266041
-0.0211014 -0.00162295
0.0710835 0.060995
0.0268142 0.0515704
-0.0271157 -0.00399659
0.0424369 0.0461701
0.0594376 0.0542008
0.104819 0.0519331
0.0449639 -0.0011023
-0.0117817 0.00219696
0.0337182 0.054665
-0.0234677 0.00796907
-0.0860431 0.0113356
0.0182077 0.0468698
0.019701 0.0578248
-0.0152583 0.00797878
0.0321586 0.0599322
-0.0117783 -0.00427135
0.0217931 0.0525474
0.09274 0.0616894
-0.0326435 -0.00391343
-0.00893962 -0.00206859
-0.0716787 0.00262618
0.0286795 0.049656
-0.0417488 -0.00346861
-0.0295559 0.00895201
0.0927397 0.0673113
0.021525 -0.00090124
-0.0234677 0.011613
0.0267369 -0.0015431
0.0870049 0.0525931
0.0416418 -0.000987685
0.0290868 0.0629827
0.0964772 0.053207
0.0897923 0.0589925
-0.0631101 0.000951704
0.0396776 0.0563458
0.014484 -0.00160156
0.0496682 0.0600878
-0.00894944 -0.00200818
0.0822309 0.0456788
0.0706156 0.0626413
0.0494968 -0.000698662
0.0625949 0.0649764
0.00131949 -0.00333936
0.0730674 0.062599
-0.036223 0.0102522
0.000213351 0.0558817
0.0592543 0.0469054
0.0730675 0.0592344
0.0589765 0.0631102
-0.0111018 0.051731
0.0716719 -0.00223452
0.0983621 0.0508884
0.046799 -0.00122228
0.000796249 0.0121778
0.0594481 0.0626471
0.0964766 0.058491
0.0385437 -0.00108344
0.076422 0.0541564
0.13219 0.0679962
0.0945153 0.0590625
0.0650734 0.0553816
0.0538297 0.0496938
0.0144704 0.00841888
0.0594325 0.0491728
-0.0449599 0.00972371
0.0594398 0.0600425
0.0867335 0.0640724
0.0910569 0.0653389
-0.0662843 0.00418245
0.0582236 0.0496492
parameters: [ 9.449  2.208  2.     1.2    4.   ]. error: 16081712.4188.
----------------------------
epoch 0, loss 0.925604
epoch 128, loss 0.984623
epoch 256, loss 0.775335
epoch 384, loss 0.837646
epoch 512, loss 0.699288
epoch 640, loss 0.817707
epoch 768, loss 0.795621
epoch 896, loss 0.81176
epoch 1024, loss 0.501974
epoch 1152, loss 0.74571
epoch 1280, loss 0.64371
epoch 1408, loss 0.854172
epoch 1536, loss 0.740767
epoch 1664, loss 0.596654
epoch 1792, loss 0.794369
epoch 1920, loss 0.770235
epoch 2048, loss 0.823893
epoch 2176, loss 0.743226
epoch 2304, loss 0.771293
epoch 2432, loss 0.569358
epoch 2560, loss 0.612488
epoch 2688, loss 0.633503
epoch 2816, loss 0.559473
epoch 2944, loss 0.68777
epoch 3072, loss 0.63268
epoch 3200, loss 0.518222
epoch 3328, loss 0.580646
epoch 3456, loss 0.613399
epoch 3584, loss 0.897899
epoch 3712, loss 0.480809
epoch 3840, loss 0.601103
epoch 3968, loss 0.649726
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0407513 0.058563
1.95996e-07 0.00441803
0.0538015 0.0593434
0.0907408 0.0615606
0.000794682 0.00216013
0.0707062 0.0107459
0.0727412 0.0613382
0.0813268 0.0568193
0.0509553 0.0565096
0.0950802 0.0640151
0.106645 0.0545154
0.0661223 0.0638075
0.0724741 0.0552489
0.109566 0.0648586
-0.0247813 -0.00173557
0.123121 0.0558096
0.0319947 0.0602815
-0.00236357 0.0601525
-0.063123 -0.00407691
0.0594399 0.0554301
-0.0532643 -0.0016271
0.0599612 0.0534271
-3.91322e-05 0.00441868
0.0435639 0.0557886
-0.0621274 -0.00315644
0.0986862 0.0563368
0.0271115 0.00119928
-0.032317 0.00113759
-0.0346424 -0.00347582
0.0859467 0.0606419
-0.0362233 0.00659335
0.0329332 0.0618399
0.0594396 0.0620733
-3.40966e-05 0.00746603
0.0857795 0.0571888
-0.0188593 0.00563849
0.00932522 0.0539669
-0.0132965 0.0643591
0.0424369 0.0553079
0.0621308 -0.00313683
0.0594398 0.0636481
0.0532535 0.0567578
-0.0118764 0.0623174
2.96104e-08 0.000622363
0.0384877 0.054394
0.092056 0.0567653
0.032933 0.0643313
-0.0716686 0.000525941
0.059435 0.064855
0.132184 0.0652675
-1.48505e-07 0.00496864
0.0496681 0.0624279
0.0326469 0.00229076
0.0295774 0.010126
0.00931417 0.0635609
0.0867067 0.0566938
0.0495768 -0.000439855
-3.91322e-05 0.0106592
0.059435 0.0536053
0.0215366 0.000785322
-0.0023637 0.0591172
-0.000835063 0.0107362
0.000835111 0.00197606
0.0267781 0.0560155
0.0989675 0.0610996
0.0867334 0.0637601
-0.0106876 -0.00119839
0.0462066 -0.00136028
0.0599029 0.0639739
0.0927397 0.0589376
0.0860364 0.00274969
0.110244 0.0570988
0.0368959 0.0566016
-0.0210896 0.00797042
0.121243 0.0644375
0.100667 0.0552417
0.032158 0.0561446
0.0870049 0.0544484
0.0594398 0.0610827
0.0458115 0.0612451
0.0496683 0.0624559
-0.0707055 0.00607691
0.0109586 4.52631e-05
0.0144846 0.00479599
0.0424529 0.056051
-0.0385434 0.00119424
0.0594325 0.052554
0.0857284 0.00404253
0.0468064 0.00917406
0.0223972 0.058143
0.0897923 0.0646245
0.0281391 0.0615931
0.080402 0.0551805
0.0251231 0.0602393
-0.0248182 4.18805e-05
0.0412702 -0.00196058
0.0396776 0.0569056
0.0113515 0.00696083
0.000793148 0.0030327
0.0217933 0.0590943
-0.00549668 0.0527687
0.0724734 0.0563876
0.0170933 0.0595332
0.0592406 0.0580358
0.0861486 0.0644815
1.74905e-07 -0.00210858
0.0362234 0.016104
0.0317697 0.062679
0.0093143 0.0597175
0.0458116 0.0618372
-0.000793636 -0.00087732
0.0867335 0.0651573
0.0211183 0.0599138
0.0710835 0.0622538
0.0323166 -0.00117577
0.0964772 0.057943
0.0168394 0.0560362
0.0710835 0.061211
-0.0707068 0.00580122
0.0335493 0.0544216
0.0362233 -0.000169217
0.0764118 0.0562814
0.00250001 -0.0036371
0.10955 0.0535311
0.100097 0.0612346
0.0870049 0.0555075
0.0224432 0.0561805
0.0191276 0.0533921
parameters: [ 9.449  2.169  2.     1.2    4.   ]. error: 762886232.054.
----------------------------
epoch 0, loss 1.37225
epoch 128, loss 0.846852
epoch 256, loss 1.16655
epoch 384, loss 0.964768
epoch 512, loss 0.696863
epoch 640, loss 0.781639
epoch 768, loss 0.652856
epoch 896, loss 0.721985
epoch 1024, loss 0.751801
epoch 1152, loss 0.640836
epoch 1280, loss 0.690885
epoch 1408, loss 0.845716
epoch 1536, loss 0.703244
epoch 1664, loss 0.53634
epoch 1792, loss 0.702363
epoch 1920, loss 0.540021
epoch 2048, loss 0.697056
epoch 2176, loss 0.583874
epoch 2304, loss 0.567385
epoch 2432, loss 0.785186
epoch 2560, loss 0.639911
epoch 2688, loss 0.824432
epoch 2816, loss 0.62563
epoch 2944, loss 0.718716
epoch 3072, loss 0.497823
epoch 3200, loss 0.542987
epoch 3328, loss 0.649183
epoch 3456, loss 0.622512
epoch 3584, loss 0.684028
epoch 3712, loss 0.689931
epoch 3840, loss 0.547737
epoch 3968, loss 0.659928
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0781223 0.05196
0.0716773 0.00585532
-0.0394792 0.0114774
0.0321586 0.0560456
0.0327313 0.0541272
-0.0631232 -0.00294985
0.0188597 0.0127252
0.0710838 0.0591815
0.0594397 0.0556811
0.0870049 0.0521404
-0.0390966 -0.00355504
-0.0529659 0.0115885
0.0781227 0.051766
0.0941706 0.0515591
-0.0188599 -0.000258173
0.0626054 0.052168
0.0113491 0.0107274
0.041765 -0.00130587
0.0417489 0.00627423
-0.0215238 0.0106686
0.0131851 0.010907
-0.0394796 0.0126218
0.0131845 -0.00109227
0.00596587 0.0554421
0.0927399 0.0607197
0.0527578 0.0549181
0.0394799 0.000290942
0.081797 0.0530775
0.0199306 0.0579518
0.0333262 0.0573661
0.0611197 0.00645471
-0.0807986 0.00863605
0.143948 0.0593757
0.0791268 0.0518105
0.0865811 0.055875
-0.0412685 -0.000641628
0.0435741 0.0489375
0.0468012 -0.000547114
0.0989678 0.0599511
0.0621334 0.00559896
-0.0109774 -0.00100578
0.121243 0.0576497
0.0857183 0.00470734
0.0937752 0.0645087
0.0594397 0.0602735
-0.0390898 0.00394089
0.0594398 0.0556672
0.0492512 0.00682587
0.0870049 0.0515796
-0.00701261 0.0548945
0.0248197 5.53139e-05
-0.0161572 0.0535594
-0.0248182 0.011142
-0.0117783 0.00384753
0.0449618 3.71122e-05
0.0322983 0.0546352
-0.0394796 0.0115992
0.0907409 0.058275
-0.0194897 0.0104871
0.0996717 0.0599533
0.0131849 -0.00113007
0.106629 0.0529201
0.0707066 -0.00090859
0.0424268 0.0546917
0.0594398 0.0566624
0.0290868 0.0597941
0.0397476 0.0507152
0.00596587 0.0576622
0.102035 0.0524512
-0.0161248 0.0548967
0.0594325 0.0503592
-0.00733491 0.0492711
0.0272916 0.0156352
0.0882823 0.0556643
0.130756 0.059131
-0.0414014 0.0109117
-0.0207316 0.00963227
0.125873 0.0561116
0.056285 0.056258
0.0461383 0.06043
3.5013e-07 -0.000392704
0.0261391 0.0561151
0.000835111 0.0104383
0.126171 0.0565844
0.0727412 0.0601485
0.097887 0.0624593
-0.0207367 0.00915199
0.0986862 0.0559362
0.0981543 0.055387
0.0261391 0.059171
-0.000835377 0.00971414
0.0327315 0.0559374
0.0911183 0.0558422
0.106624 0.0550103
0.0424529 0.0544983
-0.0117817 -0.00292817
0.0109586 0.0119363
0.0177241 -0.000897466
0.0482638 0.0599448
0.0362234 -0.000386893
-0.0118768 0.0509307
0.0707072 0.010496
-0.0209824 -0.00046293
0.0220537 0.0571792
0.0271117 0.00375819
0.0594398 0.0562702
0.0599612 0.0486523
0.0727411 0.0605472
0.0907409 0.0604227
-0.041769 0.00871361
-0.0662877 0.00325402
0.0243585 0.0515868
0.0910569 0.05596
0.014484 0.0127785
0.0867333 0.0558376
0.0857789 0.0549735
-0.063123 -0.00299913
0.0882822 0.0563741
0.0538015 0.0516014
0.0385291 0.0126829
0.0866799 0.0515701
0.0109586 0.000173294
0.0691985 0.059665
-0.000835377 0.0103351
0.0626054 0.0532884
0.0223968 0.0534892
0.129458 0.000985485
-0.0734426 -0.00127084
parameters: [ 9.449  2.183  2.     1.2    4.   ]. error: 510885.310585.
----------------------------
epoch 0, loss 1.23462
epoch 128, loss 1.09328
epoch 256, loss 0.736132
epoch 384, loss 0.848196
epoch 512, loss 0.950593
epoch 640, loss 0.828634
epoch 768, loss 0.573651
epoch 896, loss 0.809734
epoch 1024, loss 0.718788
epoch 1152, loss 0.729526
epoch 1280, loss 0.642769
epoch 1408, loss 0.57545
epoch 1536, loss 0.764508
epoch 1664, loss 0.634597
epoch 1792, loss 0.604763
epoch 1920, loss 0.688499
epoch 2048, loss 0.715712
epoch 2176, loss 0.652592
epoch 2304, loss 0.687946
epoch 2432, loss 0.726381
epoch 2560, loss 0.415767
epoch 2688, loss 0.659354
epoch 2816, loss 0.671918
epoch 2944, loss 0.550368
epoch 3072, loss 0.63768
epoch 3200, loss 0.851716
epoch 3328, loss 0.612183
epoch 3456, loss 0.741241
epoch 3584, loss 0.57018
epoch 3712, loss 0.644612
epoch 3840, loss 0.636938
epoch 3968, loss 0.508453
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.130756 0.0612968
0.0384877 0.0604469
0.0286789 0.0569817
0.018802 0.000118167
0.0278231 0.0554642
0.0140549 0.057958
0.0870049 0.0567401
0.00932522 0.0582948
0.0867335 0.0613797
0.0122194 0.0594464
0.0594759 0.057987
0.0207377 0.00109486
0.0281393 0.0572919
0.0594481 0.0685756
0.0582389 0.0554165
0.076438 0.0602713
0.0295774 0.015248
-0.0248069 0.0085717
0.0859466 0.0582858
0.0461383 0.0572192
-0.0346424 0.00492455
0.129916 0.0558951
-2.23337e-05 0.00588767
0.0945038 0.0589203
-0.129449 0.00410904
0.0594395 0.0609207
-0.0271193 0.0093656
0.088729 0.0596155
0.124277 0.0575889
0.0375382 0.0599588
-0.0271153 0.0003354
-0.0161248 0.0577998
0.0867335 0.0602762
0.0468064 0.00828145
0.100097 0.0641867
8.40829e-06 0.0123356
0.0327314 0.0573486
0.0199306 0.063262
0.046799 -0.000476285
0.130756 0.0578148
0.0495768 0.00599222
0.135017 0.0629534
0.0188015 0.0647211
0.0662801 0.00112875
-7.68032e-07 0.00297781
0.0367961 0.00993031
0.0594399 0.0574337
0.059435 0.0603494
0.0594494 0.0690034
0.0706156 0.0597076
0.0642209 0.0553278
-0.0271121 0.00886141
0.076422 0.0599974
0.0594398 0.0605098
0.0897923 0.0594951
0.0397474 0.0583257
-0.0631232 0.00162394
0.0194981 0.00696804
-0.0271278 0.00423118
-0.0621274 0.00544798
0.061289 0.068576
0.0321468 0.058847
-0.0662843 0.000334796
0.0716711 0.000213881
-0.0414033 0.00333709
0.0278229 0.0570786
0.0594397 0.0598271
0.0977802 0.0648847
-0.00856301 0.00634874
0.0764118 0.0614175
0.0710836 0.0571433
0.0594399 0.0598374
-0.0734426 0.00193471
0.0346452 -5.03875e-05
0.0648671 0.0554868
0.0317698 0.0579811
0.0605534 0.0557859
0.0593665 0.0563793
-0.00856301 0.00594351
0.0594301 0.0582316
0.0594399 0.0587309
0.0781227 0.0557467
-0.0621346 0.00427627
0.0370769 0.0559789
0.0707072 0.00475439
1.01848e-06 0.00270542
0.0387924 0.057457
0.046799 0.00272855
-0.0365701 0.0068544
0.0803921 0.0603538
-0.0210947 0.00629268
0.086885 0.0589876
0.0261393 0.0609146
0.0997468 0.057871
0.0815268 0.0594274
0.0859467 0.0580307
0.0594397 0.0598271
0.0981543 0.0597713
0.0882822 0.0596376
0.0582651 0.0549489
-0.0207342 0.0026048
0.092056 0.0613745
0.0192074 0.059519
0.0941706 0.0581729
0.0211183 0.0649234
0.0582236 0.0557552
0.0991788 0.058576
0.022444 0.0569123
0.0144707 0.0160722
0.0901953 0.0584672
0.022215 0.0574085
-6.66928e-06 0.00849262
0.057604 0.0690092
0.0592247 0.0543514
0.0661225 0.0605496
0.0318695 0.05806
0.0624794 0.0547212
-0.0109489 0.00373011
0.0215366 0.00766088
0.0594399 0.0567352
0.0495796 0.0070425
0.0631163 -0.000419038
0.0424529 0.0610579
0.0482639 0.0592294
0.00250001 0.00567941
0.093775 0.06554
0.109566 0.0591878
0.0310444 0.0584984
parameters: [ 9.449  2.195  2.     1.2    4.   ]. error: 3657258.26324.
----------------------------
epoch 0, loss 1.57358
epoch 128, loss 1.21379
epoch 256, loss 1.03613
epoch 384, loss 0.98142
epoch 512, loss 1.03542
epoch 640, loss 1.2251
epoch 768, loss 1.01092
epoch 896, loss 1.00043
epoch 1024, loss 0.889183
epoch 1152, loss 0.903536
epoch 1280, loss 0.943965
epoch 1408, loss 0.861822
epoch 1536, loss 1.04849
epoch 1664, loss 0.906282
epoch 1792, loss 0.928353
epoch 1920, loss 0.86843
epoch 2048, loss 0.877285
epoch 2176, loss 0.83727
epoch 2304, loss 0.731659
epoch 2432, loss 0.695034
epoch 2560, loss 0.828257
epoch 2688, loss 0.851624
epoch 2816, loss 0.756241
epoch 2944, loss 0.804969
epoch 3072, loss 0.565502
epoch 3200, loss 0.957874
epoch 3328, loss 0.72995
epoch 3456, loss 1.01862
epoch 3584, loss 0.805001
epoch 3712, loss 0.750576
epoch 3840, loss 0.602277
epoch 3968, loss 0.795193
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0867333 0.0487899
0.0691072 0.0517834
0.0800821 0.0496508
0.0327313 0.0534698
0.0710837 0.0520263
0.0920508 0.0501079
-0.0188062 0.00263481
-0.0776729 0.00592889
0.0384676 0.0464068
0.0144707 0.00549313
0.0594467 0.0520513
0.0321468 0.050722
0.0412725 0.00336411
0.0497671 0.0505931
0.106629 0.0497016
0.0971047 0.0572367
0.109566 0.0499439
0.0855722 0.0508198
0.0819546 0.0476222
-0.0131841 0.00957911
0.0385437 0.00807781
0.129916 0.0450361
0.00863029 0.0468644
0.0878297 0.0499614
-0.0346424 0.00414513
0.0964772 0.0489675
-0.10012 0.0119989
0.0290868 0.0531987
0.0458117 0.0487912
0.0248197 0.00856445
0.0482639 0.0545947
0.0878299 0.0500861
-0.000781954 0.00485281
0.110244 0.047176
0.0901949 0.050632
0.0981543 0.0458988
0.0113515 0.00437206
0.0326439 -0.00313625
0.0594396 0.0537964
0.0375382 0.0480216
0.0710835 0.0526384
0.105471 0.0557792
0.0724741 0.0484508
-0.0267379 0.0114722
0.121243 0.0532635
0.0327313 0.0512982
-0.0662843 -0.00311439
0.0261391 0.0550887
0.0093143 0.0519148
0.088729 0.0557738
0.0188609 0.0114612
0.0594397 0.0521574
0.0594395 0.0534471
-0.0412711 0.00220232
0.0860523 0.0130956
0.0435639 0.0463615
0.0321467 0.0496584
-0.0417792 0.00553839
0.0215366 0.00724922
-0.0621343 0.0122869
0.0365698 0.00655991
0.0290868 0.0541049
0.00931426 0.0491192
0.121243 0.0538048
0.0327313 0.0521876
0.0278229 0.0489855
0.0417448 0.0137312
-0.0152583 0.00756189
0.0870049 0.0503469
0.092056 0.0505067
0.0971047 0.0550293
0.0251234 0.0500097
0.0271185 0.000119653
0.0594297 0.0490484
-0.0857218 0.00349461
-0.0394796 0.0220502
0.0131845 0.00411937
0.0317696 0.0510904
0.0950802 0.0583079
-1.17339e-07 0.00786954
0.0527575 0.0529502
3.82209e-05 0.00725424
3.5013e-07 0.0101184
-7.68032e-07 0.0165244
0.0594397 0.052463
4.07333e-06 0.00168047
0.0210209 0.0535685
0.0390889 -0.000111903
0.0991788 0.0511205
-0.0177216 0.0129413
0.0375382 0.0472785
0.0907409 0.0520163
-3.40966e-05 0.0044292
0.0968255 0.0521936
0.0857788 0.0525193
0.0642209 0.0450113
0.0594301 0.0484292
0.0594396 0.0546331
0.085161 0.0577588
0.0594398 0.0526574
0.088282 0.0504191
0.0594397 0.053233
-0.0414014 0.0218941
0.0317696 0.0507354
-0.0318771 0.0218616
0.0375434 0.0489997
0.0290868 0.054922
0.0901948 0.0496866
0.0707066 0.0142756
0.0631196 0.000158553
0.102035 0.0498594
0.076422 0.0493976
0.0594396 0.0541087
0.0859465 0.0520721
0.0207377 0.011499
0.0482638 0.055724
-0.00699273 0.0495821
0.129458 0.00923812
0.0945038 0.0474438
6.7959e-07 0.0105775
0.090741 0.0516664
0.0217933 0.0562434
0.0318766 0.0106149
0.0897923 0.053619
0.0243594 0.0489598
-0.0390898 0.0033761
0.0201879 0.0495391
-0.0611177 0.00862631
parameters: [ 9.449  2.195  3.     1.2    4.   ]. error: 1419772755.74.
----------------------------
epoch 0, loss 1.37985
epoch 128, loss 0.671802
epoch 256, loss 0.735759
epoch 384, loss 0.600491
epoch 512, loss 0.574191
epoch 640, loss 0.806811
epoch 768, loss 0.72215
epoch 896, loss 0.771773
epoch 1024, loss 0.537159
epoch 1152, loss 0.549682
epoch 1280, loss 0.595823
epoch 1408, loss 0.587839
epoch 1536, loss 0.644832
epoch 1664, loss 0.866217
epoch 1792, loss 0.636947
epoch 1920, loss 0.518732
epoch 2048, loss 0.672197
epoch 2176, loss 0.598575
epoch 2304, loss 0.742617
epoch 2432, loss 0.606463
epoch 2560, loss 0.590723
epoch 2688, loss 0.781928
epoch 2816, loss 0.697219
epoch 2944, loss 0.542776
epoch 3072, loss 0.530077
epoch 3200, loss 0.637049
epoch 3328, loss 0.643474
epoch 3456, loss 0.556783
epoch 3584, loss 0.55664
epoch 3712, loss 0.517927
epoch 3840, loss 0.568415
epoch 3968, loss 0.752526
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.062595 0.0676369
0.0800819 0.058998
0.0318695 0.0641909
-0.0188023 -0.00110896
0.0508582 0.0462276
0.000834782 -0.00156874
0.0412752 0.0019662
0.0594397 0.0718376
0.0234681 -0.00216266
0.0330953 0.0392265
0.0133907 0.0650716
-0.0118764 0.0594784
-0.0631101 -0.000942352
-0.0204001 -0.000158857
0.0968257 0.0622981
0.0691985 0.0596035
0.0861487 0.0710884
0.0878299 0.0566028
0.0458115 0.0654338
0.0177216 0.00742104
4.07333e-06 -0.00227876
-0.0209708 -0.00175385
-3.40966e-05 -0.00218162
0.0865812 0.0725202
0.110244 0.065357
0.0234704 0.00134113
0.0208321 0.0123005
-0.00553123 0.0395335
0.0373475 0.043341
0.00460075 -0.00183922
-0.0414059 0.00227304
0.00131949 -0.000950257
0.106629 0.0660949
0.0593312 0.0592935
0.0952316 0.061149
-0.041635 -0.00205174
0.0368964 0.038752
-0.0323135 -0.00177576
0.0621339 -0.000327579
0.00131949 -0.00225205
0.0385434 0.00109547
0.0412752 -0.00185141
0.10664 0.0753466
0.0861488 0.0678907
0.0911173 0.0620791
0.0594397 0.0715032
0.100097 0.061461
0.0791268 0.0695704
0.0966587 0.0742467
0.0691068 0.0660534
0.0599029 0.0731099
0.0210958 0.000133668
0.0384777 0.0588677
0.032933 0.061861
0.0927399 0.0582608
0.059435 0.0449358
0.0168394 0.0515913
-0.0340329 -0.000948365
0.0677309 0.0516654
0.00863092 0.0421566
0.036223 -0.0021597
0.0412702 -0.00195323
-0.0189824 0.00504835
0.00460075 -0.00122803
-0.0234699 -0.00198472
0.00131949 -0.00140131
-0.0449623 -0.00218317
0.112932 0.0714567
0.0271204 0.00693435
-0.0529682 -0.00172679
0.0144846 -0.00192543
0.0911173 0.0601265
0.0494968 0.00285472
0.092056 0.0534054
0.0865811 0.071255
0.076417 0.0601253
0.0122295 0.0446325
0.0210981 -0.00194617
0.0191276 0.0505186
0.0208321 0.0123005
0.00894003 0.000831065
0.0593758 0.0464686
0.0215366 0.00165401
0.021525 -0.00216813
0.032933 0.0531003
0.038544 -0.00202644
0.00894003 -0.00124379
0.080392 0.0656349
0.0870053 0.0656756
0.0991789 0.0595498
0.0208245 0.00666186
0.0144707 -0.0018893
0.0991789 0.0622627
0.130756 0.0686353
-0.0776729 -0.000329099
0.0424529 0.0379517
-0.0272977 0.000145533
0.0267779 0.0530055
0.0594376 0.0599977
0.0691985 0.0550137
0.126127 0.0748277
0.0384877 0.0589602
0.0461385 0.0586835
-0.036223 -0.0021709
0.0950805 0.0623491
0.0710835 0.0565416
0.0375434 0.037494
0.0520969 0.073199
-0.0532547 0.0015095
0.0538177 0.0483963
0.0911173 0.0587482
0.0594449 0.04976
0.0716719 -0.00179689
0.000794682 0.00483552
0.0749536 0.0532508
0.0594396 0.0632506
-0.0323135 -0.000970184
0.0691068 0.0625424
0.0594301 0.0424855
0.0865811 0.0746663
-0.0152606 -0.00199112
0.0594396 0.0537544
0.0131845 -0.00196898
0.049499 -0.0021786
0.0436381 0.0628103
0.038544 -0.00223061
0.0871098 0.0696479
0.130757 0.0606928
parameters: [ 9.449  2.195  0.382  1.2    4.   ]. error: 8885.19273356.
----------------------------
epoch 0, loss 1.06151
epoch 128, loss 0.796302
epoch 256, loss 0.709505
epoch 384, loss 0.740559
epoch 512, loss 0.801863
epoch 640, loss 0.699681
epoch 768, loss 0.519265
epoch 896, loss 0.755867
epoch 1024, loss 0.774784
epoch 1152, loss 0.760838
epoch 1280, loss 0.57867
epoch 1408, loss 0.60871
epoch 1536, loss 0.551661
epoch 1664, loss 0.397465
epoch 1792, loss 0.791647
epoch 1920, loss 0.653455
epoch 2048, loss 0.767724
epoch 2176, loss 0.659141
epoch 2304, loss 0.667927
epoch 2432, loss 0.584299
epoch 2560, loss 0.828525
epoch 2688, loss 0.676916
epoch 2816, loss 0.653256
epoch 2944, loss 0.590374
epoch 3072, loss 0.635452
epoch 3200, loss 0.624072
epoch 3328, loss 0.615973
epoch 3456, loss 0.497365
epoch 3584, loss 0.584855
epoch 3712, loss 0.456611
epoch 3840, loss 0.658278
epoch 3968, loss 0.676476
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0234638 -0.00323801
-0.0131848 -0.00231868
0.0724741 0.0520294
0.041765 -0.00368348
0.0109517 0.0051428
0.049499 -0.000248912
0.121243 0.0665982
0.132178 0.0748514
0.00459996 -0.001191
-0.00700272 0.0224888
-0.00236373 0.0352807
0.0321632 0.0518523
0.028139 0.0543712
0.0594494 0.073897
0.0764272 0.0550666
-0.0417573 -0.00383049
0.0701141 0.0556849
0.0201879 0.0348612
0.0131855 0.0107553
0.0384676 0.0516637
0.0508582 0.0331012
0.0323166 -0.00285583
-0.0144843 0.00103412
0.121243 0.0669896
-0.0707068 0.00433825
1.96444e-05 -0.00428676
0.0182077 0.0142791
-0.0194897 -0.00131814
0.00863092 0.0210606
0.0300757 -0.00137967
0.0272988 -0.00391988
0.0113491 -0.00282688
0.0122142 0.0491073
0.0188596 -0.00284814
0.0271117 -0.00140915
0.0301505 0.0543884
0.0407513 0.0467519
0.074953 0.0508198
0.0625951 0.0566525
-0.00737805 0.0228395
0.0283785 0.0431419
-0.00484737 0.0349316
0.0482639 0.0597172
-0.0621346 -0.00316362
0.0412752 -0.00336823
0.0396776 0.0423055
0.0122452 0.052224
0.0497673 0.042341
-0.0131841 -0.000994532
0.0594399 0.0515767
-0.0318771 -0.00132186
0.0968255 0.067689
0.000213351 0.0659442
-0.0118768 0.0490673
0.0966596 0.0535235
-0.0113373 0.00614796
0.0594397 0.061648
0.0497672 0.0452974
0.0207304 -0.000226918
0.123121 0.0593196
0.0532657 -0.000737759
-0.049248 -0.00365531
-0.0267398 0.000843928
-0.0394796 0.000777864
-0.0267379 -0.00355751
-0.0807974 0.000495774
0.0494968 0.00155374
0.0783558 0.0670955
0.0417448 -0.00183968
0.0397474 0.0456983
0.0327315 0.0628417
0.101441 0.0538011
-0.0111018 0.040477
-0.036223 0.000401036
0.0177241 -0.00397158
0.0593312 0.0452807
-0.00460347 -0.00211689
0.0989675 0.0698662
0.0866747 0.0600569
0.0335493 0.0207655
-0.00236357 0.0467864
0.105503 0.0747564
0.106634 0.0622749
0.0407515 0.0528923
-4.4075e-07 -0.00317785
0.109566 0.0695071
-0.02083 0.00059338
0.0937753 0.0697997
-0.00699273 0.0167085
0.0122243 0.0441475
0.0281391 0.0545324
-0.000786289 0.00807827
0.0730676 0.0608122
0.090741 0.055794
0.0217931 0.065249
0.0532657 -0.000361986
0.121243 0.0711173
-0.016115 0.017281
0.0950804 0.0628501
0.0477956 0.0573997
0.0983621 0.0434532
-0.0271153 -4.91246e-05
1.13647e-06 0.00536355
-0.0234677 -0.00353446
-0.0394796 0.00501203
0.0385437 -0.0031099
0.121243 0.0620924
0.0594398 0.0630623
0.0868851 0.0674995
0.0272916 0.00157345
0.0529729 -0.00156074
0.0661222 0.0534714
0.0414025 0.00558692
0.032195 0.0392961
0.0594397 0.0650175
0.0247806 -0.00335827
0.0458117 0.0635198
0.0333261 0.0670135
0.124311 0.0620123
4.17498e-06 -0.00176313
0.0329021 0.0642225
-0.027288 -5.65938e-05
0.0907409 0.0633072
0.038544 -0.000979531
0.0330953 0.0335983
-0.0247793 -0.00377264
0.00131949 0.00127612
0.0594397 0.059627
parameters: [ 9.449  2.195  1.189  1.2    4.   ]. error: 44213876.1152.
----------------------------
epoch 0, loss 1.11424
epoch 128, loss 0.868146
epoch 256, loss 0.978009
epoch 384, loss 0.71418
epoch 512, loss 0.61725
epoch 640, loss 0.705957
epoch 768, loss 0.880158
epoch 896, loss 0.659054
epoch 1024, loss 0.707474
epoch 1152, loss 0.666881
epoch 1280, loss 0.769801
epoch 1408, loss 0.842899
epoch 1536, loss 0.709883
epoch 1664, loss 0.665022
epoch 1792, loss 0.63449
epoch 1920, loss 0.79532
epoch 2048, loss 0.725666
epoch 2176, loss 0.562238
epoch 2304, loss 0.719967
epoch 2432, loss 0.622829
epoch 2560, loss 0.595592
epoch 2688, loss 0.724486
epoch 2816, loss 0.657275
epoch 2944, loss 0.670277
epoch 3072, loss 0.664284
epoch 3200, loss 0.619792
epoch 3328, loss 0.573322
epoch 3456, loss 0.641444
epoch 3584, loss 0.707883
epoch 3712, loss 0.676283
epoch 3840, loss 0.656289
epoch 3968, loss 0.632741
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0267425 -0.00633108
0.100667 0.054048
0.0701141 0.0520819
-3.11621e-07 -0.00650412
0.0131861 -0.000301595
-2.95455e-07 -0.00593852
0.0764428 0.0531241
0.0813321 0.0517406
0.0859467 0.0603087
0.0093143 0.0540671
0.0174339 0.04859
0.0776735 0.00106234
-0.016115 0.0449367
0.0662801 -0.00799877
0.0199307 0.0597242
0.0594349 0.050372
-0.0188599 -0.00620147
0.0414051 0.0005412
0.104819 0.0517626
0.0189828 -0.0071964
0.0594398 0.0548625
-0.0860447 0.0001705
0.0122352 0.0481753
0.0626054 0.0516644
0.0192074 0.0580255
0.0781227 0.0477299
-0.0662867 -0.00484424
0.0594398 0.0550626
0.130756 0.0599578
0.0327314 0.0545303
0.000213748 0.0607949
0.0589133 0.0425217
0.0594473 0.0511994
0.0599612 0.0429828
0.0952317 0.0621664
0.0964307 0.055123
0.0631173 -0.00723823
0.0659505 0.00177027
-0.000792737 -0.00656592
0.125873 0.0554223
0.0776855 0.00222419
0.0952317 0.0630578
0.125883 0.059183
0.0318693 0.0468915
-0.00894125 -0.00641546
0.0405236 0.0561919
0.0037846 -0.00240453
0.0281392 0.0563903
-0.0131858 0.00046656
0.121243 0.0570662
-0.0113401 -0.00563399
0.00459996 -0.0072766
-0.0194897 -0.0013228
-0.0532547 0.001132
-0.0210969 0.00102421
-0.0247813 -0.000649455
-7.68032e-07 -0.00557431
0.0122452 0.0479382
0.0594481 0.0757266
0.090741 0.060393
0.0643397 0.0528105
0.0867067 0.0504436
-0.0807986 -0.00116763
0.10664 0.0499249
0.022215 0.0378767
0.0977802 0.0643251
0.0691072 0.0454798
0.0661222 0.0565194
0.0983621 0.0504844
-3.11149e-07 -0.000627863
-0.00549668 0.0409814
0.0860364 -0.00676653
0.0945038 0.0533536
0.0301505 0.0523871
0.0997468 0.051581
0.0594397 0.0564698
-0.0385437 -0.00648014
0.094504 0.0533347
0.0220538 0.0564525
0.0871097 0.0565867
0.0131858 -0.00664471
0.000213351 0.0594238
0.0397476 0.0469018
0.0281392 0.0538579
0.0724734 0.0536797
-0.039089 -0.0083913
0.0468012 -0.00610523
-0.0857218 -0.00759506
0.057604 0.0759415
0.110244 0.0469242
4.57767e-08 -0.00668328
-0.0716682 -0.0078153
0.0911183 0.0496052
0.0968255 0.0602873
0.029557 -0.00597501
-0.0207292 -0.00131409
-0.0295746 -0.00262128
0.0131861 -0.00528094
-0.000807624 -0.00648917
0.0281392 0.0594094
-0.0529722 -0.00168784
0.0594466 0.0744881
0.0330948 0.044898
0.0593133 0.0538029
0.0321468 0.052696
0.0496683 0.05839
0.0594398 0.0561664
0.0207377 -0.00638879
0.0989675 0.0647994
0.0853255 0.0535623
-0.0860431 -0.00681438
0.0496683 0.0568425
-0.0131858 -0.00111117
0.097887 0.0652837
0.0300726 -0.00786734
0.0983621 0.0467927
-0.00236348 0.0552707
0.032195 0.0451661
0.0599612 0.0481671
0.022444 0.0418308
0.0317699 0.0525392
-0.0271153 -0.00685719
0.0594397 0.0540602
0.129458 -0.00659824
0.0621339 -0.003331
0.0529686 -0.00606715
-0.0417792 -0.000114909
0.0281391 0.0518489
parameters: [ 9.449  2.195  2.     1.2    4.   ]. error: 629391764.847.
----------------------------
epoch 0, loss 1.31282
epoch 128, loss 0.963757
epoch 256, loss 1.06286
epoch 384, loss 0.839421
epoch 512, loss 0.784277
epoch 640, loss 0.933934
epoch 768, loss 0.700432
epoch 896, loss 0.836074
epoch 1024, loss 0.781249
epoch 1152, loss 0.695798
epoch 1280, loss 0.791657
epoch 1408, loss 0.804381
epoch 1536, loss 0.643942
epoch 1664, loss 0.615975
epoch 1792, loss 0.746286
epoch 1920, loss 0.740677
epoch 2048, loss 0.66836
epoch 2176, loss 0.778536
epoch 2304, loss 0.644693
epoch 2432, loss 0.589863
epoch 2560, loss 0.626756
epoch 2688, loss 0.714105
epoch 2816, loss 0.753705
epoch 2944, loss 0.588558
epoch 3072, loss 0.745906
epoch 3200, loss 0.713907
epoch 3328, loss 0.766989
epoch 3456, loss 0.724244
epoch 3584, loss 0.869458
epoch 3712, loss 0.672858
epoch 3840, loss 0.650983
epoch 3968, loss 0.760203
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0631196 0.00338326
0.0322983 0.0562754
-0.00894944 0.00670004
-0.0860431 0.00103502
0.032933 0.0560353
0.0822309 0.051595
0.0496682 0.0546774
-0.129445 -0.00165445
0.106624 0.0547241
0.0626054 0.0557315
0.0611197 -0.00221503
0.032933 0.0569085
0.0662934 -0.0012007
0.0495768 -0.00323202
0.0224432 0.0529321
0.0317697 0.0534539
3.82209e-05 0.0137561
0.0594399 0.0560568
0.0527576 0.0566631
-0.0776845 -0.00394168
0.0188016 0.059045
0.0538297 0.0533001
0.0461382 0.0547919
0.0462066 -0.00540805
-0.0494953 -0.00351518
0.0594427 0.0544924
0.027823 0.0536672
0.101814 0.0590971
0.088282 0.054499
0.0857788 0.0543685
-0.041635 0.00712266
0.0910567 0.0547215
0.00131949 -0.00165807
0.0800819 0.0540948
0.056285 0.0567458
0.01684 0.0542899
0.143948 0.0563265
0.0594452 0.0635063
0.0527577 0.0582989
-0.00142113 -0.00452423
0.0865811 0.0559283
0.038792 0.0541303
0.12945 0.00484743
0.0375381 0.0555374
0.027823 0.057454
0.0857788 0.0536956
0.0134225 0.0614886
0.0310444 0.0540403
0.129453 -0.003617
0.0461382 0.0563036
-0.00331175 0.0502679
0.0867333 0.0583526
-0.0707068 0.00158537
0.081797 0.055472
0.0174335 0.0522456
0.0911173 0.0546486
0.0131855 0.0126247
0.0927397 0.0541258
0.044975 0.0571103
0.0907409 0.057202
0.0964305 0.0543133
0.118684 0.0603826
-0.0248165 -0.000549088
0.0920508 0.0541217
-0.0207292 0.00593262
-0.065961 0.00203961
-0.0210969 0.00163715
-0.0707055 -0.00401633
0.0594398 0.0572534
0.0861488 0.054599
0.00931426 0.0547029
-0.0113373 5.85214e-05
0.0625949 0.0552169
0.0261391 0.0528894
0.0589133 0.0533195
0.0496683 0.054787
0.0384877 0.053726
-0.0414014 0.00646897
0.0964772 0.0532915
0.0272894 0.00171822
0.106619 0.0544573
0.059367 0.0537093
0.0407515 0.0541924
0.0730674 0.0558647
0.0220537 0.056228
0.0599029 0.0548794
0.0594449 0.0541102
-0.0385437 0.00762828
-0.0188028 -0.00916579
-0.0188028 -0.00573736
0.0594162 0.0529762
0.0188023 -0.00860687
0.0140549 0.0541645
0.0673482 0.0545694
-0.034636 0.00245685
0.0594398 0.0563356
0.0197008 0.0559876
0.0237989 0.05172
0.12945 -0.00418621
0.044975 0.0582185
0.109566 0.0553084
8.39808e-07 0.000963536
0.0611204 0.00421132
0.0277564 0.0536335
0.0971047 0.0592494
0.0594398 0.0534593
0.0131849 0.00324071
0.0673482 0.0541033
0.0734394 0.010584
0.0318766 -0.00368617
-0.0857226 0.00239553
0.0855722 0.0587231
0.0281393 0.0561861
0.0251231 0.059171
0.143948 0.0548255
0.100115 -0.00620695
0.0327313 0.0576158
0.0295687 0.00699154
0.081527 0.0543079
0.014471 0.00426479
-0.0495734 -0.0018222
0.0197009 0.0543017
0.0950805 0.0560225
0.00459996 0.00436917
0.0857788 0.0545813
0.0321586 0.0542289
0.0860523 0.00320983
0.086885 0.0540707
parameters: [ 9.449  2.195  2.382  1.2    4.   ]. error: 446423.09123.
----------------------------
epoch 0, loss 1.0206
epoch 128, loss 0.907252
epoch 256, loss 0.929882
epoch 384, loss 0.955083
epoch 512, loss 0.907982
epoch 640, loss 0.75862
epoch 768, loss 0.767095
epoch 896, loss 0.684872
epoch 1024, loss 0.836005
epoch 1152, loss 0.715644
epoch 1280, loss 0.763215
epoch 1408, loss 0.745285
epoch 1536, loss 0.808081
epoch 1664, loss 0.684947
epoch 1792, loss 0.753295
epoch 1920, loss 0.691702
epoch 2048, loss 0.760034
epoch 2176, loss 0.757647
epoch 2304, loss 0.830541
epoch 2432, loss 0.950191
epoch 2560, loss 0.905384
epoch 2688, loss 0.966834
epoch 2816, loss 0.68816
epoch 2944, loss 0.649064
epoch 3072, loss 0.643089
epoch 3200, loss 0.558033
epoch 3328, loss 0.680753
epoch 3456, loss 0.639008
epoch 3584, loss 0.715718
epoch 3712, loss 0.932368
epoch 3840, loss 0.826622
epoch 3968, loss 0.696832
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0318693 0.0541047
0.0594397 0.0561099
0.0497673 0.0526316
0.124354 0.0506488
-2.95455e-07 0.0162262
-2.23337e-05 0.00620504
-0.0161248 0.0504026
0.0424428 0.0537437
0.0234681 0.00826527
0.0223972 0.0498489
0.072741 0.0545612
0.0131858 0.0130656
-0.129449 0.00787198
0.0594398 0.0569483
0.0592247 0.0491119
0.0968257 0.0580863
0.0317697 0.0538259
-0.0131848 0.0017243
0.0424369 0.0527559
-0.041769 0.000925665
0.121243 0.0548609
0.044975 0.0563911
0.0710838 0.0534452
0.046209 0.0137737
0.109566 0.0558534
0.0594473 0.050212
0.0857284 -0.00541036
0.079197 0.0528518
0.0594398 0.0556977
-0.0776845 0.00517442
0.0337179 0.0549509
0.0520969 0.0560718
0.0867333 0.0572957
0.059435 0.061948
0.0321467 0.0560658
0.0594397 0.0554575
-0.0271193 0.00572718
0.0920613 0.0513508
0.0592543 0.0517677
-0.0271153 0.0040239
0.0189852 0.00139869
0.0594467 0.0618628
-0.0631232 -0.00248469
0.10955 0.053021
0.0594399 0.0555338
0.0593665 0.0516182
0.0477957 0.0567557
0.0594396 0.0579663
0.046218 0.0218295
0.0464004 0.0526842
0.0827282 0.00634765
0.060558 0.0506871
0.0867201 0.0555784
0.0594396 0.0579836
-0.0412711 0.00681542
0.0594397 0.0561045
0.0871098 0.0567594
0.0527578 0.0583946
0.126086 0.04932
0.102034 0.054758
0.0950805 0.0532362
-0.0189864 0.00493238
-0.0468045 0.00531347
0.0267393 0.00071646
-0.0161473 0.0513117
0.0140549 0.0522323
0.102035 0.0514802
0.0871097 0.0563461
0.0611204 -0.00240851
0.0997468 0.0508531
0.0248197 0.00224489
-0.0467978 0.00217347
0.0237988 0.0574756
-0.00894944 -5.51782e-05
0.110244 0.0532937
0.0897923 0.0574712
0.0594398 0.0528975
-0.0207292 0.00986285
0.0562648 0.054911
0.0496681 0.0552041
0.0707072 0.00797634
0.0594452 0.0615261
-6.0681e-07 0.00164461
0.022444 0.0507398
0.0878297 0.0524546
0.0278231 0.0559865
0.0267419 0.00881498
0.0945038 0.053091
0.0362234 -0.00120497
0.0860587 0.0114751
0.0920961 0.0514605
0.014471 0.0162797
-0.10012 0.00191859
0.0927397 0.0525393
0.0611204 0.00768564
-0.0161572 0.0543526
0.0983621 0.0532541
0.0222151 0.0560634
0.0273016 -0.000485218
0.0964305 0.0537264
0.0907408 0.0584328
0.0106903 -0.00327531
0.059435 0.0542786
0.0458117 0.0566146
0.106645 0.0517736
0.043921 0.0483855
0.0106903 -0.00224704
0.0945038 0.0546373
0.0174339 0.0515489
0.0950803 0.0535086
0.0621308 -0.00225368
0.090741 0.054193
0.0468064 0.00121494
0.0407515 0.0525806
0.0177216 0.00972668
-0.0707055 0.0103768
0.121243 0.0573963
0.0144704 0.0164523
0.112932 0.0560996
0.00931417 0.0553586
-0.0204101 0.0129732
0.0243585 0.0503427
0.0122142 0.0503021
0.0215366 0.00898407
-0.0188605 0.00929941
5.31452e-07 0.0187003
-0.0621343 0.00177453
-0.0385431 0.00316536
parameters: [ 9.449  2.195  2.618  1.2    4.   ]. error: 2322538804.01.
----------------------------
epoch 0, loss 1.00729
epoch 128, loss 1.28778
epoch 256, loss 0.890784
epoch 384, loss 0.769002
epoch 512, loss 0.999068
epoch 640, loss 0.732029
epoch 768, loss 0.939212
epoch 896, loss 0.895147
epoch 1024, loss 0.66833
epoch 1152, loss 0.79726
epoch 1280, loss 0.763081
epoch 1408, loss 0.886863
epoch 1536, loss 0.730531
epoch 1664, loss 0.683077
epoch 1792, loss 0.650885
epoch 1920, loss 0.631316
epoch 2048, loss 0.734466
epoch 2176, loss 0.680579
epoch 2304, loss 0.700938
epoch 2432, loss 0.919812
epoch 2560, loss 0.523126
epoch 2688, loss 0.858001
epoch 2816, loss 0.827658
epoch 2944, loss 0.635557
epoch 3072, loss 0.715635
epoch 3200, loss 0.592867
epoch 3328, loss 0.787957
epoch 3456, loss 0.691346
epoch 3584, loss 0.638362
epoch 3712, loss 0.499692
epoch 3840, loss 0.638321
epoch 3968, loss 0.796488
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-2.23337e-05 0.00502677
0.032902 0.0622188
0.0594398 0.0626156
0.129916 0.0575094
0.0594251 0.061626
-0.0390898 0.00109875
0.0188088 0.0020092
-0.0131855 0.00170454
0.0329021 0.0631786
0.0815268 0.0591511
0.0462066 0.00684121
0.0217933 0.0643556
4.07609e-05 0.00360073
-0.0734312 0.00284544
0.101441 0.0600762
0.025123 0.064292
0.0416434 0.00348906
-0.0532643 0.00318612
0.105506 0.0691541
-0.027288 0.00780811
-0.00378297 0.00462816
0.0594204 0.0616549
-0.0188057 0.00716345
0.0966596 0.0609795
0.0317698 0.061498
0.0800821 0.06097
2.41162e-06 0.0180451
0.0286789 0.0589644
0.0950804 0.0582435
-0.0495734 0.000240565
-0.0414014 0.00605832
0.0865812 0.0610735
0.0594424 0.0589033
-0.0188023 -0.00106518
0.0594397 0.06298
0.05627 0.0600644
0.0621339 0.00184317
0.088729 0.0610557
0.100097 0.0651205
-0.0807993 0.00340025
0.0323163 0.00164638
0.027823 0.0601969
0.0134225 0.0690127
0.0321468 0.0566322
-4.6184e-07 0.00704865
0.0527576 0.0612446
0.0237987 0.0623986
-0.0462049 0.00841631
0.0659505 0.00323238
0.0267443 0.0045538
0.0318693 0.0579406
0.100097 0.0646699
-0.0390898 0.00130855
7.08095e-06 0.00497603
0.0710837 0.0626632
0.0286789 0.056182
0.0414001 0.00386126
-0.00460347 0.00527024
0.0330953 0.0570338
-0.039089 -0.00328084
0.0594396 0.063283
-0.0362226 0.00247358
0.0520969 0.0608408
0.0321468 0.0605038
0.0594482 0.0693855
0.092056 0.0613095
0.0210209 0.0641239
0.0494968 0.00750115
0.110244 0.0608183
0.0950805 0.0614761
-1.92327e-05 0.0023582
0.0594162 0.0584622
0.0594396 0.0596029
0.0594398 0.053801
0.0248097 0.0065852
0.0497673 0.0580176
-0.053266 0.00753386
-0.0734426 0.00512989
0.0997462 0.0608421
0.044975 0.0637305
0.0205116 0.0586972
0.0904958 0.0592355
0.0860523 0.0128223
-0.000781954 0.00498857
0.0417583 0.000314065
0.0851612 0.0586866
0.0594376 0.0587985
0.0520969 0.0611954
-0.0113373 0.00594249
0.0977799 0.0658309
-0.0144706 0.00448145
0.090741 0.0568548
0.0707062 0.00626627
0.0205126 0.0522563
0.086885 0.0592989
-0.036223 0.00290485
0.0117782 0.00025452
-0.00553123 0.0563933
0.0582651 0.0593683
0.0281391 0.0606089
0.028139 0.0564962
0.106624 0.0606313
-0.0300731 0.00188778
0.033718 0.0585525
-0.0385424 0.0152207
0.102035 0.0570576
0.0496681 0.0585844
0.0496683 0.0587634
-0.00484737 0.0579035
0.0859467 0.0592173
0.0867201 0.0574168
-0.0271157 0.000923497
0.0385434 0.000858058
0.0594482 0.0693855
0.121243 0.0601988
0.0144704 0.00794819
-7.68032e-07 0.00931845
0.0492512 -0.00218042
-0.016115 0.0572993
0.0305923 0.0583914
0.129458 0.00653258
0.090741 0.0630438
-0.00236347 0.0581578
0.100097 0.064861
0.0562649 0.0571854
0.126169 0.058036
0.0997468 0.0588984
-0.036223 0.00290485
parameters: [ 9.449  2.195  2.235  1.2    4.   ]. error: 48433327.0943.
----------------------------
epoch 0, loss 1.34145
epoch 128, loss 0.804945
epoch 256, loss 0.935574
epoch 384, loss 1.16765
epoch 512, loss 1.00687
epoch 640, loss 0.894434
epoch 768, loss 0.840009
epoch 896, loss 1.05905
epoch 1024, loss 0.683373
epoch 1152, loss 0.886606
epoch 1280, loss 0.94813
epoch 1408, loss 0.845257
epoch 1536, loss 0.59067
epoch 1664, loss 0.510256
epoch 1792, loss 0.929872
epoch 1920, loss 0.69855
epoch 2048, loss 0.599806
epoch 2176, loss 0.787532
epoch 2304, loss 0.777295
epoch 2432, loss 0.725534
epoch 2560, loss 0.728948
epoch 2688, loss 0.573817
epoch 2816, loss 0.65287
epoch 2944, loss 0.636661
epoch 3072, loss 0.727328
epoch 3200, loss 0.757711
epoch 3328, loss 0.684769
epoch 3456, loss 0.711131
epoch 3584, loss 0.773843
epoch 3712, loss 0.622837
epoch 3840, loss 0.598326
epoch 3968, loss 0.827978
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0390895 0.0086138
0.0594482 0.0646703
0.0412776 0.00542742
0.0865811 0.0579888
0.0868848 0.060548
0.0927397 0.0586046
0.0109781 0.0121868
1.74905e-07 0.00990899
0.0710835 0.0592361
0.125873 0.0563829
0.000834782 0.00731078
-0.0707068 0.0090249
0.0859467 0.0592033
-0.0113495 0.00676588
0.0594398 0.0579663
0.0927399 0.0608232
-0.0118767 0.0587839
0.01684 0.0545316
0.0340225 0.00848487
-0.000835063 0.0097571
0.121243 0.0593197
0.0604514 0.056023
0.0424581 0.0544979
0.0201879 0.0569671
0.0407515 0.0554995
-0.0132848 0.0644928
0.0220539 0.0602775
-1.32922e-07 0.00686316
-0.0161248 0.0560841
0.0813321 0.0550967
0.0346348 -6.86451e-05
0.0424316 0.0554896
-0.0204101 0.0061805
0.0207193 0.0531158
0.0272916 0.0118055
-0.0109774 0.00933883
-1.17339e-07 0.00618078
0.0236479 0.0575227
0.0964305 0.0584385
0.0950804 0.0597843
-0.0110929 0.0528074
2.05834e-07 0.0091425
0.0752321 0.0554714
4.07609e-05 0.0113224
-0.0621346 0.00869548
0.0248097 0.00263936
0.0144846 0.00109422
0.0462066 0.0174372
0.0937753 0.0607722
0.0594398 0.0589201
0.0997462 0.0539694
4.57767e-08 0.011061
0.0174339 0.0562893
0.0210209 0.0600094
0.0224432 0.0541687
0.0605534 0.054471
-0.0295676 0.00896394
0.0952315 0.058777
0.109566 0.0580149
0.0865811 0.058764
0.0387924 0.0553187
0.0562648 0.0553805
0.0319943 0.0596069
0.0907411 0.0600921
0.0261394 0.0584327
0.0188023 0.00391795
-2.64683e-07 0.01434
0.0482639 0.0586017
-0.0111821 0.0525999
0.0594494 0.0644887
0.0207193 0.0557902
0.0631207 0.00304865
-0.0416423 0.00921245
0.0461385 0.0580481
0.0223972 0.0578336
0.0495763 -0.00115318
0.0189828 0.00415115
-0.041769 0.00312113
-0.0659495 0.00469704
0.0477957 0.0600913
-0.0234744 0.00585727
-3.91322e-05 0.00649721
-0.0412711 0.00984971
0.0140549 0.0545085
0.0317696 0.0587428
0.0752955 0.0547863
0.12614 0.0562328
-0.039089 0.00467798
0.0131848 0.00903408
0.0937753 0.0600234
0.0237988 0.0613763
0.0300726 0.00163503
0.0589764 0.058252
0.0248097 0.0139241
0.0305919 0.0557939
0.129453 0.00676856
0.0941706 0.0551823
0.0317699 0.0601879
0.130756 0.0601621
0.0857789 0.0572305
0.0589133 0.0537849
0.0594398 0.0586586
0.061289 0.0642071
0.000794682 0.00329371
-0.00378297 0.0106556
0.043628 0.0537456
0.0901953 0.0566227
-4.4075e-07 0.0103833
0.0631196 0.0080698
0.0405236 0.0585936
0.0396776 0.0550225
-0.0267425 0.0103931
0.0237989 0.0601529
0.0675205 0.0541523
0.0594397 0.0591431
0.0964772 0.0562427
-0.0210969 0.00376241
0.0191272 0.0568941
0.0724741 0.0563718
0.0661223 0.0586221
-0.00726943 0.0529136
0.12432 0.0560599
-0.016115 0.0553779
0.0223968 0.0564914
0.0594297 0.0549394
0.125873 0.0563829
3.36628e-05 0.0063299
0.0271109 0.00755462
parameters: [ 9.449  2.195  2.338  1.2    4.   ]. error: 286995832387.0.
----------------------------
epoch 0, loss 1.28964
epoch 128, loss 0.798056
epoch 256, loss 0.998916
epoch 384, loss 1.01219
epoch 512, loss 1.03452
epoch 640, loss 0.869722
epoch 768, loss 0.931851
epoch 896, loss 0.976363
epoch 1024, loss 0.829866
epoch 1152, loss 1.06586
epoch 1280, loss 0.818729
epoch 1408, loss 1.05929
epoch 1536, loss 0.732825
epoch 1664, loss 0.643415
epoch 1792, loss 0.706618
epoch 1920, loss 0.789885
epoch 2048, loss 0.774522
epoch 2176, loss 0.721468
epoch 2304, loss 0.719424
epoch 2432, loss 0.636133
epoch 2560, loss 0.854052
epoch 2688, loss 0.812038
epoch 2816, loss 0.790153
epoch 2944, loss 0.755857
epoch 3072, loss 0.712807
epoch 3200, loss 0.724394
epoch 3328, loss 0.53702
epoch 3456, loss 0.604549
epoch 3584, loss 0.655603
epoch 3712, loss 0.772822
epoch 3840, loss 0.683184
epoch 3968, loss 0.685964
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0897923 0.0561895
0.0952317 0.0563566
0.0562648 0.0531131
-0.00628321 0.050432
-0.0390959 -7.34731e-05
-0.0417624 0.00109924
0.0495768 0.0024862
0.0243594 0.0498348
0.0710835 0.0550314
-0.0494977 0.0060158
0.032195 0.0519014
-0.00699273 0.0508136
0.0594397 0.0577501
0.0317699 0.0557532
0.0321467 0.0555949
0.0247829 0.00647149
-0.0449599 0.0117297
-0.0414033 0.00435414
0.109566 0.0536538
0.0594396 0.0580253
0.0594398 0.0566387
0.0272988 0.00642455
-0.0267379 0.0185205
0.132184 0.0644359
0.0327312 0.0571997
-0.014471 0.00761878
-0.0860447 -0.00074144
0.000213748 0.0580725
0.0781223 0.0535844
0.0387924 0.0521092
0.0983621 0.0508957
0.0631173 0.000263394
0.121243 0.0552146
-0.0109774 0.00420306
0.01134 0.00561415
0.0625949 0.0561607
0.0464004 0.053672
0.059435 0.0540504
0.014471 0.0130527
0.00863088 0.0525084
0.12432 0.0494076
-0.0247793 0.00129644
-2.15513e-06 0.00388091
0.0594324 0.052658
0.0716719 0.00242478
0.0911173 0.0535721
-0.0412731 0.00180157
-0.0776729 0.00841246
0.0991789 0.0563238
0.0776735 0.0101555
0.0210212 0.0566198
0.059181 0.0519177
-0.0117822 0.00737686
0.0945038 0.0525154
-0.00553123 0.0489822
0.0188018 0.0585568
-0.0111018 0.0490789
-0.0362233 0.00845622
-0.0117788 -0.00236182
0.105503 0.0630117
-0.0385434 0.00549903
-0.00699273 0.0513189
0.100116 0.00114021
0.0390889 0.0022347
0.0251233 0.0583698
-0.0144706 0.00976031
0.0562851 0.0555532
0.0901949 0.0532462
0.133575 0.0508679
0.0340225 0.00722401
-0.0495737 0.00285385
0.0532677 0.0108917
0.0594399 0.0520615
0.0631207 0.00115433
-0.0621274 0.00283255
0.129445 0.0149933
0.0477959 0.052839
-0.0161572 0.0537157
-0.0247793 0.0121492
-0.0462049 0.00119099
-0.0716782 -0.00195145
0.0416418 0.0108299
0.05937 0.0502667
0.0950804 0.0563723
0.0436381 0.0530604
1.95996e-07 0.00491473
0.0593758 0.0513452
0.125873 0.0550893
0.0248076 0.00493864
0.0204028 0.00468798
0.0390991 0.00300282
0.0529729 0.00435473
0.059367 0.0498211
-0.0412731 0.0207094
0.0968257 0.055483
0.0594398 0.0552839
0.0152591 0.00798377
0.0122295 0.0530765
0.0986864 0.0522233
0.0964766 0.0523569
-0.0188606 0.0146322
-0.0271126 0.0069283
0.0237987 0.051494
-0.0272995 0.0126659
0.0144859 0.010677
0.0901952 0.0517082
-0.0272904 0.00928132
-0.0662833 0.000607752
0.0496681 0.0567619
0.0362237 0.0121573
0.0482639 0.0557962
0.0326439 0.000534815
0.0327313 0.0544831
0.0710836 0.0552774
0.102035 0.0510832
0.0327313 0.0550531
0.0286789 0.0526823
-0.0056768 0.0469795
0.0971043 0.0562323
0.0301505 0.0545996
0.0414051 0.00618533
-0.0532547 0.015174
0.104819 0.0534937
0.0621334 0.0102926
0.0267393 0.0137657
-3.17974e-05 0.00943576
-5.97989e-06 -0.000685646
0.0910569 0.0548434
parameters: [ 9.449  2.195  2.386  1.2    4.   ]. error: 995774765.699.
----------------------------
epoch 0, loss 1.08873
epoch 128, loss 0.784163
epoch 256, loss 0.835258
epoch 384, loss 0.694456
epoch 512, loss 0.999237
epoch 640, loss 0.66897
epoch 768, loss 0.936829
epoch 896, loss 0.708331
epoch 1024, loss 0.899527
epoch 1152, loss 0.684612
epoch 1280, loss 0.611403
epoch 1408, loss 0.625285
epoch 1536, loss 0.785533
epoch 1664, loss 0.471589
epoch 1792, loss 0.973956
epoch 1920, loss 0.640576
epoch 2048, loss 0.679677
epoch 2176, loss 0.764841
epoch 2304, loss 0.692051
epoch 2432, loss 0.637162
epoch 2560, loss 0.584836
epoch 2688, loss 0.720663
epoch 2816, loss 0.76698
epoch 2944, loss 0.621781
epoch 3072, loss 0.574872
epoch 3200, loss 0.572044
epoch 3328, loss 0.622795
epoch 3456, loss 0.561883
epoch 3584, loss 0.724989
epoch 3712, loss 0.612699
epoch 3840, loss 0.662702
epoch 3968, loss 0.764017
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594398 0.0563353
0.0642209 0.052695
0.102035 0.0587738
0.0243585 0.0539484
1.96444e-05 -0.000267807
0.0368959 0.0545972
0.0424428 0.0565117
0.0593665 0.0538966
-4.6184e-07 0.0109956
0.0362237 0.0041268
0.0217933 0.0633503
0.0385434 0.00917667
0.081797 0.0589875
0.0710835 0.0589008
0.0626054 0.0585284
0.132184 0.0685774
0.0594396 0.0600217
0.0267781 0.0580609
0.0626054 0.0605057
-3.91322e-05 0.00875546
0.134985 0.0569848
0.0396776 0.0587014
0.0594399 0.0546556
0.0414025 -0.0001145
0.0920508 0.0608978
-0.0414059 0.00773878
0.0188023 0.00266833
-0.00893962 -0.00115735
0.000835126 0.000876483
0.0820225 0.0554743
0.0337182 0.0596299
-0.0131841 -0.000128019
0.0764272 0.0587675
0.0776735 0.00212086
0.0594397 0.0596737
0.121243 0.0609102
0.0621308 0.00646334
0.00142251 0.000826833
0.0375382 0.0599832
0.0272916 0.0150054
-7.68032e-07 0.0136194
0.0210958 0.00345988
0.0950805 0.0594408
0.0329331 0.0580132
0.0323166 0.00211959
-0.0132904 0.0655308
-0.00460347 0.0055429
-0.0662867 -0.000919859
-0.00728242 0.0522351
0.0439209 0.0583388
0.0594395 0.0606887
0.0927398 0.0543288
0.0582651 0.0561694
0.0396776 0.0562623
-0.0417573 -2.32785e-05
0.0333262 0.0626314
-0.0416401 0.00778987
0.0950804 0.0565325
0.0861487 0.0579339
0.0199307 0.0640931
0.0133907 0.0679673
0.0290868 0.0581945
0.0305923 0.0589874
0.0599029 0.057057
0.0319947 0.0577123
-0.0529682 0.0016295
0.0414075 0.008973
0.0861487 0.0594748
-0.053257 0.00338015
0.0706156 0.0572881
0.0329331 0.0561906
-0.0111732 0.0528131
0.0677309 0.0551937
-0.0188062 -0.00272822
-0.0234744 0.0115954
0.0882822 0.0576701
-0.014471 0.0164357
0.0920969 0.0546502
0.088282 0.0563927
0.102035 0.0597662
0.0385427 0.00451457
0.0417484 -0.000419079
0.0867335 0.0593374
0.0904958 0.0545347
0.0301505 0.0589347
0.0467944 -2.63295e-05
0.0859465 0.0582779
0.0477956 0.0603144
0.0593133 0.0542473
0.0220539 0.0557798
0.0859465 0.0563291
0.0174335 0.0532873
-0.129445 0.00676693
-0.0131855 0.000851233
0.0626002 0.0586983
0.0197009 0.0542756
0.0211183 0.0630261
0.0594399 0.0543054
0.0385431 0.0114022
3.40452e-06 -0.00116274
0.042448 0.0584788
0.0562851 0.0597186
0.0851612 0.0592056
0.106629 0.061122
-0.0734312 0.0103549
0.0907408 0.0546815
0.0991788 0.0558225
0.104819 0.0571651
-0.00236368 0.0574459
0.0261391 0.055078
0.0458117 0.0600863
0.0710838 0.0571852
-0.000835392 0.0040752
0.0594027 0.0547224
0.106655 0.0601976
0.0417448 0.00579851
0.0318695 0.0579618
0.135017 0.0576538
0.05937 0.0536709
0.0385437 0.00534899
0.044975 0.0643164
0.0385437 0.00994382
0.05937 0.0536709
0.0278229 0.0551931
0.000807032 0.00286391
-0.0109842 0.0128501
0.00250001 0.00592383
0.109566 0.0576597
parameters: [ 9.449  2.195  2.365  1.2    4.   ]. error: 1362836.74455.
----------------------------
epoch 0, loss 1.11683
epoch 128, loss 1.216
epoch 256, loss 1.01577
epoch 384, loss 0.824493
epoch 512, loss 1.10168
epoch 640, loss 0.837882
epoch 768, loss 0.747914
epoch 896, loss 0.798826
epoch 1024, loss 0.843341
epoch 1152, loss 0.732505
epoch 1280, loss 0.677007
epoch 1408, loss 0.810288
epoch 1536, loss 0.716093
epoch 1664, loss 0.72999
epoch 1792, loss 0.767696
epoch 1920, loss 0.551129
epoch 2048, loss 0.633436
epoch 2176, loss 0.721481
epoch 2304, loss 0.805497
epoch 2432, loss 0.690813
epoch 2560, loss 0.701488
epoch 2688, loss 0.758533
epoch 2816, loss 0.643194
epoch 2944, loss 0.682814
epoch 3072, loss 0.667011
epoch 3200, loss 0.779952
epoch 3328, loss 0.81877
epoch 3456, loss 0.741526
epoch 3584, loss 0.529491
epoch 3712, loss 0.641949
epoch 3840, loss 0.750998
epoch 3968, loss 0.556994
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594759 0.0565789
0.0594398 0.0621153
0.102034 0.0584709
-3.66166e-06 0.0183838
0.130757 0.0616236
0.00131949 0.00189105
-4.4075e-07 0.0170708
-0.0207316 0.00133739
0.080382 0.0595524
0.0385437 0.00735732
-0.0390898 0.000865468
0.0367961 0.0117628
0.0727411 0.0616869
0.0707066 1.67087e-05
-0.0234626 0.0150554
0.0752321 0.0609832
0.0626002 0.0605691
0.0594374 0.0602005
1.95996e-07 0.00715994
0.0199306 0.0641128
0.0439209 0.0608492
0.0337179 0.061432
0.129993 0.0550472
-0.0394796 0.0221769
0.0346447 0.00636067
0.121243 0.0602588
0.0691068 0.0586917
0.0971047 0.0639663
0.0927396 0.0628695
-2.15513e-06 0.0101808
-0.0144713 0.00339439
0.0859465 0.0623071
0.0582589 0.0558571
-0.0346424 0.00432659
0.088282 0.0590618
-0.000835377 0.00030762
0.0599612 0.0592551
-0.00726943 0.0528989
0.0532657 0.00936025
0.0201879 0.0581561
0.0271115 -0.00266567
0.0417454 0.00234188
0.00142251 0.00421364
0.081797 0.0580852
8.40829e-06 0.00498503
-0.00460347 -0.000774353
0.0707062 0.00421457
0.0330953 0.0589439
-0.129449 0.0064709
0.0321468 0.0612925
0.0897923 0.0593446
0.0532578 0.00705955
-0.0248069 0.0106956
0.0462156 0.00170916
0.110244 0.0603168
-1.17339e-07 0.00982975
0.0968257 0.0619277
-0.0857218 0.00251329
0.100097 0.0644373
0.0594494 0.0684495
-1.17339e-07 0.0182987
0.022444 0.0599402
0.0813321 0.0606194
0.0594297 0.0611312
0.118684 0.0656829
0.0236481 0.0618391
1.13647e-06 0.00234528
0.0594399 0.0590285
0.0220537 0.0617979
-0.0248165 0.0127102
0.0317699 0.0606968
0.0223968 0.0585979
0.118684 0.0656829
0.059435 0.0672459
-0.0467928 0.00201465
-0.0106908 0.00898415
0.061289 0.0681704
0.0727409 0.0594054
0.126169 0.0547636
0.00460075 -0.000948689
0.014471 0.0164595
-0.0416468 0.00450809
-0.0267352 0.00454154
-0.0734361 0.00636511
-0.0023637 0.05972
0.0407513 0.0582095
0.0174339 0.0597355
-0.0707068 0.0118514
0.0204119 0.00635709
0.0867333 0.0609331
0.0267781 0.057502
0.0859467 0.0614908
0.0318693 0.0577903
0.0527578 0.0625117
0.0273016 0.00505474
0.0317696 0.0611736
0.0589133 0.0597845
0.0727411 0.0606105
0.0710835 0.0625502
-0.00378297 0.00849752
0.0927397 0.060614
0.0188086 -0.00211553
0.0477956 0.0594803
0.106634 0.0598219
0.0594397 0.0592936
-0.0250779 0.0585437
-0.053266 0.00404475
0.0937752 0.0641147
0.0261394 0.0620158
0.0290868 0.0608339
-0.0394796 0.0216313
0.00460075 0.0066288
0.088729 0.0613027
0.0317697 0.0622703
0.0691068 0.0593466
0.026809 0.0603304
0.0122295 0.0600274
-0.02083 0.0136196
-0.0611171 0.00531565
-0.0131855 0.0174986
0.0477958 0.0617039
0.0236479 0.0578077
0.0650734 0.0596318
0.0871097 0.0595546
0.0594481 0.0677768
0.0529686 0.00542539
0.0964772 0.0606349
-0.0113401 0.0126383
parameters: [ 9.449  2.195  2.376  1.2    4.   ]. error: 2562333203.85.
----------------------------
epoch 0, loss 0.962226
epoch 128, loss 0.988356
epoch 256, loss 1.02594
epoch 384, loss 1.06935
epoch 512, loss 0.888073
epoch 640, loss 0.998365
epoch 768, loss 1.0215
epoch 896, loss 0.697151
epoch 1024, loss 0.749661
epoch 1152, loss 0.753804
epoch 1280, loss 0.593052
epoch 1408, loss 0.689948
epoch 1536, loss 0.810987
epoch 1664, loss 0.71145
epoch 1792, loss 0.65312
epoch 1920, loss 0.80005
epoch 2048, loss 0.797895
epoch 2176, loss 0.661164
epoch 2304, loss 0.788705
epoch 2432, loss 0.728167
epoch 2560, loss 0.61831
epoch 2688, loss 0.605188
epoch 2816, loss 0.735068
epoch 2944, loss 0.718081
epoch 3072, loss 0.685437
epoch 3200, loss 0.644433
epoch 3328, loss 0.791002
epoch 3456, loss 0.822847
epoch 3584, loss 0.671458
epoch 3712, loss 0.846115
epoch 3840, loss 0.703514
epoch 3968, loss 0.612802
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.00250418 0.000744079
-0.0414014 0.000846454
0.0237987 0.0496244
0.044975 0.0601051
0.0237986 0.0528988
0.0390889 -0.00191326
0.0412752 -0.000593248
0.0882823 0.0517197
0.059435 0.054838
0.0496682 0.0556527
-0.0362233 0.010615
0.0594398 0.0527575
0.059367 0.0508497
0.121243 0.054418
0.100668 0.0516191
0.0734394 0.00534009
-0.0494953 0.00910439
-0.0117783 0.00175587
0.0920961 0.0535932
0.0859465 0.0548922
0.0971047 0.0627275
0.0904958 0.052503
-0.129453 -0.00322609
0.0170834 0.0605714
0.0907409 0.0559504
0.129925 0.0528985
0.109566 0.053969
0.0283783 0.0519615
0.0211185 0.0595786
0.00863088 0.0512324
-0.0295676 0.0011759
0.0621334 0.00558777
0.0248169 0.0106782
0.121243 0.0535495
-0.0621274 -0.00412143
0.0594562 0.0523364
0.0222151 0.0510138
0.0144707 0.0154319
0.0387924 0.0523889
-0.0113495 -0.00232384
-0.080798 0.00109948
0.0859467 0.0533988
0.0458117 0.052025
0.0131845 -0.000552024
-0.0204001 0.0117667
0.0952317 0.0525211
-0.0189824 0.00571751
0.0791969 0.0508364
-0.0113401 0.000218053
0.0487559 0.0547753
0.0113418 0.00529291
0.0346452 -0.00422998
0.0594398 0.0540999
0.0971043 0.0627718
0.0396776 0.0522272
0.0594398 0.0552869
0.0223972 0.0524623
0.0117785 0.00470345
0.102035 0.0539589
0.0648671 0.0540547
0.0589127 0.0523094
0.0496681 0.0536179
0.00459996 -0.00352203
-0.0144706 0.00189194
0.034034 -0.0025794
0.014471 0.00728856
0.0271117 -0.00234475
0.0677309 0.0536898
0.0631196 -0.00423029
0.0390986 -0.00487739
-0.0621343 0.000758034
0.000213351 0.0599732
0.0286795 0.0514134
-0.0106908 0.00449316
0.0734439 0.0149884
2.96104e-08 0.00340206
0.0871096 0.0543219
0.0224432 0.0535874
0.0384777 0.0540527
0.0667826 0.055018
0.0691068 0.051598
0.0867199 0.056238
0.0594398 0.0539245
0.034034 0.010608
0.0594473 0.0521045
0.0211182 0.0600486
0.0527577 0.0548219
0.0662801 -0.00296758
-0.00484737 0.0500306
0.0375434 0.0533482
0.0710836 0.0536467
0.0589765 0.055064
0.0986864 0.0533719
0.0866799 0.0548756
-0.129445 0.0093914
0.0691068 0.0513548
0.0529666 0.00113509
-0.0189887 0.00138808
0.0611263 -0.00442908
0.0813321 0.0559208
-0.0204028 0.00235053
0.0248076 0.0110875
0.0594374 0.0546007
0.0109781 0.000969079
3.82209e-05 0.0131589
-0.0449599 0.00574069
-0.0390959 0.000463926
-0.0188599 0.00609794
0.0251233 0.0618226
0.0626054 0.0542301
0.0458116 0.0549024
0.0650733 0.051333
0.081797 0.0530641
-0.0860447 -0.00220886
0.109549 0.0516527
-0.0300731 -0.00221218
0.0865811 0.0559702
0.101814 0.0622508
-0.0132965 0.0674883
-0.0449599 -0.00249194
0.0966587 0.0544793
0.0861486 0.054673
0.0122243 0.0535218
0.101441 0.0547063
0.0322983 0.0545285
0.0278229 0.0548162
0.0390991 0.00581953
0.0461382 0.0558569
parameters: [ 9.449  2.195  2.382  1.2    4.   ]. error: 44744692683.0.
----------------------------
epoch 0, loss 1.13542
epoch 128, loss 0.994801
epoch 256, loss 0.956517
epoch 384, loss 0.809713
epoch 512, loss 0.708094
epoch 640, loss 0.797626
epoch 768, loss 0.807682
epoch 896, loss 0.921192
epoch 1024, loss 0.715991
epoch 1152, loss 0.753177
epoch 1280, loss 0.713291
epoch 1408, loss 0.621275
epoch 1536, loss 0.745139
epoch 1664, loss 0.604465
epoch 1792, loss 0.781354
epoch 1920, loss 0.714922
epoch 2048, loss 0.690682
epoch 2176, loss 0.585624
epoch 2304, loss 0.770694
epoch 2432, loss 0.673586
epoch 2560, loss 0.778538
epoch 2688, loss 0.657162
epoch 2816, loss 0.688465
epoch 2944, loss 0.733777
epoch 3072, loss 0.725273
epoch 3200, loss 0.627003
epoch 3328, loss 0.60778
epoch 3456, loss 0.593056
epoch 3584, loss 0.734225
epoch 3712, loss 0.553678
epoch 3840, loss 0.733781
epoch 3968, loss 0.706617
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594398 0.0611253
0.0220538 0.0582864
0.0857284 -0.00238078
0.028139 0.0600221
0.0251234 0.0617278
0.0807987 0.00782858
-0.0161473 0.0571868
0.0594396 0.0596723
0.022444 0.0565218
0.0594251 0.0575376
-0.0621346 -0.00384273
0.0594297 0.0578849
-0.0177216 0.00405516
0.0414001 0.00678705
-6.0681e-07 0.00531239
0.0286789 0.0552589
-0.0113495 0.000937135
0.0327313 0.0592252
0.000794682 0.00508081
0.0631207 0.00162065
0.0625949 0.0596941
0.0496681 0.0590037
0.135017 0.0600512
0.0667826 0.0598062
0.0093143 0.0609586
-0.0111018 0.0511716
0.0996717 0.0614558
-0.0416468 0.00045256
-0.0412711 0.0150854
-0.129457 0.00856399
-7.68032e-07 0.0151481
0.0310444 0.0558522
0.0991789 0.0592226
0.0857284 -0.000475674
-0.0734384 0.00277495
-0.0412685 0.00648494
0.0626106 0.0586933
-0.0462169 0.00827741
-0.0188606 -0.00176951
-0.0318771 0.00287251
0.0208245 0.00899026
0.0594395 0.0602809
0.129916 0.054489
0.0330953 0.0569743
0.0496681 0.0596738
0.0477956 0.0594315
0.124354 0.0558376
0.105503 0.0679843
0.01684 0.0560251
0.0170834 0.061302
-0.0776729 0.00293043
0.0271115 -0.00364331
5.31452e-07 0.0173698
-0.0413988 0.0103368
-0.0494953 0.00465828
0.0396776 0.0567533
9.99101e-07 -0.0020348
-0.0207367 -0.000277571
0.0606162 0.0538758
0.0739226 0.0624666
0.0511643 0.0535014
0.0599031 0.0604801
0.062595 0.0596384
0.0800821 0.0577683
0.0482639 0.0606122
0.0191276 0.0555339
0.0966595 0.0567444
0.0541227 0.0527778
0.102034 0.0576542
0.0739226 0.0634675
0.0991789 0.0600988
0.0373475 0.05719
-0.0204122 0.00346911
-0.000792737 -0.0019956
0.0594399 0.0613905
0.00378707 0.000349068
0.0997462 0.0583815
0.0964307 0.0561795
-0.0106908 -0.00166697
0.0236482 0.0597512
0.0417484 -0.00562802
-0.0111732 0.0530781
0.0592543 0.0542672
0.0482639 0.0604943
0.0871095 0.0586886
4.57767e-08 0.00586046
0.0318693 0.0555531
0.0643397 0.0536816
0.105503 0.068857
0.0417489 0.00178823
0.118684 0.0635704
0.0710835 0.059477
0.0594398 0.0600715
0.0346353 -0.0041805
0.0210981 -0.00229918
0.0261392 0.0617541
0.0327314 0.0592128
-4.71368e-07 0.00118945
0.0131855 -0.00213482
0.0594398 0.0590831
0.0370769 0.0561339
0.0997468 0.0583976
0.0131845 0.00825961
0.0322983 0.0588667
0.0326439 -0.00469783
0.0594396 0.0594771
0.100668 0.0570272
0.0527578 0.0611445
-0.0215238 0.00148109
-0.00236348 0.0576839
0.0594374 0.0583178
0.0604514 0.0544814
0.0730676 0.0594698
0.0904958 0.0577183
0.0222152 0.0564267
0.0977801 0.0623036
0.0861486 0.0601066
0.0626054 0.0574642
-0.00142797 0.0035463
0.129445 -0.000978618
0.0783558 0.0602137
0.0273016 0.00326412
0.0271117 0.00145164
0.0594251 0.0588351
-0.080798 0.0152621
0.0611263 -0.00211015
-0.0109566 0.00480646
0.0215366 0.00149315
parameters: [ 9.449  2.195  2.382  2.2    4.   ]. error: 58643069023.9.
----------------------------
epoch 0, loss 1.10741
epoch 128, loss 0.882326
epoch 256, loss 1.15017
epoch 384, loss 0.682082
epoch 512, loss 0.96327
epoch 640, loss 0.891564
epoch 768, loss 0.931678
epoch 896, loss 0.853439
epoch 1024, loss 0.992008
epoch 1152, loss 0.740122
epoch 1280, loss 0.733339
epoch 1408, loss 0.709092
epoch 1536, loss 0.798514
epoch 1664, loss 0.693692
epoch 1792, loss 0.692156
epoch 1920, loss 0.769301
epoch 2048, loss 0.635904
epoch 2176, loss 0.93376
epoch 2304, loss 0.699055
epoch 2432, loss 0.614476
epoch 2560, loss 1.01921
epoch 2688, loss 0.699055
epoch 2816, loss 0.86507
epoch 2944, loss 0.710251
epoch 3072, loss 0.742022
epoch 3200, loss 0.65406
epoch 3328, loss 0.74704
epoch 3456, loss 0.677494
epoch 3584, loss 0.630557
epoch 3712, loss 0.777799
epoch 3840, loss 0.817007
epoch 3968, loss 0.628996
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0527576 0.0458238
-0.0385434 0.0143102
-7.68032e-07 0.0127336
0.0927397 0.0554729
-0.034636 0.00661015
0.0122194 0.0582413
0.0424369 0.0614209
0.0197009 0.0556041
-2.15513e-06 0.000242685
0.0305919 0.0502398
0.0920969 0.054782
0.0267779 0.0553371
0.00459996 -0.00186518
0.0904958 0.058451
0.133575 0.0561519
-0.0621272 0.00881015
0.0170834 0.0611782
-0.049248 0.00330452
0.00378707 0.00463499
0.0237988 0.049797
0.0237989 0.0529395
0.088729 0.0576986
0.0281391 0.0607225
0.0394803 0.0116998
-0.016115 0.0588863
0.0710836 0.0570689
-0.0152583 0.00772223
0.0594398 0.0593697
0.0887289 0.0592893
0.0368964 0.0562699
0.0594919 0.0437595
0.0716711 0.00198957
0.092056 0.0553594
0.00863088 0.0543992
0.129993 0.0544014
0.101441 0.0500237
0.0394799 0.00768626
0.0861487 0.0595978
0.110244 0.0596895
0.026809 0.0572987
-6.30661e-07 0.00809842
0.0941706 0.0535713
0.0800819 0.0579082
1.13647e-06 0.0139015
0.0716773 0.00486408
0.0327313 0.0538577
-0.0248182 0.00579023
0.0215366 0.00745885
0.0592247 0.0540258
0.0606162 0.0551793
0.0710837 0.0550991
0.0541227 0.0556362
0.0813321 0.0557539
0.00460075 -0.0018945
0.0191276 0.0539677
0.0659615 0.025692
0.0643397 0.0538307
0.0907409 0.050192
0.00863092 0.0581324
0.0330953 0.0576263
-0.0807986 0.00998145
0.0920561 0.0558443
-0.014485 0.00625995
0.0589763 0.0582747
0.0113515 0.00158628
0.101441 0.0550717
-0.0621272 -0.0061954
0.0971047 0.0617415
0.0113418 0.0106199
0.0152591 0.00864352
0.0188019 0.061219
0.0827282 0.00259358
0.0144704 0.0104611
-0.0532547 0.0178862
0.0215366 0.00450269
0.0144859 0.00483958
0.0541227 0.0545059
0.00891968 0.000573282
0.0301504 0.0575543
0.0527576 0.0567306
0.0989678 0.0615511
-0.0495799 0.00135827
0.0210981 -0.000416885
0.0859466 0.0583717
0.0037846 0.00467211
0.0234681 0.00735056
0.0327314 0.0559984
-0.00236373 0.0541365
0.0920961 0.052079
0.0109781 0.0178331
0.0182077 0.055997
-0.0271157 0.00552655
0.0851613 0.0526226
0.0234754 0.0226464
-0.065961 0.0163759
0.022215 0.0539283
0.015257 0.0130463
0.0562849 0.057876
0.0871095 0.0474273
0.0791961 0.0472581
0.0394799 -0.000783972
0.0631173 -0.00164524
-0.00142113 0.0108409
0.000835111 0.00765373
0.00894922 0.00604546
0.0237989 0.04653
-0.0495737 0.00178084
0.0385427 0.00857994
0.0122404 0.0552438
0.0594396 0.0528103
0.0281392 0.0569407
0.0937752 0.0606208
0.0594396 0.0545513
0.112932 0.0617551
0.0594396 0.0547912
0.0210212 0.0619633
-0.129445 7.83921e-05
0.0199309 0.061221
0.0272916 0.00184702
-0.00700262 0.0578209
-0.0117783 0.00840164
0.0626106 0.0598857
0.0901953 0.0533869
0.0977802 0.0609559
0.09274 0.0606966
0.101441 0.0535921
-0.0385427 0.00649191
0.059407 0.0535441
parameters: [ 9.449  2.195  2.382  0.418  4.   ]. error: 165483475.677.
----------------------------
epoch 0, loss 0.912236
epoch 128, loss 0.909878
epoch 256, loss 0.938844
epoch 384, loss 0.856535
epoch 512, loss 0.82347
epoch 640, loss 1.00363
epoch 768, loss 0.811745
epoch 896, loss 0.880242
epoch 1024, loss 0.761666
epoch 1152, loss 0.813809
epoch 1280, loss 0.88831
epoch 1408, loss 0.643074
epoch 1536, loss 0.717408
epoch 1664, loss 0.759138
epoch 1792, loss 0.771728
epoch 1920, loss 0.802024
epoch 2048, loss 0.676592
epoch 2176, loss 0.69926
epoch 2304, loss 0.702399
epoch 2432, loss 0.787214
epoch 2560, loss 0.671777
epoch 2688, loss 0.703619
epoch 2816, loss 0.759959
epoch 2944, loss 0.669359
epoch 3072, loss 0.756468
epoch 3200, loss 0.772639
epoch 3328, loss 0.682945
epoch 3456, loss 0.667452
epoch 3584, loss 0.802162
epoch 3712, loss 0.717772
epoch 3840, loss 0.55385
epoch 3968, loss 0.736555
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0199307 0.059533
0.121243 0.0585654
0.0764272 0.0566222
-0.00628221 0.0553434
0.0870053 0.054434
0.0716773 0.00205512
0.0417702 0.00465223
0.0373484 0.0546869
0.0375434 0.0555101
0.0224432 0.0510641
0.132184 0.0649042
0.0882822 0.0559154
0.0327312 0.0518795
0.00894003 0.000584346
0.0865812 0.0563778
0.0865811 0.0550042
0.00142251 0.0169669
0.121243 0.0551997
-0.0267398 0.00501135
-3.66166e-06 0.0073461
-0.0390966 -0.0028482
0.0520969 0.0553599
0.135027 0.05637
0.00250001 -0.000113684
0.0271185 0.0048059
-0.00733491 0.0511966
0.106645 0.0543615
0.000786701 0.00333187
0.0271163 0.000264273
0.0562851 0.0530102
0.0323166 0.00097035
-0.0161248 0.0522783
3.82209e-05 0.0156356
0.0677309 0.0467718
0.0346353 -3.44022e-05
0.0197009 0.0541994
0.0301505 0.0541302
0.0866799 0.057855
0.130756 0.0584482
0.0599612 0.051741
0.0818743 0.049768
0.0194912 0.00164994
0.129993 0.0507333
8.39808e-07 0.00405268
-0.0023636 0.0540179
0.0968256 0.0570626
0.0174335 0.0515467
0.0234638 0.0047277
0.135017 0.0584088
0.0857795 0.0537449
0.01684 0.0518685
0.014471 0.0188302
0.0691985 0.056383
0.0329332 0.0576278
0.0662931 0.00986784
0.0625949 0.0542062
0.0268143 0.0546449
-0.0857218 -0.000580824
0.0487559 0.0550884
0.0871097 0.0578647
0.0867334 0.0550888
-0.0247813 0.00912613
-3.11621e-07 0.0110905
-0.0117817 -0.00364788
-0.0417422 0.000300919
0.0861488 0.0559285
0.0594398 0.054716
-0.0210896 0.0115271
-0.00484737 0.0485276
0.0117847 0.00234963
0.0117782 -0.000332746
0.130757 0.059047
-0.0188593 0.0113375
-0.0109489 -0.00167692
-0.00700272 0.0577311
0.129925 0.0497656
0.0330953 0.0499973
-0.000795203 0.00601491
0.121243 0.0596059
0.0223968 0.0509734
0.0144859 0.00374268
0.0188015 0.0597523
-0.0111821 0.0505025
0.0144846 0.00294524
0.0497671 0.0519517
0.0327313 0.0523381
2.41162e-06 0.0115136
0.100097 0.061419
-0.0271111 0.00584668
0.0734322 0.00139381
0.0511643 0.0508079
0.0188603 0.00618894
0.0541426 0.0507629
-0.0318771 1.7003e-05
0.125873 0.0587753
0.0867067 0.0535073
0.0907409 0.0548084
0.0191272 0.0551384
0.0509553 0.0479542
-0.0462076 0.00222635
0.0477956 0.0533615
-0.0662833 -0.000371951
-0.053257 0.0114814
-0.0529698 0.00397147
0.124277 0.0514134
-0.0495737 -0.00186438
0.000213351 0.0585863
2.05834e-07 0.00362138
0.0234754 0.0067663
0.0594467 0.0637496
0.0538009 0.0507258
-0.0267425 0.00270139
-0.0631101 0.00454141
-4.4075e-07 0.00193361
0.121243 0.0574249
-0.0716686 0.00358562
0.129458 0.0144838
-0.0529722 -0.00266564
-0.00249422 0.00857545
0.0532535 0.0496527
0.0191276 0.0560258
-0.0234626 0.00143247
0.12614 0.0536015
0.0920969 0.0520376
0.0416434 0.000944068
0.121243 0.0590241
0.0272916 0.0016928
-0.000801651 0.00739919
parameters: [ 9.449  2.195  2.382  3.036  4.   ]. error: 410580420.77.
----------------------------
epoch 0, loss 1.03133
epoch 128, loss 1.27433
epoch 256, loss 0.794391
epoch 384, loss 0.954884
epoch 512, loss 0.672736
epoch 640, loss 0.901375
epoch 768, loss 0.901127
epoch 896, loss 1.00204
epoch 1024, loss 1.10311
epoch 1152, loss 0.897815
epoch 1280, loss 0.974118
epoch 1408, loss 0.467876
epoch 1536, loss 0.929921
epoch 1664, loss 0.904393
epoch 1792, loss 0.59541
epoch 1920, loss 0.742649
epoch 2048, loss 0.59774
epoch 2176, loss 0.988255
epoch 2304, loss 0.818123
epoch 2432, loss 0.663923
epoch 2560, loss 0.568786
epoch 2688, loss 0.611239
epoch 2816, loss 0.542791
epoch 2944, loss 0.687064
epoch 3072, loss 0.456654
epoch 3200, loss 0.790609
epoch 3328, loss 0.706005
epoch 3456, loss 0.372791
epoch 3584, loss 0.847227
epoch 3712, loss 0.637497
epoch 3840, loss 0.523659
epoch 3968, loss 0.60331
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.060558 0.0444253
0.0281391 0.0469391
0.134985 0.0534334
0.0952317 0.0486658
0.0204045 -0.00195854
0.000786701 0.00201873
0.110244 0.0482595
0.0691072 0.0490233
0.0594397 0.0497186
0.0611263 -0.00315884
-0.0326435 -0.00449424
0.0390889 -0.00513948
0.0859466 0.0476067
-0.0367964 0.00781363
0.0727412 0.0476335
0.0174335 0.0465508
0.0317699 0.0473155
0.0319947 0.0514625
0.088282 0.0480857
0.0861486 0.0501116
-0.0144843 -0.000236995
-0.00735889 0.0439953
0.0267393 -0.0032221
0.0730674 0.0523609
0.041765 0.00233562
0.0390986 -0.00320318
-0.0346416 -0.00567344
-1.32922e-07 0.00887031
0.0370775 0.0469184
-0.0385424 5.65098e-05
0.0611204 -0.00408817
-7.99663e-06 -0.00181937
0.0278231 0.0512615
0.0188019 0.0580303
-0.0267352 -0.00178825
0.0375434 0.0528142
-0.0417792 0.000573234
-0.0385437 0.00819747
0.102034 0.0479097
-0.0707062 0.000818537
0.0594396 0.0500518
0.0859467 0.0508875
0.102034 0.0509611
0.0199306 0.0584504
-0.0247813 -0.00279783
-0.0412757 0.00154576
1.96444e-05 -0.000585391
0.020735 -0.000489178
0.0511643 0.046268
0.0223968 0.0487894
-0.0111018 0.044153
0.042448 0.0503903
0.0251231 0.0595178
-7.99663e-06 0.000120141
0.0860428 -0.00397365
-0.0109842 -0.000217243
0.0866799 0.05347
0.0691068 0.0495896
0.0621334 -0.000969357
0.0131861 0.00110083
-0.00894125 -0.00395319
0.0199306 0.0587695
0.0803921 0.0548531
0.0989675 0.0601063
0.0783558 0.0531281
0.0820225 0.0434383
0.0317697 0.0468535
0.0764272 0.0514495
0.0396777 0.0491391
0.0813321 0.0522613
0.0868849 0.0560473
0.0901953 0.0505243
-0.0468045 0.00134721
0.0661224 0.0489491
-0.0177216 0.000291892
0.0677309 0.0444825
-0.0188057 -0.000272933
0.0122352 0.0483464
0.0964772 0.0486366
-1.48505e-07 0.00336792
0.0373484 0.0477834
0.0140549 0.0486657
0.0596272 0.0448857
0.0606162 0.0444001
0.0529709 -0.00135271
0.0330953 0.0450122
0.0813321 0.0531704
0.0131845 -0.00319661
0.0791268 0.0497505
0.0170933 0.0603506
0.0983621 0.0491459
-0.0707068 0.00397815
0.0650733 0.0477096
0.0375381 0.0531136
0.0964766 0.0483867
0.124311 0.0464487
0.0764118 0.0528871
0.081527 0.050981
-0.080798 0.00513049
0.0170933 0.0603506
-0.0161572 0.0515173
3.5013e-07 0.00128955
0.0152591 0.00130889
0.126222 0.046813
0.05627 0.0536751
0.059435 0.067335
0.0815268 0.0506567
-0.0662867 -0.00432563
0.0223972 0.0466792
0.0706155 0.0506332
0.0734322 -0.00143116
-2.86114e-05 -0.00193742
-6.15975e-07 -0.00095494
0.0867067 0.0515397
0.0813321 0.0519008
-0.0295676 0.0022358
0.0997468 0.0493997
-0.00378521 0.00774838
0.0385431 0.000956581
2.05834e-07 0.00219673
0.0207331 0.00993648
-0.0248165 0.00610552
0.0867333 0.0511241
0.032933 0.050984
0.0407513 0.0487379
-0.0318771 -0.000247537
-2.64683e-07 0.00106272
0.0387924 0.0474723
parameters: [ 9.449  2.195  2.382  0.418  4.   ]. error: 117668343.038.
----------------------------
epoch 0, loss 1.35841
epoch 128, loss 1.0339
epoch 256, loss 0.952375
epoch 384, loss 1.00703
epoch 512, loss 0.96763
epoch 640, loss 0.774219
epoch 768, loss 0.827675
epoch 896, loss 0.836506
epoch 1024, loss 0.758262
epoch 1152, loss 0.669432
epoch 1280, loss 0.763636
epoch 1408, loss 0.659696
epoch 1536, loss 0.886183
epoch 1664, loss 0.806318
epoch 1792, loss 0.57035
epoch 1920, loss 0.815526
epoch 2048, loss 0.924056
epoch 2176, loss 0.848784
epoch 2304, loss 0.825057
epoch 2432, loss 0.763913
epoch 2560, loss 0.586836
epoch 2688, loss 0.652018
epoch 2816, loss 0.687485
epoch 2944, loss 0.655546
epoch 3072, loss 0.605632
epoch 3200, loss 0.591874
epoch 3328, loss 0.776651
epoch 3456, loss 0.725189
epoch 3584, loss 0.672878
epoch 3712, loss 0.803474
epoch 3840, loss 0.545714
epoch 3968, loss 0.713703
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0868849 0.055049
-0.032317 0.00316915
0.0529686 -0.00115509
0.085161 0.0550505
-0.0117817 0.00209762
0.0867333 0.0570546
4.07609e-05 0.000623894
0.0861488 0.0570682
-0.0188028 0.00266692
0.0405236 0.0565374
-0.0204122 0.00316431
8.39808e-07 0.0208545
0.0937753 0.060232
0.00931426 0.0559258
0.0861486 0.0559642
0.0706156 0.0528916
6.39156e-06 0.0067019
0.12614 0.0514071
0.0529709 0.00166652
0.100097 0.0589915
0.0189852 0.00194208
-0.0144856 0.0087931
0.0131849 0.00108887
0.0865811 0.057415
0.0904958 0.0546312
0.0396776 0.0547045
0.0322983 0.055729
0.0323163 0.00133749
0.0497674 0.0533384
0.0907411 0.054122
0.0707066 0.0110018
0.0594397 0.0550122
-0.0394792 -0.00165221
0.0897923 0.0534204
0.129916 0.0517699
-0.0716782 -0.000404331
0.129916 0.0513535
0.0871095 0.0557641
0.0271187 0.00025804
0.0131861 0.00885297
-0.0188028 0.00929102
0.097887 0.0605034
-0.0362226 0.00321668
0.0887289 0.0565862
0.0384776 0.0566764
-0.0250779 0.0556348
0.0594301 0.0562175
0.0496683 0.0539698
0.0322982 0.0561604
0.0131855 0.00196606
-0.0662843 0.00269902
0.059435 0.0641362
0.0527577 0.0555467
0.0464004 0.0529623
0.0927398 0.0571713
0.0781223 0.055835
0.0191276 0.0535694
0.0661225 0.0544224
0.0511643 0.049573
0.0117847 -0.00324477
0.0853255 0.0541243
0.0300726 0.000397903
-0.0247793 0.0187072
0.0467944 0.00780634
0.0317699 0.0547245
0.0134334 0.0640732
0.0562851 0.0556269
0.0168394 0.0529378
0.022444 0.0532749
0.0594204 0.0527197
3.36628e-05 0.0085978
0.0223968 0.0539766
-0.00378521 0.000683793
0.0188019 0.0589806
0.101441 0.0523658
0.0122352 0.0557796
0.0268195 0.0543679
0.0321468 0.0563073
-0.0417491 -0.00035791
0.0277564 0.0541504
-0.0152556 0.00247857
0.0223968 0.0522122
0.0791268 0.0560128
0.059435 0.0549213
0.0937753 0.0591851
4.07609e-05 0.0113056
0.0234754 0.00317526
0.0927399 0.0564336
-0.0776845 0.0137302
0.0318693 0.0545408
0.0730674 0.0557304
0.0859865 0.0605511
0.00894003 -0.00203648
0.0458117 0.0561541
0.0594482 0.0641831
0.0268143 0.0559853
0.0807993 0.00216147
0.0977801 0.0602368
0.0199309 0.0602886
0.0327313 0.0551394
-0.0462148 0.00392663
0.0468064 0.00385837
0.0210981 0.0152687
0.0144707 0.0110928
-0.0611272 0.00118909
0.032902 0.0594076
0.0997462 0.0526606
-0.000795203 0.00301741
-0.129457 0.00490608
0.0621308 -0.000479608
0.0223972 0.051577
0.0968255 0.0552445
0.0968256 0.0541283
0.0867199 0.0565726
0.0818743 0.0503268
0.0807987 0.0149779
-0.00249422 -0.00252566
0.076417 0.0552278
1.13647e-06 0.0133487
0.0412702 0.00764882
0.0199307 0.0606029
0.0865811 0.0565512
0.0268142 0.0541504
0.0234754 0.0163675
6.7959e-07 -0.00050279
-0.080798 0.0107867
0.0144859 0.00482513
0.022444 0.0546561
parameters: [ 9.449  2.195  2.382  1.418  4.   ]. error: 971738464.511.
----------------------------
epoch 0, loss 0.675972
epoch 128, loss 1.02263
epoch 256, loss 0.691003
epoch 384, loss 1.34929
epoch 512, loss 1.13913
epoch 640, loss 0.65755
epoch 768, loss 1.24007
epoch 896, loss 0.677056
epoch 1024, loss 0.536441
epoch 1152, loss 0.95829
epoch 1280, loss 0.530477
epoch 1408, loss 0.786228
epoch 1536, loss 0.85866
epoch 1664, loss 0.85116
epoch 1792, loss 1.31718
epoch 1920, loss 0.532602
epoch 2048, loss 0.548964
epoch 2176, loss 1.41635
epoch 2304, loss 0.559726
epoch 2432, loss 0.774024
epoch 2560, loss 0.454126
epoch 2688, loss 0.648905
epoch 2816, loss 0.870302
epoch 2944, loss 0.419409
epoch 3072, loss 0.89631
epoch 3200, loss 0.699798
epoch 3328, loss 0.549942
epoch 3456, loss 0.615516
epoch 3584, loss 1.29252
epoch 3712, loss 0.763073
epoch 3840, loss 0.463799
epoch 3968, loss 0.362159
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0281391 0.0592313
0.105503 0.0693105
0.0375434 0.0600208
0.0594375 0.0608879
-0.00378297 0.00639486
0.0593133 0.0552838
0.0781227 0.0592985
0.0813321 0.0602615
0.0204045 0.00754995
1.01848e-06 0.00568271
0.072741 0.0596711
0.0594399 0.0573499
0.0210209 0.06466
3.62028e-05 0.018086
0.0375381 0.0605883
0.0659615 0.00653769
0.0394799 0.00371334
-0.0189887 0.00641334
0.126127 0.0582614
0.0133878 0.068041
0.0174335 0.0577924
-0.0210896 0.00828307
0.0327312 0.0619472
0.0117845 0.00195028
-0.0621346 0.00584112
-0.00628221 0.0591532
0.0991788 0.0578602
0.0199307 0.0642323
0.0716719 0.0121829
-0.0023637 0.0607844
0.0897923 0.0602187
-0.0208232 -0.000299688
0.0529729 0.0255342
0.056285 0.0612207
-0.0194897 0.00224291
-0.0234677 0.0172295
-0.0234699 0.0186597
-0.00737805 0.059918
-0.0413988 0.00164014
0.0594399 0.0601043
0.014484 0.0192732
0.028139 0.0593423
-1.32922e-07 0.00585696
0.0385291 0.00958957
0.121243 0.0627626
0.126171 0.0618155
0.0605534 0.0598189
-0.0860447 0.00675471
0.0594397 0.0584841
0.0594027 0.058125
0.0927399 0.0619168
0.0210209 0.0635036
0.032195 0.0610654
0.0323166 -0.00178388
0.125146 0.0583027
0.0424369 0.0603845
0.0589765 0.0628119
-0.00378521 0.0014072
0.121243 0.0613586
0.0594398 0.0619836
0.0207304 0.00737763
0.0281392 0.0615718
-0.0492483 0.00132308
-0.0394796 0.00706645
0.00931421 0.0603312
0.00460075 0.0107382
0.0330949 0.0597365
-0.0118768 0.0606701
-0.000807624 0.00422695
0.0317697 0.0605221
0.126206 0.0611635
-0.00700272 0.0592777
0.0532657 0.00935594
0.041765 0.00552828
0.0927397 0.0626376
0.0310444 0.0553499
-0.0109842 0.00510435
0.0859466 0.0628495
0.0318693 0.0589704
0.0326469 0.00163338
0.0871097 0.0624975
0.0390895 -0.00087561
0.0626054 0.0617668
0.126169 0.058678
0.018802 0.00907263
0.0691072 0.0606411
0.0878299 0.0558387
0.0449618 0.00285087
0.0131845 0.00644962
0.0815268 0.059912
0.0594398 0.0602168
0.0866799 0.0604395
-0.0118764 0.0556953
0.0174335 0.0560416
-0.0247813 0.00974084
-0.00142797 0.00805721
-0.00378297 0.0121951
-0.0416401 0.00877234
0.0412776 0.00752542
0.0594424 0.0602156
0.0109781 0.0143576
0.0691984 0.056549
0.081797 0.0593171
-0.0204101 0.00470196
-0.0860447 0.00386734
0.0820225 0.0576074
0.00932522 0.0591762
0.09274 0.0602117
-0.0462076 0.0064701
0.0710837 0.0597258
-0.00250418 0.00265132
0.0247038 0.0599386
0.00250079 0.00996219
0.100097 0.064691
0.0952316 0.0604757
-0.00236343 0.062301
-0.036223 0.00780332
-0.0188062 0.00143057
0.0261392 0.0596177
0.0815268 0.0567962
-0.0707062 0.00508545
-0.0394792 0.0142696
-0.0248182 0.0118658
0.0368964 0.0587199
0.0211182 0.0627982
-0.0417422 0.00584898
0.109566 0.061932
-2.95455e-07 0.0103959
parameters: [ 9.449  2.195  2.382  0.2    4.   ]. error: 23255076.0454.
----------------------------
epoch 0, loss 0.847974
epoch 128, loss 1.00059
epoch 256, loss 0.905975
epoch 384, loss 0.453145
epoch 512, loss 0.998174
epoch 640, loss 0.96346
epoch 768, loss 0.935394
epoch 896, loss 0.768988
epoch 1024, loss 1.14723
epoch 1152, loss 1.1894
epoch 1280, loss 1.73347
epoch 1408, loss 0.757828
epoch 1536, loss 0.347836
epoch 1664, loss 0.572897
epoch 1792, loss 0.927489
epoch 1920, loss 1.31759
epoch 2048, loss 1.10021
epoch 2176, loss 0.318833
epoch 2304, loss 0.746346
epoch 2432, loss 0.424968
epoch 2560, loss 0.595179
epoch 2688, loss 0.409521
epoch 2816, loss 0.859715
epoch 2944, loss 0.701223
epoch 3072, loss 0.65461
epoch 3200, loss 0.862891
epoch 3328, loss 0.540821
epoch 3456, loss 1.06785
epoch 3584, loss 1.43182
epoch 3712, loss 0.286272
epoch 3840, loss 1.0136
epoch 3968, loss 0.861619
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0813269 0.0611828
0.135027 0.0609082
0.0194912 0.041743
0.0122142 0.0613379
0.0201879 0.0608932
0.00131924 0.0446558
-0.00250418 0.0344083
0.0109781 0.0469977
0.0594398 0.0603486
0.0594396 0.0616303
0.0594395 0.0602364
0.0182076 0.0615262
0.0594562 0.0618076
0.0860428 0.0425167
0.0596272 0.0611461
0.0594375 0.0615838
0.0950804 0.06072
0.0964307 0.0607857
-0.00142797 0.0525234
0.000786701 0.0242
0.0113515 0.0274662
-3.40966e-05 0.0296971
0.0461382 0.0612793
0.0330542 0.0610423
-0.0394792 0.0470596
0.0997468 0.0604443
0.0140549 0.0606385
0.00596546 0.0622437
0.0691986 0.0612871
0.0414001 0.0402182
0.032195 0.0614024
0.0911173 0.0609453
0.080382 0.0616717
0.0189892 0.0484324
0.000782366 0.0509408
-0.014471 0.0350156
0.0964305 0.0611439
0.0870053 0.0613757
0.0594204 0.0620657
0.0335492 0.0605966
-0.00236348 0.0615862
0.0599031 0.0616334
-0.0394792 0.0277015
2.41162e-06 0.0346391
0.0509553 0.0613214
0.0321586 0.0615314
0.05627 0.0619327
0.0122452 0.061705
-0.0621272 0.0155161
0.0866799 0.0616249
0.0562648 0.0619267
-0.00891927 0.0463781
0.0346348 0.0349401
0.0492474 0.0240625
0.0449618 0.0518636
0.101814 0.0622869
-0.00894944 0.0433112
-0.0776729 0.0428777
0.032933 0.0609306
0.0661223 0.0610928
0.0867119 0.0616195
0.0133907 0.0628132
0.0986862 0.0610077
-0.027288 0.0255225
0.0594452 0.0625573
0.0397474 0.0616545
0.0582236 0.0614543
0.0495796 0.0409911
0.0220538 0.0609052
0.106619 0.0617471
0.0730675 0.061551
0.059435 0.0608643
0.126171 0.0615515
0.0667826 0.0610762
0.00932421 0.0609251
-0.0177194 0.0203766
0.0407513 0.0615467
0.0248197 0.0241138
0.0724734 0.0606795
-0.00142797 0.0460752
-0.0631101 0.0412993
0.0495801 0.0147892
0.0337182 0.0617691
-0.0204122 0.022773
0.0494968 0.0328842
0.129925 0.0616685
0.0412725 0.0357914
-0.0707062 0.0393986
0.0487559 0.0625448
0.0897923 0.0612333
0.0691984 0.0611514
0.081527 0.0610278
0.0989675 0.0626276
0.022444 0.0606906
0.0177241 0.0315094
0.0322982 0.0614419
-0.049248 0.0240101
0.0594374 0.0619998
0.05937 0.0616206
0.022215 0.060918
0.0117845 0.0296403
0.0710837 0.0604877
-0.0188028 0.0130617
0.0283785 0.0613267
-0.00894944 0.0532305
0.0764272 0.0614557
0.0866799 0.0614844
-0.0621274 0.0295393
0.000835126 0.0260917
0.105471 0.0628174
0.0904958 0.0617484
0.0871098 0.0605349
0.0861486 0.0611712
-0.0109489 0.0303273
0.0604514 0.0613341
0.0329332 0.0608273
-0.0529722 0.0515354
0.0927397 0.0614579
-0.0211014 0.0179162
0.0329332 0.0604859
0.0527577 0.060871
0.081527 0.0607498
0.081797 0.0612247
0.0281392 0.0607533
-6.30661e-07 0.0522588
0.080382 0.0613015
0.0144707 0.0465326
0.0920969 0.0613306
parameters: [ 9.449  2.195  2.382  0.067  4.   ]. error: 247255002.447.
----------------------------
epoch 0, loss 0.903579
epoch 128, loss 0.99539
epoch 256, loss 0.753979
epoch 384, loss 1.12426
epoch 512, loss 1.00723
epoch 640, loss 1.06143
epoch 768, loss 0.79763
epoch 896, loss 0.742448
epoch 1024, loss 0.790194
epoch 1152, loss 0.678217
epoch 1280, loss 0.600899
epoch 1408, loss 0.649725
epoch 1536, loss 0.632756
epoch 1664, loss 0.751411
epoch 1792, loss 0.6897
epoch 1920, loss 0.942084
epoch 2048, loss 0.768615
epoch 2176, loss 0.83834
epoch 2304, loss 0.701978
epoch 2432, loss 0.834918
epoch 2560, loss 0.541988
epoch 2688, loss 0.748243
epoch 2816, loss 0.627726
epoch 2944, loss 0.524811
epoch 3072, loss 0.702107
epoch 3200, loss 0.64545
epoch 3328, loss 0.679715
epoch 3456, loss 0.525131
epoch 3584, loss 0.667273
epoch 3712, loss 0.809013
epoch 3840, loss 0.711537
epoch 3968, loss 0.596116
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0317699 0.0523475
0.018802 0.00245139
0.056285 0.0529816
0.0211185 0.0595775
0.0594396 0.0508298
-0.0412757 -0.000465936
-0.0267379 0.000618146
0.0464004 0.0476833
0.0497673 0.0496112
0.0650734 0.0502072
0.0897923 0.054298
0.0691985 0.0516195
-0.0417792 0.00831281
0.01134 0.000246436
-0.0449623 -0.00224174
0.0707072 -0.00125958
0.0321468 0.0517078
0.132184 0.06622
0.0290868 0.0500851
0.0710838 0.0542788
0.0497671 0.0499824
-0.00131975 0.00115219
0.0964772 0.0496762
0.0321467 0.0488911
0.0384776 0.0525611
0.097887 0.0597665
0.0237989 0.0532596
-0.0117783 7.17462e-05
0.125873 0.0520264
0.026809 0.0504401
0.0188018 0.0578855
0.0220537 0.0488302
0.0329331 0.0545223
0.0188018 0.0577194
0.104819 0.0482975
0.000213351 0.0574858
0.0237986 0.0556706
-0.0621343 -0.00172002
0.104819 0.0510659
-1.92327e-05 -0.000729892
0.0412702 0.00786794
-0.0131855 0.000406799
0.0594759 0.0476534
0.0152545 -0.000696237
-0.0118765 0.0524324
-0.0188593 0.00277234
0.0375381 0.0513723
-0.000835063 -0.00104695
0.0387924 0.0469472
0.0727409 0.0516085
0.106634 0.0520277
-0.0204028 -8.65749e-05
0.0281391 0.0552281
0.0964772 0.0479574
0.0691984 0.0536611
0.124311 0.04788
0.0210981 0.010327
0.0868851 0.0546027
-0.0857218 -0.00433662
0.000793148 -0.00201198
0.0261391 0.053981
0.088282 0.0490214
0.133575 0.0480793
0.0396776 0.0505637
0.0384676 0.0542438
0.0122142 0.0518522
0.123121 0.0501769
0.0462066 -0.00194871
0.0327314 0.052809
0.0131848 -0.000994797
0.0927398 0.0557652
0.0562648 0.0502972
0.0593665 0.0486092
0.0188019 0.0580496
-0.00553123 0.0473599
-3.11149e-07 0.000436277
0.0716781 -0.00151288
0.0907408 0.0568131
-0.0394792 0.00411524
0.038792 0.0485364
-0.0492483 -0.00292424
0.0860364 0.000995633
0.0272916 0.00572958
0.05627 0.0523059
-0.0394796 0.0083398
-0.0109489 0.00478982
0.0911183 0.0521846
-0.0529722 0.00308004
-0.0113495 -0.00059457
-0.0611272 -0.000693685
0.014471 0.00126918
-0.10012 -0.00311191
-0.0188605 0.00526407
0.0851609 0.0533113
0.0910569 0.0510187
0.0867333 0.0552764
0.0964772 0.0480751
0.0541426 0.048184
0.0396776 0.0473853
-0.0234626 -0.00160849
0.0631173 -0.00197853
0.0945151 0.0478682
0.0624794 0.0477969
-0.000801651 0.00824061
0.0290868 0.0503336
0.0739227 0.0580324
0.0508582 0.0454335
0.0710838 0.0562561
0.0317699 0.0542755
0.0417454 0.00168945
-0.0272904 -0.00190784
0.130005 0.0509266
0.109549 0.0510221
-0.0161248 0.0504407
0.0495796 0.00186829
0.0267781 0.0453885
0.05627 0.0516632
-0.0416468 0.00815423
-0.0267425 0.000177906
0.0207304 -0.00128538
0.0211183 0.0579834
-0.0247813 -0.000765539
0.032933 0.0541322
0.0322982 0.0533139
0.0281391 0.0521795
0.0851609 0.0515228
0.000835126 -0.000836126
0.0968255 0.0543861
parameters: [ 9.449  2.195  2.382  0.582  4.   ]. error: 20442.9005962.
----------------------------
epoch 0, loss 1.1959
epoch 128, loss 1.24745
epoch 256, loss 1.22034
epoch 384, loss 0.785929
epoch 512, loss 0.582315
epoch 640, loss 0.704661
epoch 768, loss 0.789825
epoch 896, loss 0.695425
epoch 1024, loss 0.592833
epoch 1152, loss 0.766363
epoch 1280, loss 0.94644
epoch 1408, loss 0.829341
epoch 1536, loss 0.794349
epoch 1664, loss 0.576254
epoch 1792, loss 0.746822
epoch 1920, loss 0.574637
epoch 2048, loss 0.582188
epoch 2176, loss 0.747321
epoch 2304, loss 0.579733
epoch 2432, loss 0.806261
epoch 2560, loss 0.532648
epoch 2688, loss 0.763596
epoch 2816, loss 0.728673
epoch 2944, loss 0.717408
epoch 3072, loss 0.733354
epoch 3200, loss 0.786165
epoch 3328, loss 0.653018
epoch 3456, loss 0.551348
epoch 3584, loss 0.783776
epoch 3712, loss 0.799441
epoch 3840, loss 0.814164
epoch 3968, loss 0.626639
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.125873 0.0597048
-0.0106908 -0.00043892
0.0329021 0.066645
0.0385437 0.00570873
0.0301505 0.0564618
-0.02083 0.00498512
0.0594398 0.0583789
0.0594325 0.0580768
0.0346348 -0.00334447
0.0594399 0.0583772
-0.0631232 -0.00396306
0.0494968 0.00383283
0.0859962 0.067089
0.0594325 0.0599134
0.0593312 0.0599302
0.123121 0.0558707
0.0927397 0.0615491
-0.0113401 -0.00168359
-0.00236357 0.063539
0.0532657 0.00764015
0.0327315 0.0549785
-0.00236373 0.0609143
-0.000835392 -0.00161054
0.0807977 -0.0010308
0.100097 0.0673197
-0.0132904 0.0724562
0.0594379 0.0716635
-0.00236373 0.0555392
0.061289 0.0722756
0.059367 0.0577943
0.0707072 0.00667857
-0.129449 0.00260295
-0.00236373 0.053174
-0.00236347 0.0561674
0.0317699 0.060305
0.0631207 -0.00447262
0.0482638 0.0575377
-0.032317 -0.00500747
1.01848e-06 -0.00451647
0.109566 0.0580707
0.125873 0.0662958
0.0813321 0.0628487
0.0394799 0.00557193
0.0860587 0.00488687
0.032158 0.0595566
-6.15975e-07 -0.000450022
-0.0494953 -0.00081253
0.0267781 0.0610899
0.0281392 0.0614915
0.0310444 0.0593064
0.00863032 0.0527723
0.0859465 0.0565974
-0.032317 -0.00457163
-0.0204028 -0.00440079
3.36628e-05 -0.0010926
-0.129449 0.0070991
0.0424316 0.0605872
-0.00131975 0.0014458
0.0497673 0.0558354
0.0234681 -0.00221541
0.0267369 0.00399109
-2.99285e-06 0.00353331
0.0346353 -0.00401225
-0.0716686 -0.00424408
0.0283785 0.0563201
0.0317699 0.0555887
0.0237989 0.0572412
0.0387924 0.0559495
0.10664 0.0605236
0.0631196 0.00190362
0.130756 0.0562415
-0.00549668 0.0496676
-0.0340213 -0.00245736
0.046209 0.00706616
-0.0144706 0.00975518
-0.0631232 -0.000467437
-0.0776845 0.00236679
0.0707072 -0.00107299
-0.0234744 -0.00235651
0.032933 0.0605203
0.0882823 0.0574469
0.0857795 0.0607996
0.0991789 0.0577661
-0.0417573 -0.00331769
0.0496682 0.0598668
-0.0390959 -0.00581573
0.0853245 0.0573616
0.0424215 0.0597959
-0.0267425 0.00299483
0.0650733 0.0565294
0.0330949 0.0566594
0.0727409 0.059208
-0.0271152 -0.00438372
0.0813268 0.0627306
-0.0248165 -0.00170326
0.0701141 0.062559
0.0201879 0.0552954
-0.0109842 0.00472094
0.0286789 0.06106
0.0532578 0.00443594
0.0417454 -0.00623434
0.0477959 0.055395
0.0271163 0.00805214
-0.0271121 0.0027937
-0.0529659 0.00188904
0.0416368 -0.000909164
-2.64683e-07 -0.000491413
-6.0681e-07 -0.00469389
0.0631207 -0.00714758
0.0871097 0.0563558
0.0724741 0.0566421
0.01684 0.0556637
0.0739227 0.0675816
-0.0118765 0.060666
-0.0414033 -0.0006483
-0.014471 -0.00269646
0.0424268 0.0623516
-0.0117783 0.00194037
0.0330953 0.056789
0.0594427 0.0580488
0.0971047 0.0671568
0.101441 0.0571075
0.0305923 0.0589971
0.130756 0.0569187
0.088282 0.0561823
0.000793148 -0.00354212
-0.0271193 -0.000476065
0.023648 0.0537619
parameters: [ 9.449  2.195  2.382  0.722  4.   ]. error: 52628042.7019.
----------------------------
epoch 0, loss 1.24363
epoch 128, loss 1.28389
epoch 256, loss 1.39056
epoch 384, loss 0.956984
epoch 512, loss 0.962801
epoch 640, loss 1.082
epoch 768, loss 0.853033
epoch 896, loss 0.853947
epoch 1024, loss 0.963793
epoch 1152, loss 1.24141
epoch 1280, loss 0.867023
epoch 1408, loss 1.29447
epoch 1536, loss 0.773493
epoch 1664, loss 1.03474
epoch 1792, loss 0.757898
epoch 1920, loss 0.774478
epoch 2048, loss 0.898157
epoch 2176, loss 0.858849
epoch 2304, loss 0.76783
epoch 2432, loss 0.490009
epoch 2560, loss 0.900818
epoch 2688, loss 0.643385
epoch 2816, loss 0.949613
epoch 2944, loss 0.801359
epoch 3072, loss 0.716679
epoch 3200, loss 0.344069
epoch 3328, loss 0.823085
epoch 3456, loss 0.890348
epoch 3584, loss 0.642087
epoch 3712, loss 0.582138
epoch 3840, loss 0.786636
epoch 3968, loss 0.523476
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0177194 0.00455676
0.0691072 0.0565762
0.0201879 0.0557016
0.0346447 0.00973488
-0.0417422 -0.00161787
0.0716773 -0.00286021
0.0882822 0.0548197
-0.0414033 0.0198823
0.129916 0.0573452
0.0290868 0.0599655
-0.0131848 0.00960832
0.029557 0.0040856
0.0594562 0.0573459
0.0468064 0.0152248
0.0281393 0.0576574
-0.0194973 0.0209572
-0.0132848 0.0646737
0.0387924 0.0542115
3.36628e-05 0.017515
0.105503 0.0652268
0.0468012 0.00710416
0.0594466 0.0654182
0.059367 0.0586394
0.0594273 0.0573625
0.0911183 0.0542805
0.0920961 0.0565456
0.0594397 0.060867
0.0673482 0.0554009
-0.0412685 0.017369
0.0317697 0.0598314
0.0346348 -0.00337425
-0.0318771 7.25769e-05
0.0243594 0.0548786
0.0333261 0.0598508
0.00460075 0.00286244
0.105471 0.0660012
0.0868849 0.0617541
0.109566 0.0606835
-0.0109489 0.00488042
0.0462066 0.00848199
0.0871097 0.0617572
0.0305919 0.0536044
-0.0462076 0.00725355
-0.0860447 0.00405288
0.0117847 0.00214013
0.00143008 0.00416743
0.130005 0.0584949
0.0237986 0.059195
0.0122142 0.057397
-0.0340329 0.0108423
-0.0113495 0.0117628
0.0188086 0.00136813
0.0662801 -0.00108442
0.0710838 0.0620961
-0.0111692 0.056439
0.0295687 0.00724451
-0.0234626 0.00943382
-0.00378521 0.00360436
-0.0144713 0.0175601
-0.0390966 -0.00085815
0.0562851 0.0609193
-0.0204028 0.00823107
0.0496682 0.0603841
0.0131855 0.00517596
-0.0394796 0.0142543
0.0706156 0.0585907
0.0191272 0.053711
0.0385291 0.0179166
0.0532677 0.0186188
-1.1614e-09 0.0054209
0.00378707 0.0221552
-0.0210969 0.000806385
-0.0204001 0.021209
-1.92327e-05 0.00628611
0.0375434 0.0544477
0.0897923 0.0582978
-0.0659495 0.00990665
-1.17339e-07 0.0109829
0.0182076 0.0522101
6.39156e-06 0.0114994
0.0594426 0.0566321
0.0937753 0.0582304
0.0207331 0.00784394
0.0631163 0.00288589
0.0966595 0.0543769
0.0317697 0.0599081
0.0861487 0.0593385
0.0621339 0.00344988
0.027745 0.0553234
0.0215366 0.0142774
0.0582589 0.0566902
-0.0807986 0.0110649
0.0764272 0.0528901
0.0861488 0.0603712
0.0220537 0.0600613
0.00250079 0.00994697
-0.02083 0.0218743
0.09274 0.0592242
-0.0860382 0.0136336
0.061289 0.0638805
0.0538297 0.054555
0.059407 0.0576706
0.0867199 0.0621492
0.0346353 0.00806969
-0.0394796 0.0166612
0.0482639 0.0583574
0.0037846 0.0223462
0.0897923 0.0602053
0.0730674 0.0598171
-0.0248165 0.00454155
-1.99995e-06 0.000116364
0.0037846 0.0156933
-0.053266 0.0145977
0.0188596 0.00673748
-0.0860366 0.0106022
0.0532657 0.0124008
0.0626002 0.0556901
0.102035 0.0564607
0.062595 0.0601539
-0.027288 0.00902648
0.0373484 0.0557287
-0.0417488 -0.00249024
0.10664 0.0569555
-0.0272904 0.0131665
0.0424428 0.0558456
-0.0417573 0.0131664
0.00863032 0.0552261
0.0144846 0.00279523
parameters: [ 9.449  2.195  2.382  0.427  4.   ]. error: 1373143.7418.
----------------------------
epoch 0, loss 0.869569
epoch 128, loss 0.825064
epoch 256, loss 0.66052
epoch 384, loss 0.700373
epoch 512, loss 0.976899
epoch 640, loss 0.489337
epoch 768, loss 0.74744
epoch 896, loss 0.952962
epoch 1024, loss 0.593818
epoch 1152, loss 0.610216
epoch 1280, loss 0.894112
epoch 1408, loss 0.688142
epoch 1536, loss 0.807293
epoch 1664, loss 0.95081
epoch 1792, loss 0.674198
epoch 1920, loss 0.736822
epoch 2048, loss 0.659045
epoch 2176, loss 0.52218
epoch 2304, loss 0.809109
epoch 2432, loss 0.79461
epoch 2560, loss 0.790884
epoch 2688, loss 0.723011
epoch 2816, loss 0.741849
epoch 2944, loss 0.585484
epoch 3072, loss 0.746054
epoch 3200, loss 0.849967
epoch 3328, loss 0.802799
epoch 3456, loss 0.803918
epoch 3584, loss 0.581468
epoch 3712, loss 0.828891
epoch 3840, loss 0.593703
epoch 3968, loss 0.694256
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0261395 0.0487676
0.076417 0.0517875
0.0170933 0.057249
0.081797 0.0502096
0.0945153 0.0511827
0.0467944 -0.000288903
0.0730675 0.053939
0.0286795 0.050982
-0.0211014 0.00255774
0.0945151 0.0510185
0.000834798 0.000633669
0.0152616 -0.0031151
0.0368964 0.0502079
0.0867333 0.0563639
0.062595 0.0550427
0.0300726 -0.00638777
0.0283785 0.0513562
0.0667826 0.0543988
-0.0111732 0.0505244
0.05627 0.0525846
-0.0662867 -0.00622114
0.0286795 0.0517689
0.0197009 0.0538118
0.028139 0.0500606
0.0261391 0.0561734
0.0290868 0.0565032
0.0901948 0.0497365
-0.0188606 -0.00718162
-0.0340329 -0.00457004
-0.049248 -0.0094476
0.0626002 0.0522763
0.0626054 0.0521832
-0.0109566 0.00181444
0.126169 0.0518409
0.0424268 0.0509953
0.0594399 0.0564547
0.090741 0.0547521
0.105503 0.0631184
0.0964305 0.0503942
0.0941706 0.052436
0.018802 -0.00611452
-0.0707062 -0.00310646
0.0281393 0.0570323
0.0966595 0.0490824
0.100097 0.0573364
0.0133878 0.0627239
0.0323163 -0.00611192
0.0691068 0.0520912
-0.0111018 0.0490102
0.0813269 0.0520971
0.080392 0.0538718
0.0188015 0.056321
0.0267781 0.0493203
0.090741 0.0568464
0.102035 0.0527867
0.0477957 0.0543125
-0.0611272 -0.00114208
-0.0109489 0.00190988
0.0813321 0.05181
0.0870053 0.0520476
-0.049248 -0.00119722
0.0327315 0.0532049
0.102034 0.0522688
0.101441 0.0498317
0.0329331 0.0538445
0.0861486 0.0533532
-0.0807974 0.00273048
-0.063123 -0.00686396
0.0188603 -0.00176414
-0.000786289 -0.00738217
2.5668e-06 -0.00640228
0.0191276 0.0500365
0.0207304 0.00223233
0.0122504 0.0523668
0.101441 0.0491276
0.0861486 0.0562642
0.024704 0.0511692
0.0594397 0.0552782
0.0730675 0.0546437
0.0605996 0.0519818
0.0871096 0.0533448
-0.0152606 -0.000246043
4.57767e-08 0.00300104
0.0911173 0.0505957
0.0730675 0.0524013
0.0384777 0.0503829
0.0477956 0.0542275
0.0594397 0.0526792
0.125863 0.0518399
0.0594399 0.0488051
-0.0161248 0.0495567
0.109549 0.0503881
0.0387924 0.0486806
0.0236482 0.054394
0.0182076 0.0509902
-2.95455e-07 0.00742985
0.0321467 0.0536191
0.0131861 0.00190682
0.00894166 0.00633979
-0.129445 -0.00262529
0.0594481 0.062888
8.39808e-07 0.0106314
0.0859466 0.0568233
3.62028e-05 -0.00241301
0.0144853 0.00305858
0.0384777 0.0511929
0.0337182 0.0563645
0.0706155 0.0531859
0.0416368 0.00160778
0.0117847 -0.0043374
0.0281393 0.056844
0.102035 0.0502291
3.82209e-05 -0.00611989
0.0217931 0.0563912
0.121243 0.0543421
0.0394802 -0.00288815
-0.0412731 -0.00408704
0.0251233 0.0566812
0.0594324 0.0524704
0.0594426 0.0528957
0.0911173 0.05063
0.109566 0.0571112
0.134995 0.0527309
0.10955 0.0538305
-0.0295823 0.00834259
0.0599029 0.0547829
-0.0707055 0.00762669
0.0538009 0.0513332
parameters: [ 9.449  2.195  2.382  0.524  4.   ]. error: 11004421777.4.
----------------------------
epoch 0, loss 1.10658
epoch 128, loss 0.817443
epoch 256, loss 1.15854
epoch 384, loss 0.90091
epoch 512, loss 0.676244
epoch 640, loss 0.699864
epoch 768, loss 0.88002
epoch 896, loss 0.658085
epoch 1024, loss 1.28632
epoch 1152, loss 0.597255
epoch 1280, loss 0.765075
epoch 1408, loss 0.646538
epoch 1536, loss 0.612562
epoch 1664, loss 0.640525
epoch 1792, loss 0.656306
epoch 1920, loss 0.8259
epoch 2048, loss 0.481125
epoch 2176, loss 0.710067
epoch 2304, loss 0.434105
epoch 2432, loss 0.793529
epoch 2560, loss 0.733834
epoch 2688, loss 0.673905
epoch 2816, loss 0.813537
epoch 2944, loss 0.607724
epoch 3072, loss 0.547427
epoch 3200, loss 0.628345
epoch 3328, loss 0.78607
epoch 3456, loss 0.713462
epoch 3584, loss 0.590922
epoch 3712, loss 0.697088
epoch 3840, loss 0.798075
epoch 3968, loss 0.697778
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0390966 -0.00339318
-7.68032e-07 -0.00307821
0.0621302 -0.00830072
0.0860523 0.00714196
-0.039089 -0.00325508
0.0333262 0.0547346
-0.0385424 -0.00403013
0.0321467 0.0559585
0.0321897 0.0496926
0.0977799 0.054107
-0.0056768 0.0497777
0.0496681 0.0504888
0.0174339 0.0452372
0.0594427 0.0449086
0.072741 0.0523147
0.0412776 -0.00261453
0.0122142 0.0537891
0.00894003 0.00616086
0.101441 0.0457929
0.0594399 0.0507112
1.95996e-07 -0.00226412
0.0243586 0.0471047
-0.0390898 -0.00725057
0.0964307 0.0463358
-0.0161572 0.0529527
0.0188015 0.0545477
0.0966587 0.0443317
0.0594397 0.0503102
0.081797 0.0490127
0.0950802 0.0527656
0.0582651 0.0497232
0.0662934 -0.00382773
0.0281391 0.0540218
0.021525 -0.0106237
0.000786701 -0.00125699
0.0327312 0.051754
0.0192074 0.0560047
0.046209 -0.00351116
0.0706155 0.0539405
0.130756 0.0544495
-0.0267352 -0.00676483
0.0870053 0.0502621
0.0594273 0.0483974
0.0496683 0.0525459
0.0370769 0.0515445
0.0237986 0.0514324
0.0868848 0.051542
0.043628 0.0481738
-0.0131858 -0.0061448
0.0642209 0.0492635
0.032933 0.0535494
0.0477956 0.0495552
0.0594301 0.0526106
0.0538009 0.0509295
0.0920613 0.0493906
-0.0385431 -0.00821943
0.0907408 0.0510677
0.081527 0.0506365
-0.0272995 -0.00571524
0.0977801 0.0557643
0.0937752 0.0549574
0.00863032 0.0419873
0.0971043 0.0603836
0.0416418 -0.00589584
0.0385437 -0.00570493
0.0594399 0.051938
0.0662803 -0.0108077
0.0861488 0.0535504
-0.0417488 -0.00372939
0.0321586 0.0548665
0.0417489 -0.00844806
0.0424268 0.0481998
0.0897923 0.0501389
0.0318787 0.000131429
0.0594759 0.0510985
3.62028e-05 0.000906643
-1.32922e-07 -0.00134873
0.0346452 -0.00928939
0.0868851 0.0510095
-0.00699273 0.0480066
0.0707062 -0.00252389
0.129916 0.0492809
0.0417484 -0.0104174
0.049499 -0.00414886
0.0208321 -0.00587803
0.0290868 0.0520951
0.0140549 0.0476176
3.5013e-07 -0.000170219
0.0134225 0.0605207
-3.40966e-05 -0.00861664
0.00080867 -0.00681285
0.0594396 0.0569521
0.132184 0.0613805
0.0691985 0.0513589
-0.0776845 -0.00735147
0.0482639 0.0535043
0.088729 0.0528922
0.0594397 0.0509264
0.130756 0.0544495
0.0397474 0.0537163
0.0209836 0.00244301
0.0781223 0.0502163
-0.0807974 -0.00839935
0.0813321 0.048067
-0.0385431 0.000207432
-0.0716787 -0.0116931
0.0234681 -0.00461598
-3.66166e-06 0.000402176
0.081527 0.0475276
0.0813269 0.0482139
-0.0807986 -0.00940933
0.0857789 0.0482226
-0.00891927 -0.00730733
-0.0414059 -0.00510383
0.0220539 0.0503998
0.130005 0.0559114
0.0207331 -0.00669565
0.0920508 0.0537491
0.0631163 -0.0102255
-0.0300728 -0.00814802
0.0321467 0.0555398
0.0867333 0.0554723
0.0594398 0.0501818
0.0322983 0.0510708
1.95996e-07 0.00334611
-0.00700272 0.0487514
0.0396777 0.047097
0.0910569 0.0535833
parameters: [ 9.449  2.195  2.382  0.635  4.   ]. error: 380839576.4.
----------------------------
epoch 0, loss 1.23041
epoch 128, loss 0.695704
epoch 256, loss 1.15202
epoch 384, loss 1.18557
epoch 512, loss 0.779606
epoch 640, loss 0.831071
epoch 768, loss 0.829087
epoch 896, loss 0.815415
epoch 1024, loss 0.830564
epoch 1152, loss 0.760682
epoch 1280, loss 0.909632
epoch 1408, loss 0.681302
epoch 1536, loss 0.928111
epoch 1664, loss 0.756783
epoch 1792, loss 0.920318
epoch 1920, loss 0.753335
epoch 2048, loss 0.725916
epoch 2176, loss 0.675911
epoch 2304, loss 0.828402
epoch 2432, loss 0.475263
epoch 2560, loss 0.586927
epoch 2688, loss 0.668501
epoch 2816, loss 0.735259
epoch 2944, loss 0.466822
epoch 3072, loss 0.695487
epoch 3200, loss 0.61716
epoch 3328, loss 0.988543
epoch 3456, loss 0.838252
epoch 3584, loss 0.919874
epoch 3712, loss 0.648907
epoch 3840, loss 0.782198
epoch 3968, loss 0.857123
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0131848 0.0117527
0.0385434 -0.00530901
0.0594482 0.0641881
0.00142251 -0.00129509
-0.0662877 -0.00428558
0.0140549 0.047678
0.121243 0.0490555
0.0631163 -0.0033512
-0.0417488 -0.00706242
-0.00700262 0.0520433
0.0920969 0.0511118
0.0194912 0.000932739
-0.0390898 -0.00141461
0.130756 0.0575798
0.09274 0.0559835
-3.66166e-06 0.000625838
-0.0271152 -0.0098301
0.0986864 0.0487054
-0.0860431 0.00156516
0.0188023 -0.00715313
0.125873 0.0479411
0.0950804 0.0512719
-0.0109842 0.000958555
0.0691985 0.0524136
0.0650733 0.0478198
0.0642209 0.0502165
0.0859466 0.0499452
0.0468012 0.00363603
0.0978866 0.0552273
0.0807987 -0.00441185
0.0594397 0.0521332
0.125873 0.0543566
-0.0271157 -0.00851526
0.0396776 0.0494269
0.0691985 0.0500131
0.0271185 -0.0052078
0.132184 0.0608783
0.0950802 0.0528828
0.0791961 0.0463229
0.0562597 0.049419
0.0594397 0.05349
-7.99663e-06 -0.00276057
0.0716781 -0.00697612
0.0318693 0.0483917
0.0625949 0.0529235
0.0594324 0.0498371
0.0661222 0.0556845
0.0594562 0.0500901
0.0261394 0.0511729
0.0273016 0.004152
-0.0860447 0.00477854
0.0322983 0.0516323
0.0582389 0.0495502
0.0192074 0.0491852
0.0857788 0.0448092
-0.0177216 -0.00437574
-0.00628221 0.0442635
0.032195 0.0471479
0.0860364 -0.000637665
0.105503 0.0638779
0.0462066 -0.0038938
0.0117847 -0.00504278
-0.0131851 0.00382811
0.086885 0.052474
0.0281391 0.0518534
0.0210981 0.00443976
0.0920969 0.0486125
0.0390889 -0.00954199
-0.0532547 -0.00767047
0.0301504 0.0515844
0.0210212 0.054555
-0.129457 0.00287962
0.13219 0.0623311
-0.0394796 -0.000101927
0.0220539 0.054246
-0.0118768 0.0449436
0.00863092 0.0468642
0.0188016 0.0560052
0.0295843 0.00165872
0.0301504 0.0508962
0.0582389 0.0494377
-0.0346416 -0.00178114
0.0800821 0.0502222
0.0781223 0.0497661
0.0952317 0.0570172
-0.00731719 0.0478202
0.118684 0.058867
0.0594466 0.062869
0.0333262 0.0558361
0.0211031 0.0100553
0.0122243 0.049704
-0.0189887 -0.00209706
0.022444 0.0478467
0.0859962 0.0564939
0.0813321 0.0495685
0.0594397 0.0536767
0.0144707 -0.00439126
-0.0367964 -0.00342644
-0.00968441 0.0502443
0.0968257 0.0533564
0.0753057 0.047526
0.022215 0.0461374
0.0950804 0.0512719
0.0927397 0.0548672
0.110244 0.0510264
-0.0346424 -0.00578693
-0.0295746 -0.00146644
-0.0271153 -0.00183987
0.0964766 0.0520207
0.0857789 0.0463015
0.0201879 0.0473218
0.0496682 0.0523865
0.0477957 0.049446
0.0188016 0.0544481
-0.000801651 -0.00272884
0.025123 0.05709
0.0594397 0.0531028
0.01684 0.0499041
0.0319945 0.0489921
0.0322983 0.050132
0.129445 0.00304761
0.0594398 0.0527628
0.0189892 0.00108319
-0.0827278 -0.00690923
-0.00735889 0.0411801
0.0234681 0.00262776
0.0318693 0.0492764
0.0477958 0.0539919
parameters: [ 9.449  2.195  2.382  0.56   4.   ]. error: 51669.0707439.
----------------------------
epoch 0, loss 0.995051
epoch 128, loss 1.58581
epoch 256, loss 1.22833
epoch 384, loss 0.894876
epoch 512, loss 0.529832
epoch 640, loss 1.10592
epoch 768, loss 0.943232
epoch 896, loss 0.70551
epoch 1024, loss 1.10732
epoch 1152, loss 1.01889
epoch 1280, loss 0.52261
epoch 1408, loss 0.707286
epoch 1536, loss 0.875518
epoch 1664, loss 0.822731
epoch 1792, loss 0.904313
epoch 1920, loss 0.867825
epoch 2048, loss 0.896129
epoch 2176, loss 1.2232
epoch 2304, loss 0.802481
epoch 2432, loss 0.677628
epoch 2560, loss 0.643555
epoch 2688, loss 0.719303
epoch 2816, loss 0.564121
epoch 2944, loss 0.845033
epoch 3072, loss 0.516744
epoch 3200, loss 0.560483
epoch 3328, loss 0.432429
epoch 3456, loss 0.656706
epoch 3584, loss 1.00124
epoch 3712, loss 0.633604
epoch 3840, loss 0.569137
epoch 3968, loss 0.602282
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.062595 0.0566646
0.0461385 0.0596147
0.0631207 0.00310867
0.0532677 0.0130881
0.0734394 0.0092237
0.0220537 0.0588799
-0.00628221 0.0548232
0.092056 0.0553847
0.0261395 0.0566737
-4.6184e-07 0.0152773
0.0594398 0.0532153
0.0626054 0.0567855
0.0211182 0.0597057
0.0210915 0.00834965
0.0562851 0.0566769
0.0236481 0.0578148
0.0330948 0.0556942
0.0659505 0.0106817
0.014484 0.00417534
0.0661222 0.0586102
0.0346348 0.00279288
-0.0211014 0.0144779
0.0385427 0.0127488
0.0508582 0.0538209
-0.00733491 0.0520549
0.0477959 0.0568849
0.0937752 0.0630337
0.0871097 0.0581703
0.0764118 0.0571319
-0.0188606 0.0107097
0.125146 0.0575437
0.0194981 0.0114676
-6.15975e-07 0.0129698
-0.0467928 0.0175934
0.0109781 0.0180012
-0.0106876 0.00193507
0.0870053 0.0539312
0.0662931 0.0129108
-0.0111732 0.05191
0.0904958 0.05593
-0.0189887 0.00384401
0.0170933 0.0619684
0.0319943 0.0591569
0.0373475 0.0549694
0.0867335 0.0587406
0.0234704 0.00421818
0.0321468 0.05555
0.143948 0.0595026
0.0319943 0.055651
-0.0621346 0.00590849
-3.76332e-06 0.0107351
0.0677309 0.0527263
0.0589765 0.0566131
0.0667826 0.0586507
0.0527575 0.0604947
0.0390986 0.00414451
0.0277564 0.0531638
0.046218 0.00971625
0.0966587 0.0542297
0.0865811 0.0567515
0.0373484 0.0554163
0.0268143 0.0576509
0.0197009 0.0575957
0.0261392 0.0592424
0.0907408 0.0580262
-0.0611177 0.0053817
-0.0271121 0.0107215
-0.0340329 0.00863319
0.0286795 0.0539676
0.0509553 0.0502762
-0.0385437 0.00881064
-0.0188062 0.0118831
0.130005 0.0566989
-0.032317 0.00566308
0.0317697 0.0572296
0.0594452 0.0681493
0.0716711 0.00187929
0.129445 0.00933229
-0.0807986 0.0147142
-0.0532547 0.0112128
0.0224432 0.0576138
0.106634 0.0582223
0.0968257 0.0595852
0.0611271 0.00165613
0.0734322 0.017993
0.0199309 0.0629763
-0.00249422 0.0117382
0.0373484 0.0543238
0.0199309 0.061672
0.0621302 0.0021155
0.100668 0.0542942
0.135027 0.0570585
-0.0161248 0.0546051
0.0996717 0.0590821
0.125883 0.0586002
0.0527576 0.0542657
0.0364958 0.0512582
0.0192074 0.0576855
0.0330542 0.0516121
-0.0417573 0.00511192
0.121243 0.0594932
0.081797 0.0554795
0.0117785 0.00386117
0.0319947 0.0548856
0.00596546 0.061343
0.0321468 0.0567422
0.00931417 0.058623
-0.0210947 0.0163068
0.126222 0.0527982
-0.00484737 0.0479485
0.0385427 0.0082135
0.0716711 0.00368236
0.0436381 0.0563981
0.0207377 0.00548287
0.0144707 0.00407261
-0.02083 0.00389822
8.39808e-07 0.00697473
0.0390889 0.00191993
0.0384676 0.0555507
0.0461383 0.0593976
0.0152591 0.00563872
-0.0113473 0.0136131
0.0734394 0.00959836
0.0251231 0.0620436
-0.0271111 0.0093351
0.0859465 0.0583824
0.0860523 0.0123843
0.0626002 0.0560743
parameters: [ 9.449  2.195  2.382  0.588  4.   ]. error: 62859146.4717.
----------------------------
epoch 0, loss 1.35586
epoch 128, loss 0.881704
epoch 256, loss 1.15884
epoch 384, loss 0.748011
epoch 512, loss 0.750148
epoch 640, loss 0.772873
epoch 768, loss 0.806798
epoch 896, loss 0.663644
epoch 1024, loss 0.722313
epoch 1152, loss 0.686633
epoch 1280, loss 0.579081
epoch 1408, loss 0.651417
epoch 1536, loss 0.879805
epoch 1664, loss 0.536389
epoch 1792, loss 0.617563
epoch 1920, loss 0.536986
epoch 2048, loss 0.815674
epoch 2176, loss 0.580595
epoch 2304, loss 0.713845
epoch 2432, loss 0.751072
epoch 2560, loss 0.519331
epoch 2688, loss 0.466021
epoch 2816, loss 0.718388
epoch 2944, loss 0.870865
epoch 3072, loss 0.606508
epoch 3200, loss 0.71351
epoch 3328, loss 0.521537
epoch 3456, loss 0.719681
epoch 3584, loss 0.666498
epoch 3712, loss 0.730448
epoch 3840, loss 0.777813
epoch 3968, loss 0.835644
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.018802 0.00418682
-0.0210969 0.00934715
-0.0468045 0.0100199
0.0764118 0.0554023
-0.02083 0.00438775
0.0321467 0.0558838
0.0594399 0.0542558
0.0870053 0.0546584
-0.0416423 0.00495504
-0.0144836 0.00664045
-0.0807986 0.00906151
0.0860587 0.0113223
-0.129445 0.00449114
-2.64683e-07 0.00947464
0.0593105 0.0540018
0.0168394 0.0553356
-0.00236373 0.0540752
0.0594399 0.0555316
0.0319945 0.0560152
-0.0734426 0.00867499
0.0417484 0.00358969
0.0710836 0.0547509
0.0594399 0.0541612
0.0414075 0.00563483
0.0375434 0.0547313
2.41162e-06 0.00581982
0.059181 0.0541596
0.0927398 0.0540467
0.0734322 0.0111729
0.0594452 0.0620674
0.0764481 0.0556119
0.0251233 0.0588166
0.0937753 0.0584837
-0.0385427 0.00567755
0.0594395 0.0561688
-0.0111018 0.0557428
0.0859467 0.0537006
0.0800819 0.0527087
0.0871097 0.0565647
4.07333e-06 0.0108366
-0.0210896 0.00383129
-0.00131656 0.00080476
0.0562851 0.0563729
0.0278229 0.055619
0.0950805 0.0565938
0.125157 0.053348
0.0582651 0.0557335
-0.0449599 0.000910067
0.062595 0.0558174
0.0604514 0.0527886
-0.00378521 0.00994777
0.0305919 0.0516598
0.0867202 0.0564915
-0.0362226 0.0199272
0.0945153 0.0528591
0.0496683 0.0545731
0.0691072 0.0539543
0.0764118 0.0547039
-0.0131841 0.0209438
0.0781227 0.0560265
0.00142251 0.00444023
-0.0111732 0.0548502
0.0417448 0.00990604
4.07609e-05 0.00414338
0.0594398 0.0572945
-0.0300728 0.00406745
0.0631173 0.00441447
-0.129449 0.0110725
-0.0131855 0.00523795
-0.00142797 0.00988892
0.05627 0.0566783
0.0594759 0.0541662
-0.0177194 0.000657008
0.0144853 0.010197
0.00932522 0.0544855
0.0813321 0.0547842
0.0582589 0.0548773
0.0405236 0.0563861
-0.0161473 0.0533302
0.0691986 0.0555732
0.0194912 0.0090489
-0.00700262 0.0532399
0.0207304 0.00453821
0.129445 0.00569727
0.0205126 0.0547499
0.0508582 0.0540338
0.09274 0.054858
-0.0707055 0.0111997
-0.0188605 0.010611
0.0594399 0.0556725
0.0492474 0.0105304
0.00596546 0.0595693
0.0991789 0.0548285
0.0807993 0.00604265
0.132184 0.0635347
0.0197009 0.0538929
-0.0131855 0.0102307
0.0482639 0.0543181
0.0950802 0.0569796
0.0327313 0.0550847
-1.92327e-05 0.0011258
0.014471 0.00666665
0.0589133 0.0557432
0.0188086 0.00900362
0.0188597 0.00657004
-0.0385437 0.0104666
-0.0468001 0.00809174
4.07333e-06 0.00439075
-0.013282 0.0631002
-0.0215238 0.00141487
-0.0417422 0.00098401
0.0208245 0.00419161
0.0189828 0.00981392
-0.0271157 0.00407713
0.129453 0.0135424
0.0594397 0.0546077
0.0367961 0.0120642
0.0317696 0.0541642
-0.0417573 0.0101935
0.059435 0.0546595
0.0594399 0.0551304
0.0707072 0.0140005
0.0594396 0.0554442
0.0204028 0.0105049
0.0477956 0.0537402
-0.0144836 0.0140145
0.0631207 0.00843219
0.0594204 0.0549468
parameters: [ 9.449  2.195  2.382  0.576  4.   ]. error: 3650574.93927.
----------------------------
epoch 0, loss 1.11017
epoch 128, loss 0.682863
epoch 256, loss 0.901665
epoch 384, loss 0.997677
epoch 512, loss 0.784465
epoch 640, loss 0.674292
epoch 768, loss 0.685281
epoch 896, loss 0.55236
epoch 1024, loss 0.623004
epoch 1152, loss 0.818845
epoch 1280, loss 0.731799
epoch 1408, loss 0.551423
epoch 1536, loss 0.761668
epoch 1664, loss 0.844335
epoch 1792, loss 0.64669
epoch 1920, loss 0.723626
epoch 2048, loss 0.671034
epoch 2176, loss 0.713408
epoch 2304, loss 0.732574
epoch 2432, loss 0.579756
epoch 2560, loss 0.671085
epoch 2688, loss 0.822742
epoch 2816, loss 0.733887
epoch 2944, loss 0.758153
epoch 3072, loss 0.652888
epoch 3200, loss 0.703439
epoch 3328, loss 0.873124
epoch 3456, loss 0.770992
epoch 3584, loss 0.680935
epoch 3712, loss 0.567376
epoch 3840, loss 0.427173
epoch 3968, loss 0.556145
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0776735 0.00476591
-0.0412757 0.00350228
0.0194981 -0.0104074
0.0496683 0.0552017
0.0562851 0.0563277
0.0594397 0.0568646
0.0594374 0.0502755
-0.0707068 -0.0109489
0.0210209 0.0599044
-0.00249422 -0.0095101
0.0977799 0.0600509
-0.0118764 0.0589107
0.0281392 0.0562143
6.7959e-07 -0.000285137
-0.0621346 -0.00720013
-0.0267352 -0.00786366
0.0261391 0.0545248
-3.11621e-07 -0.0090301
0.022444 0.0470077
0.126222 0.0517015
0.0494968 -0.00077043
0.110244 0.0502369
0.0278229 0.0552311
0.0487559 0.0513956
0.0424215 0.0563486
0.0594399 0.052139
0.0691985 0.0538943
0.062595 0.0588311
2.05834e-07 -0.00971118
-0.0631232 -0.0133301
-2.95455e-07 -0.0071638
0.0267781 0.0499465
0.079197 0.0466009
0.0594396 0.0543951
0.0594379 0.0640736
0.104819 0.0501044
0.121243 0.0614602
-0.00737805 0.04863
0.0144707 -0.00590277
-0.0414059 -0.00643722
-0.0211014 0.00212669
0.0562851 0.0542717
-0.0295823 -0.011079
0.0595095 0.0531018
0.0865811 0.0581321
0.0211031 -0.00249068
-0.0662867 -0.0131129
-0.00968441 0.0493396
-0.0271157 -0.00855701
-0.0412711 -0.0104512
0.0727411 0.0620061
-0.0152606 -0.00354112
-0.0734312 -0.013058
0.135017 0.0593678
0.0397476 0.0499066
0.0373475 0.0480297
0.0870053 0.0499693
0.0594397 0.0590382
0.0394803 -0.00901322
0.0730674 0.0595685
0.0424268 0.0551754
0.0037846 -0.00998958
0.0251234 0.0571158
0.0594395 0.0535808
0.0204045 -0.00523946
0.133575 0.0535485
0.0247829 0.000579753
0.0861488 0.059935
0.023648 0.0517683
0.0791268 0.050946
0.0368964 0.0500314
0.0237987 0.0608184
-0.0271193 -0.00786777
0.0267369 -0.00923075
0.0897923 0.0528967
0.118684 0.0578347
0.059435 0.0537153
0.0594398 0.0553335
0.0611197 -0.00960179
0.0174335 0.0469993
0.0251231 0.0564707
0.0868849 0.0600481
-0.00378297 -0.00153386
2.41162e-06 -0.00211692
0.100667 0.0524863
0.0631163 -0.00273134
0.121243 0.0556986
0.0594452 0.0662214
0.100097 0.0598531
0.0860428 0.00443403
0.0210209 0.0583397
0.0691987 0.0543759
0.0989675 0.0605684
0.110244 0.0502976
-0.00459356 -0.00819072
-0.0271152 -0.0155046
0.0449618 0.00331877
0.0497671 0.0497176
0.060558 0.0509394
0.0247829 -0.00795056
0.0204119 1.67348e-05
-0.0247813 -0.00232146
0.0131845 -0.00125489
-0.0161248 0.0508545
-0.00142113 -0.00264338
0.0113491 -0.0114142
0.0950803 0.0551659
0.0594116 0.0502723
-0.0707068 -0.00994171
-1.92327e-05 -0.00256504
0.132178 0.0665629
-0.0807986 -0.00758138
0.0599031 0.054655
0.0594396 0.0568597
-0.0152556 -0.00721928
0.0803921 0.0544983
0.0861486 0.054753
-6.66928e-06 -0.0102592
0.0318766 -0.0076339
2.96104e-08 -0.00701367
-0.0207367 -0.00143487
0.0631207 -0.0117331
0.0813321 0.0552769
0.0461383 0.0558797
0.032158 0.0532536
0.0321632 0.0517134
0.023648 0.0554609
0.0267393 -0.00152454
parameters: [ 9.449  2.195  2.382  0.582  4.   ]. error: 1429951768.73.
----------------------------
epoch 0, loss 0.919453
epoch 128, loss 0.846708
epoch 256, loss 0.95809
epoch 384, loss 1.57195
epoch 512, loss 0.989738
epoch 640, loss 0.845373
epoch 768, loss 0.47958
epoch 896, loss 0.642723
epoch 1024, loss 0.92106
epoch 1152, loss 0.596412
epoch 1280, loss 0.77344
epoch 1408, loss 0.894636
epoch 1536, loss 0.918311
epoch 1664, loss 0.737204
epoch 1792, loss 0.801517
epoch 1920, loss 0.811206
epoch 2048, loss 0.882905
epoch 2176, loss 0.594015
epoch 2304, loss 0.78176
epoch 2432, loss 0.810957
epoch 2560, loss 0.681472
epoch 2688, loss 0.517716
epoch 2816, loss 0.648368
epoch 2944, loss 0.474447
epoch 3072, loss 0.677082
epoch 3200, loss 0.447979
epoch 3328, loss 0.53276
epoch 3456, loss 0.476019
epoch 3584, loss 0.509785
epoch 3712, loss 0.693018
epoch 3840, loss 0.562737
epoch 3968, loss 0.521527
epoch 4096, loss 0.837531
epoch 4224, loss 0.620323
epoch 4352, loss 0.678459
epoch 4480, loss 0.614306
epoch 4608, loss 0.606349
epoch 4736, loss 0.795618
epoch 4864, loss 0.501637
epoch 4992, loss 0.706228
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0362237 0.0114583
-0.0662867 -0.00145021
0.032902 0.0570539
-0.0144836 0.000352451
-0.0417488 0.00186328
0.0449751 0.0576034
0.0182077 0.0461781
0.0532677 0.00780763
-0.0412731 0.00673028
0.0562849 0.0581382
0.0871098 0.0560901
0.0997468 0.0545392
-0.0209824 -0.00293778
0.0458115 0.0560802
0.0870049 0.0505564
-0.0188599 0.00960553
2.5668e-06 0.007784
-0.0152606 0.0050517
0.0211031 -0.00295979
0.022444 0.0536397
0.0251233 0.0593262
0.0197009 0.0574684
0.0037846 -0.00069246
0.126171 0.0518889
0.12945 0.00543962
-0.129445 -0.000281333
0.0897923 0.0606155
-3.40966e-05 -0.0009909
0.0727412 0.0586747
0.0996717 0.0596969
-0.065961 0.00926908
0.0271187 -0.00386884
0.0327314 0.0542226
-3.6833e-05 -0.00106556
0.0594397 0.052257
0.0223968 0.0521752
-0.0631232 -0.00366552
-0.039089 0.00243784
-0.0318771 -0.00210837
0.0495796 -0.00357832
-0.0416423 -0.000612649
0.0859865 0.0595149
-0.0662867 -0.00145021
-0.0394792 0.00660317
0.0134225 0.0616749
0.125146 0.0519928
0.0594396 0.0489301
0.0188019 0.0557989
-2.86114e-05 0.00898421
-0.034636 -0.00101668
0.0897923 0.0607652
0.0857284 -0.0042646
-0.0707068 0.00839594
0.0237988 0.0525661
0.0495768 -0.000763117
0.0424529 0.0546805
-0.0118764 0.0547132
0.059435 0.0639543
1.96444e-05 -0.000770218
0.0594397 0.0554833
-0.036223 0.00761038
-0.000835063 -0.00205567
0.00596587 0.0593785
1.01848e-06 -0.00344273
0.0605534 0.0508389
-0.0271111 -6.0632e-05
0.0482639 0.0572599
0.0368959 0.0520685
0.0248197 -0.00219843
0.0673482 0.0510387
-0.0462076 -0.00129201
0.0859466 0.0589588
0.0390895 -0.0035211
0.101441 0.0500899
-3.91322e-05 0.00800037
0.0594397 0.0573798
0.0594397 0.0578635
-0.0662843 0.00235014
0.022215 0.0468082
0.0861488 0.0580321
0.0611204 -0.00128679
0.0117845 -0.003036
-0.0207367 0.00239917
0.076417 0.0519521
-0.10012 -0.00345341
-0.000835048 -0.000650468
0.0594402 0.0508395
0.0261394 0.0534186
0.0739227 0.0599186
0.0122194 0.0494942
-2.23337e-05 -0.00241395
0.059367 0.0475062
0.01684 0.0522635
0.0968255 0.0565699
0.13219 0.0651017
-0.0390959 0.0015295
0.0594396 0.0575398
0.0989677 0.0594699
-0.00628321 0.0461412
0.102034 0.0497872
0.0691068 0.0518748
0.0207331 0.00598117
0.021525 0.0104472
0.0626054 0.0537209
-0.0611177 0.00138645
0.0199307 0.058224
0.0661222 0.058454
0.0866799 0.0547337
0.00459996 -0.000767942
0.0594494 0.0633615
0.12945 -0.000168736
-0.0716782 -0.00467808
0.0927396 0.0548397
0.0964766 0.0536869
0.105503 0.0639164
0.0281392 0.0564519
-0.053266 -0.00205393
0.0859465 0.056402
0.0330948 0.0454176
-0.0204001 0.00311554
0.0492512 -0.00328857
-0.0362226 0.00487315
0.0322983 0.0542806
0.0477957 0.0570264
0.0996717 0.0596969
0.088729 0.0582523
0.0594204 0.0523859
0.0117845 0.00187127
parameters: [ 9.449  2.195  2.382  0.582  5.   ]. error: 5109773.16336.
----------------------------
epoch 0, loss 1.36181
epoch 128, loss 0.87748
epoch 256, loss 0.812715
epoch 384, loss 1.40417
epoch 512, loss 1.07025
epoch 640, loss 0.673604
epoch 768, loss 0.696145
epoch 896, loss 0.90821
epoch 1024, loss 0.763232
epoch 1152, loss 0.94892
epoch 1280, loss 0.639758
epoch 1408, loss 0.719457
epoch 1536, loss 0.548729
epoch 1664, loss 0.812825
epoch 1792, loss 0.718209
epoch 1920, loss 0.969803
epoch 2048, loss 0.515215
epoch 2176, loss 0.617101
epoch 2304, loss 0.82729
epoch 2432, loss 0.604823
epoch 2560, loss 0.611393
epoch 2688, loss 0.455418
epoch 2816, loss 0.669243
epoch 2944, loss 0.942562
epoch 3072, loss 0.603177
epoch 3200, loss 0.57245
epoch 3328, loss 0.645254
epoch 3456, loss 0.661155
epoch 3584, loss 0.681791
epoch 3712, loss 0.590087
epoch 3840, loss 0.612666
epoch 3968, loss 0.882502
epoch 4096, loss 0.778678
epoch 4224, loss 0.901075
epoch 4352, loss 0.685229
epoch 4480, loss 0.732297
epoch 4608, loss 0.531648
epoch 4736, loss 0.767064
epoch 4864, loss 0.702194
epoch 4992, loss 0.703717
epoch 5120, loss 0.840243
epoch 5248, loss 0.869446
epoch 5376, loss 0.739746
epoch 5504, loss 0.619399
epoch 5632, loss 0.607819
epoch 5760, loss 0.576905
epoch 5888, loss 0.673395
epoch 6016, loss 0.457807
epoch 6144, loss 0.585366
epoch 6272, loss 0.631671
epoch 6400, loss 0.59387
epoch 6528, loss 0.543785
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0776735 0.00293838
0.032933 0.0574035
0.0983621 0.0587375
-0.0177216 0.0125151
-0.000835048 0.00425296
0.0494968 0.0136356
0.0217933 0.0622838
0.0594398 0.0581407
0.0594562 0.0582492
-0.0414059 0.0103228
0.0859466 0.0600237
0.0106905 0.00504409
0.0594759 0.0574548
0.0594481 0.0687761
0.0991788 0.0582681
4.07333e-06 0.00557019
0.0346348 0.00401019
0.0626002 0.0586413
0.0562597 0.0590413
0.027823 0.0586523
0.0144853 0.010686
0.0589763 0.0584036
0.088729 0.0604043
0.0897923 0.060139
0.038792 0.0567914
0.0222152 0.0599261
-0.000792737 0.01338
0.0209836 0.0119012
0.000793148 0.0220745
0.0327313 0.0575241
0.0945038 0.0574829
-0.129445 0.00178922
0.0594397 0.0611175
0.088282 0.0563532
0.059435 0.059359
0.0749536 0.0563319
0.0592406 0.0584748
0.0236481 0.0582256
0.0131861 0.0201495
0.0317696 0.0571339
0.0707066 0.00406672
-0.0716787 0.00444905
0.0541227 0.0563697
-1.99995e-06 0.0102319
0.0412725 0.00720617
-0.0776729 0.00443383
0.0648671 0.0574122
0.0237988 0.0582303
-1.63011e-07 0.00383749
0.0901953 0.058239
6.7959e-07 0.0113472
0.0131851 0.0127646
-0.0113473 0.0127244
0.0234681 0.00777143
0.0199307 0.0627982
0.0594397 0.060427
0.0197009 0.0577015
-0.00236357 0.0587269
-0.000835377 0.00829703
0.0594397 0.058801
-0.0707062 0.0164453
-0.0207292 0.0117256
0.0691068 0.0581203
0.00459996 0.010671
2.96104e-08 0.00874297
0.125873 0.0591301
-0.0734426 0.00607385
0.0368964 0.0575943
0.0191272 0.0547452
0.0321467 0.0579662
0.0346348 0.00589792
-0.0117817 0.00319595
0.0223972 0.0555511
0.062595 0.0592614
0.0237987 0.0577544
0.0594301 0.0588795
0.0234704 0.00324375
0.0271115 0.00647478
0.0594396 0.0621404
-0.0056768 0.0568805
0.0435741 0.0590598
0.0362234 0.00905907
-0.0734361 0.0103841
-0.0131855 0.00755769
-0.0734426 0.00511035
-0.0188023 0.00537856
0.0174339 0.0566994
0.0464004 0.0584443
0.0582389 0.057897
0.0861488 0.0583999
-0.0109489 0.016956
-0.0318771 0.00787503
0.101441 0.0571448
0.0477959 0.0575099
0.0887289 0.0587525
0.0582589 0.0577309
-0.0110929 0.0573203
0.0710837 0.0585144
0.027745 0.0546197
-0.0394796 0.00695133
0.0983621 0.0600451
-0.0161473 0.0589424
0.0394799 0.0205665
0.000834798 0.0105597
0.0594398 0.0607894
0.0317699 0.0617301
0.000802697 0.0138593
0.0589133 0.0573395
-0.00236357 0.0587665
-0.0662877 0.00236428
0.0482639 0.0580479
0.0117785 0.00306195
-0.0326435 0.00424746
0.0330953 0.0562787
0.028139 0.0586244
0.0611197 0.00495677
-0.0271152 0.0032864
0.0594395 0.0555273
-0.0152533 0.0105206
0.0281391 0.0547729
0.0594402 0.059956
0.0396776 0.058142
0.0621308 0.00355549
0.109566 0.0596652
-0.034636 0.00275965
0.0375434 0.0586616
0.00142251 0.0107966
-0.0132848 0.0683254
parameters: [ 9.449  2.195  2.382  0.582  6.618]. error: 473734965355.0.
----------------------------
epoch 0, loss 1.25023
epoch 128, loss 1.05863
epoch 256, loss 0.795581
epoch 384, loss 0.924728
epoch 512, loss 0.759871
epoch 640, loss 0.804913
epoch 768, loss 0.658257
epoch 896, loss 0.562829
epoch 1024, loss 0.808209
epoch 1152, loss 0.801436
epoch 1280, loss 0.720694
epoch 1408, loss 0.641235
epoch 1536, loss 0.672421
epoch 1664, loss 0.642583
epoch 1792, loss 0.650041
epoch 1920, loss 0.727283
epoch 2048, loss 0.74983
epoch 2176, loss 0.53921
epoch 2304, loss 0.551833
epoch 2432, loss 0.641205
epoch 2560, loss 0.589337
epoch 2688, loss 0.83682
epoch 2816, loss 0.466972
epoch 2944, loss 0.743047
epoch 3072, loss 0.713387
epoch 3200, loss 0.569562
epoch 3328, loss 0.597826
epoch 3456, loss 0.779077
epoch 3584, loss 0.911362
epoch 3712, loss 0.623905
epoch 3840, loss 0.697532
epoch 3968, loss 0.619164
epoch 4096, loss 0.751928
epoch 4224, loss 0.608915
epoch 4352, loss 0.818005
epoch 4480, loss 0.744578
epoch 4608, loss 0.648933
epoch 4736, loss 0.634409
epoch 4864, loss 0.535993
epoch 4992, loss 0.482592
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0477959 0.0550885
0.0329331 0.0552289
0.0920969 0.0512561
0.0594398 0.0544486
-0.053257 0.0133187
-0.0707068 0.00492439
-0.0394796 0.01235
0.0989675 0.0563097
0.0417702 0.00715618
-0.00236373 0.0540153
0.0281391 0.0522776
0.104819 0.0524577
0.081527 0.0488828
0.132184 0.0621501
0.0791969 0.0503899
0.0477959 0.0521145
-0.0194897 0.000766017
0.0305923 0.0494079
0.0983621 0.0489261
0.0857794 0.0497316
0.0859962 0.0562314
0.0594349 0.0511439
0.00460075 0.000425988
-0.0188028 -0.00266296
-0.10011 0.00116796
0.0300726 -0.00190481
0.0300757 0.00131189
0.0385291 0.000533939
0.0237989 0.0513264
0.0529686 0.0107661
0.0487559 0.0508522
0.000793148 0.00621309
0.0791961 0.0514253
0.0815268 0.0504848
0.0734394 0.000841864
-0.0271278 0.00283361
-0.053257 0.00597265
0.0861488 0.056328
0.129445 0.00365128
-0.00459356 0.00804
0.0594273 0.0528839
0.0204143 0.0145478
-0.0144843 0.00804612
-0.00894944 0.00365672
0.0865811 0.0561581
-0.0207342 0.00574685
-0.0210947 0.000265756
0.0234681 0.000580733
0.0131861 0.0134046
0.100116 0.00387021
0.0467944 0.00889973
0.0435741 0.0507116
0.0626054 0.05012
-0.0110929 0.0421496
0.0538015 0.0530832
0.0211182 0.0549452
0.0950805 0.0584981
0.0458116 0.0554235
0.0327314 0.0548445
0.106629 0.0523466
0.0592406 0.0472676
0.112932 0.0561182
0.0188018 0.0569532
0.100097 0.0579062
0.043628 0.0505999
0.0373475 0.0483879
0.0321586 0.0558336
0.0317696 0.0574468
0.0659505 0.00350545
0.0234704 0.0159407
0.0243594 0.0502326
0.0861487 0.0563923
0.088282 0.0518139
0.0520969 0.0552065
0.032933 0.0564091
0.129916 0.0486065
-0.0326435 -0.000481139
-0.0131858 0.00481444
0.0468064 0.0157476
-0.0394792 0.0112576
0.0594376 0.0516336
-0.0394792 0.00349603
0.09274 0.0569243
0.0197008 0.0547514
-0.0390898 0.00499172
0.0594397 0.0537065
-0.0250779 0.0502184
0.032933 0.0574485
0.0907411 0.0563333
0.0495763 -0.000603906
0.0322983 0.0553893
0.0861486 0.0550937
0.0861488 0.0538716
0.0194981 0.000164606
0.0248097 0.00882642
-0.00894944 0.0117728
0.0237987 0.0545647
-0.0271152 0.00253089
3.40452e-06 0.0125421
-0.0109842 0.00563044
0.0541227 0.0488468
0.121243 0.0596027
0.0997468 0.0498211
0.0482639 0.0541037
0.0234704 -4.84838e-05
0.0594324 0.0512851
-0.063123 0.00171826
-0.0271157 -0.00345239
0.0707066 0.00338609
0.121243 0.0580092
-0.129449 0.00856352
0.05627 0.0505706
0.094504 0.0535516
0.135027 0.052888
0.0527578 0.0555832
0.0362233 0.0055107
0.0482639 0.0548893
0.0621302 -0.00160078
0.100668 0.0470195
0.0237987 0.056403
0.0691986 0.0559463
0.0677309 0.04988
0.062595 0.0559059
0.0247038 0.0516462
0.0605996 0.0473578
-0.0416401 0.00729905
0.0220537 0.0531578
0.0977801 0.057484
parameters: [ 9.449  2.195  2.382  0.582  5.   ]. error: 8238861.12524.
----------------------------
epoch 0, loss 0.933409
epoch 128, loss 1.53538
epoch 256, loss 0.773385
epoch 384, loss 0.799725
epoch 512, loss 0.663408
epoch 640, loss 0.784237
epoch 768, loss 0.752959
epoch 896, loss 0.871026
epoch 1024, loss 0.707483
epoch 1152, loss 0.537942
epoch 1280, loss 0.749573
epoch 1408, loss 0.890626
epoch 1536, loss 0.858043
epoch 1664, loss 0.911555
epoch 1792, loss 0.707532
epoch 1920, loss 0.889192
epoch 2048, loss 0.680738
epoch 2176, loss 0.752332
epoch 2304, loss 0.626034
epoch 2432, loss 0.649587
epoch 2560, loss 1.1419
epoch 2688, loss 1.03009
epoch 2816, loss 0.39579
epoch 2944, loss 0.658217
epoch 3072, loss 0.705104
epoch 3200, loss 0.836957
epoch 3328, loss 0.875498
epoch 3456, loss 0.604588
epoch 3584, loss 0.621861
epoch 3712, loss 0.755403
epoch 3840, loss 0.824836
epoch 3968, loss 0.696499
epoch 4096, loss 0.763114
epoch 4224, loss 0.686795
epoch 4352, loss 0.514257
epoch 4480, loss 0.801849
epoch 4608, loss 0.60693
epoch 4736, loss 0.805618
epoch 4864, loss 0.536443
epoch 4992, loss 0.664479
epoch 5120, loss 0.644779
epoch 5248, loss 0.802908
epoch 5376, loss 0.596686
epoch 5504, loss 0.423161
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.110244 0.05159
0.0318693 0.0548091
0.0734394 0.0109036
0.101441 0.0560355
0.0106905 0.00476898
0.0667826 0.0563158
0.059407 0.0492993
0.0327313 0.0593042
-0.0152583 0.00566956
0.0194981 0.00853382
0.0327315 0.0585384
0.0593665 0.0494352
4.17498e-06 0.00215402
0.0807987 0.00451907
-0.0188606 0.0123859
0.028139 0.0593624
0.0857795 0.0565358
0.0859962 0.0627204
-0.0152606 0.00326678
0.0878299 0.0559499
-0.0860382 0.00666735
0.0781223 0.0523347
0.0594494 0.0682902
0.0807987 0.0161958
0.0927396 0.0606065
0.0562851 0.0558938
-0.0449599 0.00992339
0.0865811 0.0582873
0.0781227 0.0502701
0.0861488 0.0603271
0.0267779 0.0528602
0.0878297 0.0536772
0.0412752 0.00550815
0.0594473 0.0497451
-7.68032e-07 0.00833781
0.0870053 0.0561412
0.0707072 0.00619784
0.0594397 0.0578235
0.112932 0.0629388
-0.0413988 0.004727
0.000794682 0.0105918
0.0691068 0.0515329
-0.000835063 0.00366613
-0.00731719 0.0450831
0.0868851 0.0597206
0.130756 0.0582961
0.0243585 0.0524734
-0.0385424 0.0131583
0.0594396 0.0548191
0.0867335 0.0610974
-0.00894125 0.00482359
0.0152616 0.0099315
0.0424581 0.0575374
-0.0631103 0.00127787
0.043921 0.0535832
0.129993 0.0496529
0.0968257 0.0602557
3.36628e-05 0.0133107
0.0706155 0.0592321
-0.0109566 0.0100913
0.0458117 0.0535231
0.121243 0.0580305
0.0594397 0.0582954
0.0224432 0.0538473
0.00863088 0.0540454
0.0482639 0.0576962
0.0599029 0.0573144
0.0496681 0.0588526
0.0237986 0.0574538
0.0464005 0.0534924
0.0529686 0.0032423
0.0394799 0.0147494
0.0859466 0.0603932
0.0871095 0.0602958
0.0594324 0.0541696
0.0594919 0.0522767
0.0867333 0.0590665
0.0220537 0.0590131
0.074953 0.05571
0.00856342 0.00286413
-0.0716787 0.00741745
0.0857789 0.0545293
0.088729 0.0580567
0.097887 0.062626
0.0188088 0.00522267
0.0520969 0.0573144
0.0945038 0.0538886
0.0267781 0.0537556
-0.041769 0.00480286
0.0177216 0.0101038
0.0248097 0.00536132
0.0458117 0.0556706
0.0416434 0.016202
0.100668 0.0510521
0.0261395 0.050688
0.0333261 0.0609855
0.026809 0.0541004
0.00460075 0.00701022
0.00931421 0.0551186
0.0319945 0.05273
0.0776735 0.0053974
-0.0495737 0.00934544
-0.014471 0.00890088
-0.00628221 0.0468545
0.0362233 0.0146221
-0.00131656 0.00902799
0.0813321 0.0577629
0.00250079 0.00183116
0.0562649 0.0544617
0.0251233 0.0623957
0.05937 0.0475759
0.0468012 0.00391646
-0.000805986 0.00496305
-0.0118768 0.0543132
0.0868848 0.0568152
0.0117845 0.00364906
0.0301505 0.0552567
-0.0416468 0.0119593
-0.0189887 0.0079079
0.100116 0.00446374
0.0966596 0.0545796
-0.0207316 0.00958039
0.0122504 0.0537867
0.000782366 0.0111892
0.0215366 0.0132462
-0.00250418 0.00758212
-0.000835392 0.00961331
0.0251234 0.0610987
parameters: [ 9.449  2.195  2.382  0.582  5.618]. error: 60486.2642009.
----------------------------
epoch 0, loss 1.13869
epoch 128, loss 1.16228
epoch 256, loss 0.778426
epoch 384, loss 0.892342
epoch 512, loss 1.16387
epoch 640, loss 0.634445
epoch 768, loss 0.84952
epoch 896, loss 0.795152
epoch 1024, loss 0.599815
epoch 1152, loss 0.798092
epoch 1280, loss 0.543923
epoch 1408, loss 1.04979
epoch 1536, loss 0.815836
epoch 1664, loss 0.779374
epoch 1792, loss 0.75425
epoch 1920, loss 0.648573
epoch 2048, loss 0.678902
epoch 2176, loss 0.607533
epoch 2304, loss 0.783704
epoch 2432, loss 0.944301
epoch 2560, loss 0.890142
epoch 2688, loss 0.673063
epoch 2816, loss 0.480616
epoch 2944, loss 0.807554
epoch 3072, loss 0.603507
epoch 3200, loss 0.895637
epoch 3328, loss 0.574966
epoch 3456, loss 0.721805
epoch 3584, loss 0.741495
epoch 3712, loss 0.843102
epoch 3840, loss 0.501357
epoch 3968, loss 0.894341
epoch 4096, loss 0.562813
epoch 4224, loss 0.619721
epoch 4352, loss 0.519018
epoch 4480, loss 0.51009
epoch 4608, loss 0.573367
epoch 4736, loss 0.593886
epoch 4864, loss 0.676277
epoch 4992, loss 0.710356
epoch 5120, loss 0.531437
epoch 5248, loss 0.649115
epoch 5376, loss 1.03487
epoch 5504, loss 0.654619
epoch 5632, loss 0.668065
epoch 5760, loss 0.53972
epoch 5888, loss 0.736992
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0177194 -0.00301404
0.0326439 -0.00134468
0.0594397 0.0536419
-0.0662833 -0.00512282
-0.0662833 -0.00616997
0.0414075 -0.00458239
0.0144704 0.00118792
0.0237988 0.0510872
0.0414025 0.00468605
0.0859465 0.0566156
0.0594399 0.0485366
0.015257 0.00184432
0.0727411 0.0553533
0.0776855 -0.00189662
0.076417 0.0498829
4.07609e-05 -0.00290079
-0.00236357 0.0506152
0.0189892 0.0126548
0.0461383 0.0527406
0.0182076 0.0439798
0.0981543 0.0487598
0.0037846 0.00604382
-0.0716686 -0.00566959
0.036223 -0.00306177
0.0337182 0.0462039
0.0497672 0.0473904
0.0871095 0.0545827
0.0390895 -0.00479092
0.0482638 0.0541689
0.0807987 0.0105505
5.31452e-07 0.00418161
0.0322983 0.0540629
0.0871097 0.0496594
-0.0631101 -0.00508993
0.0329331 0.0541498
0.024704 0.0441771
0.0631207 -0.00538783
0.0859467 0.0562272
0.0188609 -0.00289409
0.0730674 0.0563906
0.0800819 0.0474491
0.0968256 0.0573946
0.0144704 -0.00423231
0.0594398 0.0510693
0.0529729 0.000923184
-0.0707055 0.000120633
0.0305919 0.0495987
0.100668 0.0486402
0.0321468 0.0517452
0.0594398 0.0542151
0.0594397 0.0535322
2.41162e-06 0.00195577
0.0261394 0.0567089
0.0368964 0.0501442
1.13647e-06 -0.00266296
-0.00700262 0.0447552
-0.0194973 -0.00150631
-0.0323135 -0.00644169
0.0927397 0.0582561
0.0594397 0.0535854
-0.0412757 0.00359237
0.0594397 0.0528968
-0.0272995 -0.00045172
0.0911173 0.0508521
0.0882822 0.0476202
-3.91322e-05 -0.00197502
-0.0416468 0.00180131
0.0278231 0.0503572
0.0592543 0.0487669
0.0243594 0.0476915
0.0532535 0.0480689
-0.0611272 -0.00089912
0.130756 0.0587844
0.00596546 0.0553277
0.0267419 -0.00400971
0.125157 0.0480371
0.132184 0.0666833
0.0373484 0.0460538
-0.0412731 -0.00459035
0.0860364 0.00465837
0.0321468 0.0509862
0.0197009 0.0541906
0.0192074 0.0543798
0.0134225 0.0655214
-1.92327e-05 0.0011269
-0.000805986 0.00498863
0.0945153 0.0480379
0.130756 0.0501179
0.0384776 0.0513693
0.0330542 0.0482041
0.0991788 0.0577862
0.0861487 0.0549754
0.0582389 0.0495382
0.0217931 0.0558503
0.0941706 0.0466378
-0.032317 -0.00445051
0.0594494 0.0658555
0.0220537 0.0534894
0.0691068 0.0479754
0.00894166 -0.00261792
-0.027288 0.00621387
-0.0118765 0.0504703
-0.0707068 0.00299937
-0.0417573 0.00448589
0.10665 0.0501924
0.0594397 0.0559707
0.0594399 0.0550617
0.043628 0.0471715
-0.0417573 -0.0027957
0.062595 0.0543713
0.0236481 0.0507386
-0.0362233 0.0041063
-0.0467978 0.00757039
0.0152545 0.00349116
0.0261392 0.0534511
0.0859466 0.0564003
0.0394802 0.0076033
-0.053257 0.00106995
-0.0621274 -0.00607353
-0.0194973 0.00191922
0.0594399 0.0511354
0.10664 0.051849
0.0210212 0.0533153
0.0188016 0.0562469
0.0267781 0.0452831
0.093775 0.0572466
0.0538297 0.0493883
-0.0416468 -0.00270275
parameters: [ 9.449  2.195  2.382  0.582  6.   ]. error: 54916498.2729.
----------------------------
epoch 0, loss 1.32429
epoch 128, loss 0.944127
epoch 256, loss 1.20309
epoch 384, loss 0.891444
epoch 512, loss 0.833652
epoch 640, loss 0.956989
epoch 768, loss 0.933017
epoch 896, loss 0.966424
epoch 1024, loss 0.744886
epoch 1152, loss 0.970885
epoch 1280, loss 0.989884
epoch 1408, loss 0.748587
epoch 1536, loss 0.643693
epoch 1664, loss 0.56836
epoch 1792, loss 0.51715
epoch 1920, loss 0.608614
epoch 2048, loss 1.10432
epoch 2176, loss 0.767265
epoch 2304, loss 0.750263
epoch 2432, loss 0.838023
epoch 2560, loss 0.765411
epoch 2688, loss 0.767643
epoch 2816, loss 0.689438
epoch 2944, loss 0.67321
epoch 3072, loss 0.874684
epoch 3200, loss 0.750683
epoch 3328, loss 0.916569
epoch 3456, loss 0.771281
epoch 3584, loss 0.6934
epoch 3712, loss 0.680548
epoch 3840, loss 0.608525
epoch 3968, loss 0.581831
epoch 4096, loss 0.676487
epoch 4224, loss 0.650766
epoch 4352, loss 0.655784
epoch 4480, loss 0.460219
epoch 4608, loss 0.821427
epoch 4736, loss 0.793852
epoch 4864, loss 0.641502
epoch 4992, loss 0.528881
epoch 5120, loss 0.721113
epoch 5248, loss 0.608264
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0093143 0.0499332
0.0594396 0.0519008
0.0867335 0.0521802
0.0871097 0.0501386
0.021525 0.00140225
-0.0340329 0.00714013
0.0594273 0.0490045
-0.00700272 0.0525885
0.0458117 0.0520429
0.0594297 0.0508815
0.01684 0.0482088
0.09274 0.0536392
-0.0417573 0.00117961
0.129453 0.00297313
-0.0860431 0.00443161
-0.000792737 -0.00212165
0.059181 0.0463158
-0.0468045 -0.00518642
0.0724741 0.0469591
-0.0295559 0.00768879
0.0691072 0.0472099
-0.0734426 0.00563779
0.0424581 0.0508677
0.125873 0.052714
-0.0161248 0.0512459
0.0716711 -0.00278025
-2.64683e-07 0.00330616
0.0281393 0.0522771
0.0594399 0.0549109
-2.15513e-06 -0.00411575
0.0991789 0.0532074
0.0417702 -0.00294173
0.0871096 0.0510868
0.0384676 0.050723
0.0174339 0.0474367
0.0182076 0.0493289
0.0267779 0.0471019
0.112932 0.0544413
0.0860587 0.00448882
0.0109517 -0.000536158
-0.0492483 -0.00389535
0.0362237 -0.00313903
0.100116 -0.00570797
-0.0208232 -0.00490609
0.0188609 0.00819136
-0.0362226 0.00293194
0.0661224 0.0539001
0.100097 0.0571268
-0.0248092 0.00866183
-4.6184e-07 -0.00360595
0.0234754 -0.00237229
-0.0860447 -0.000391985
0.0220539 0.0523706
-0.00726943 0.0442275
0.0268195 0.0510314
0.0417583 -0.000444605
0.0482639 0.0522412
0.0174335 0.047442
0.0867202 0.0522827
0.090741 0.0531042
0.0417702 -0.00457251
0.0271289 0.00178586
-0.0215356 0.00527994
0.0997468 0.0494998
-0.02083 0.00108676
-0.0271153 -0.00424534
0.0281393 0.0534149
0.0710837 0.0530816
0.0201879 0.049953
-0.0529659 -0.00434456
0.088282 0.0480978
0.130757 0.0511546
0.0882822 0.0485945
-0.0412731 -0.00076298
0.13219 0.0631766
-0.0144713 0.000695341
-0.0023637 0.0487301
0.0268142 0.0508104
0.0396776 0.0481649
0.12945 0.0032902
-0.041769 -0.00164224
0.0317696 0.0532352
-0.129453 -0.00118367
-0.00331175 0.0431331
-0.0394796 -0.00372821
-4.6184e-07 0.0102167
0.0986864 0.0505345
0.0594273 0.0504134
0.022444 0.0494766
0.0131855 -0.00334293
0.0764272 0.0496168
0.0373484 0.0485254
-0.0118767 0.0528556
0.100097 0.0566598
0.0330953 0.0499745
-0.0494953 -0.00030101
0.0594398 0.0517014
0.0593105 0.044652
-3.6833e-05 -0.00014853
0.0927397 0.0529884
0.0271289 -0.00210993
0.026809 0.0488331
0.0385431 0.000416291
0.0234704 0.00287999
0.0621339 -0.00164733
-0.065961 0.00203958
-0.0412711 -0.000413037
0.0593346 0.0440439
-0.129457 0.00219336
0.059435 0.0619807
0.0594374 0.0499634
0.0220538 0.052267
0.0461385 0.052263
-0.0385433 -0.00153265
0.0599029 0.0530144
0.0964772 0.0498069
0.0362237 -0.0027665
-0.0250779 0.049938
0.0318695 0.0475357
0.0458115 0.0529377
0.00378707 -0.00403624
-0.00731719 0.0406004
0.0305919 0.0488521
0.0611204 -0.00172473
0.0901952 0.0496042
0.0594396 0.0496646
0.0781227 0.0482969
0.100097 0.0573161
parameters: [ 9.449  2.195  2.382  0.582  5.351]. error: 53873714.2173.
----------------------------
epoch 0, loss 1.11632
epoch 128, loss 1.1842
epoch 256, loss 1.52549
epoch 384, loss 0.946332
epoch 512, loss 0.724329
epoch 640, loss 0.810237
epoch 768, loss 0.934568
epoch 896, loss 0.879356
epoch 1024, loss 0.594512
epoch 1152, loss 0.721828
epoch 1280, loss 0.550328
epoch 1408, loss 0.78091
epoch 1536, loss 0.532789
epoch 1664, loss 0.797374
epoch 1792, loss 0.58638
epoch 1920, loss 0.76184
epoch 2048, loss 0.626513
epoch 2176, loss 0.646623
epoch 2304, loss 0.595635
epoch 2432, loss 0.629305
epoch 2560, loss 0.454569
epoch 2688, loss 0.730771
epoch 2816, loss 0.558524
epoch 2944, loss 0.673187
epoch 3072, loss 0.804586
epoch 3200, loss 0.876307
epoch 3328, loss 0.565985
epoch 3456, loss 0.718416
epoch 3584, loss 0.685078
epoch 3712, loss 0.640172
epoch 3840, loss 0.690071
epoch 3968, loss 0.601191
epoch 4096, loss 0.897126
epoch 4224, loss 0.925571
epoch 4352, loss 0.654455
epoch 4480, loss 0.78437
epoch 4608, loss 0.979276
epoch 4736, loss 0.643949
epoch 4864, loss 0.644288
epoch 4992, loss 0.744824
epoch 5120, loss 0.741092
epoch 5248, loss 0.593589
epoch 5376, loss 0.442919
epoch 5504, loss 0.539595
epoch 5632, loss 0.530874
epoch 5760, loss 0.760204
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0424428 0.0572671
0.0223972 0.0571279
0.0290868 0.058558
0.133575 0.0552157
0.0243594 0.0566929
0.0330948 0.0568719
0.090741 0.0589733
0.028139 0.0604552
0.00931421 0.0569094
0.00931426 0.0585555
0.0152545 0.0127031
-0.00378521 0.00574288
0.0594399 0.0603286
0.0248197 0.0138475
0.0390991 0.0101409
-0.129453 0.0063217
-0.0188062 0.00191935
0.0952315 0.0605072
0.0594397 0.059584
0.0321468 0.0592539
0.0851612 0.0569851
-0.0023636 0.0557512
0.0248076 0.0161483
0.0412752 0.00576939
0.0966587 0.0562185
0.0283783 0.0581198
3.5013e-07 0.0199551
0.129453 0.0168704
0.0295774 0.0101222
0.0594398 0.0599218
0.0594481 0.0656947
-0.0210947 0.016839
0.0271185 0.00382047
0.0673482 0.0556954
0.0417702 0.021093
0.0286795 0.0558497
0.018802 0.00236944
0.0496683 0.0592758
0.0482639 0.0594727
0.0941706 0.0567923
0.134995 0.0588729
-0.041769 0.00484357
0.0764272 0.0584713
0.0497673 0.0572479
0.0871098 0.0576774
0.0945153 0.0555244
0.0223972 0.0576992
7.08095e-06 0.0062789
0.0477959 0.0594241
-0.0188593 0.0225738
-0.0161248 0.0573002
0.0222151 0.0570886
0.0407515 0.0570062
0.143948 0.0593074
0.0865811 0.0595825
0.0807993 0.00960538
0.0385437 0.00958832
0.000834798 0.00771191
0.090741 0.0577735
0.0867202 0.0594648
7.08095e-06 0.0133454
0.0468064 0.0107923
-0.0621274 0.0117546
0.0330953 0.0563909
0.0251233 0.0617163
-0.0394796 0.0126002
0.10955 0.0591713
-0.00737805 0.0554189
0.0113491 0.0050472
0.110244 0.0575232
0.0907411 0.0593515
-0.0106876 0.0090189
0.0508582 0.057032
0.106655 0.0579717
0.0271185 0.0117747
0.0910567 0.0597983
-0.0394796 0.0197271
-0.00459356 0.00367253
0.059367 0.0551046
0.0599612 0.0573968
-4.6184e-07 0.00802284
-0.129445 0.0187009
-3.11621e-07 0.0135888
0.0477959 0.0603762
-0.0113473 0.00481019
9.99101e-07 0.00548858
0.01684 0.0573574
0.0857183 0.00459378
0.019701 0.0599095
0.0764118 0.0591073
-0.0385431 0.0055568
-0.000835063 0.0125293
0.0941706 0.0581633
0.0983621 0.0557867
0.090741 0.059193
0.0412725 0.0123655
0.0968257 0.0586077
0.021525 0.00957329
0.032158 0.0570064
-4.71368e-07 0.00832695
0.109566 0.0602191
0.0330953 0.0574588
0.0861488 0.0603735
0.0662931 0.0048605
0.0710835 0.0603976
0.0941706 0.058068
-0.0131855 0.0125
-0.0707068 0.00903081
-0.0494953 0.0158183
0.0827282 0.0144317
1.13647e-06 0.0116208
-0.00131975 0.00725393
-0.0177216 0.00606787
0.0897923 0.0595766
-4.4075e-07 0.00633121
0.0594467 0.065108
0.0251234 0.0610119
0.0871096 0.0599337
-0.0295823 0.0141882
0.121243 0.0609845
0.0390991 0.00736229
-0.0117817 0.00746013
0.046209 0.00438098
-0.053266 0.0124824
-5.97989e-06 0.00656246
0.0424529 0.0584342
0.0174339 0.056368
0.0267443 0.010108
parameters: [ 9.449  2.195  2.382  0.582  5.764]. error: 7754667338.96.
----------------------------
epoch 0, loss 1.81609
epoch 128, loss 0.895854
epoch 256, loss 0.785201
epoch 384, loss 0.959423
epoch 512, loss 0.905836
epoch 640, loss 0.676397
epoch 768, loss 0.709694
epoch 896, loss 0.886409
epoch 1024, loss 0.940783
epoch 1152, loss 0.703085
epoch 1280, loss 0.773138
epoch 1408, loss 0.957176
epoch 1536, loss 0.70689
epoch 1664, loss 0.898954
epoch 1792, loss 0.742112
epoch 1920, loss 0.732789
epoch 2048, loss 0.713826
epoch 2176, loss 0.84148
epoch 2304, loss 0.655177
epoch 2432, loss 0.652761
epoch 2560, loss 0.615598
epoch 2688, loss 0.725709
epoch 2816, loss 0.705925
epoch 2944, loss 0.796194
epoch 3072, loss 0.7483
epoch 3200, loss 0.866518
epoch 3328, loss 0.893981
epoch 3456, loss 0.880649
epoch 3584, loss 0.711956
epoch 3712, loss 0.659159
epoch 3840, loss 0.855271
epoch 3968, loss 0.840426
epoch 4096, loss 0.570716
epoch 4224, loss 0.60453
epoch 4352, loss 0.623424
epoch 4480, loss 0.843442
epoch 4608, loss 0.552328
epoch 4736, loss 0.760817
epoch 4864, loss 0.622823
epoch 4992, loss 0.588625
epoch 5120, loss 0.790767
epoch 5248, loss 0.543188
epoch 5376, loss 0.523961
epoch 5504, loss 1.02344
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0527575 0.0633412
-0.0416423 0.00906461
0.0122295 0.0613226
0.0201879 0.0601842
0.0278228 0.0620068
-0.0152533 0.0224074
0.0390895 0.0119145
0.0261392 0.0620307
-2.23337e-05 0.0122728
0.0318695 0.0604377
0.046209 0.00921786
-0.016115 0.0591105
0.0122352 0.061584
-0.0295746 0.0106284
0.0532557 0.0104774
0.00891968 0.00660162
0.0321468 0.062431
0.0295843 0.01066
0.126086 0.0608009
-0.0495737 0.00373053
0.0305923 0.0587414
0.0815268 0.0588376
0.085161 0.063078
0.081527 0.0592877
0.0286789 0.062841
0.0707072 0.0171244
0.13219 0.0648898
-0.0132965 0.0640193
-0.032317 0.00365266
0.0384777 0.061021
0.0857183 0.0161441
0.0327314 0.0619148
-0.00236373 0.0651528
0.076417 0.0595478
1.01848e-06 0.010493
0.0208321 0.0230045
0.0322983 0.06384
-0.0462049 0.019462
-0.0267352 0.0097996
0.0529686 0.0190868
-0.0362233 0.0177947
0.0188609 0.0194262
0.121243 0.0653279
0.135027 0.0592561
0.0329331 0.0622101
0.0594398 0.0637354
0.0851609 0.0616323
0.023648 0.0627029
0.093775 0.0619066
9.99101e-07 0.0264996
0.00932522 0.0588433
0.100097 0.0622707
0.0237987 0.0594053
0.0594376 0.0621506
0.100097 0.0627249
0.0237988 0.0607941
0.132184 0.0644371
-4.6184e-07 0.0181593
0.00894003 0.0097906
0.0991788 0.0622443
0.0707072 0.0345039
0.0462156 0.0308721
0.109566 0.0653449
0.0807977 0.0209249
-0.0776729 0.0356913
-0.0807993 0.00614528
-0.0734312 0.0157201
-0.0188023 0.00544538
0.0661225 0.0628898
0.0318693 0.0611988
-0.0417491 0.00309958
0.028139 0.0626021
0.0594494 0.0655882
0.0594398 0.063816
0.0592543 0.0575888
0.0621308 0.0139691
0.018802 0.00978072
0.0496681 0.0613946
0.00596587 0.0629871
-0.0621343 0.00119676
0.0271185 0.00308794
0.0594473 0.0583288
0.0594397 0.0637442
0.0494968 0.021527
0.0390895 0.00564367
0.0859466 0.0623999
0.0477959 0.0631146
-0.0340213 0.0171391
0.0290868 0.0616747
0.0920961 0.0624265
0.020735 0.0219952
-0.0113495 0.0154996
0.0907408 0.0625775
-0.00378521 0.0297808
0.0220538 0.0619645
-0.0468045 0.0206753
0.0621302 0.0064758
0.057604 0.0642782
0.0327313 0.0649913
-0.036223 0.0226743
0.0941706 0.0622943
-0.0529722 0.00709755
0.0710835 0.0621936
0.0197009 0.0605386
0.0532535 0.0572982
0.0594397 0.0645863
0.0991789 0.0618385
0.0271204 0.0198204
0.0290868 0.0617811
0.000213351 0.0623441
-0.036223 0.0193103
0.102035 0.0614511
0.00596587 0.0628553
0.0910567 0.0640358
0.118684 0.0634291
0.0477959 0.0633437
-0.000795203 0.00635848
0.0791268 0.0625897
-0.0295746 0.0224352
0.0191272 0.0594495
0.0594398 0.0636712
0.0188086 0.00711443
-0.0109774 0.0307709
0.0927399 0.0633007
0.0820225 0.0577074
0.0964307 0.0600009
0.081527 0.0598721
-0.0860431 0.00416711
parameters: [ 9.449  2.195  2.382  0.582  5.516]. error: 1145708259.05.
----------------------------
epoch 0, loss 0.895453
epoch 128, loss 0.777946
epoch 256, loss 0.6408
epoch 384, loss 0.748929
epoch 512, loss 0.640356
epoch 640, loss 0.755303
epoch 768, loss 0.477962
epoch 896, loss 0.779994
epoch 1024, loss 0.71017
epoch 1152, loss 0.868826
epoch 1280, loss 0.795933
epoch 1408, loss 0.847461
epoch 1536, loss 0.650514
epoch 1664, loss 0.742458
epoch 1792, loss 0.871597
epoch 1920, loss 0.594546
epoch 2048, loss 1.01583
epoch 2176, loss 0.572048
epoch 2304, loss 0.675122
epoch 2432, loss 0.495884
epoch 2560, loss 0.830129
epoch 2688, loss 0.789717
epoch 2816, loss 0.64568
epoch 2944, loss 0.808899
epoch 3072, loss 0.671711
epoch 3200, loss 0.745487
epoch 3328, loss 0.63547
epoch 3456, loss 0.795422
epoch 3584, loss 0.628368
epoch 3712, loss 0.607413
epoch 3840, loss 0.923511
epoch 3968, loss 0.49168
epoch 4096, loss 0.618985
epoch 4224, loss 0.693686
epoch 4352, loss 0.554926
epoch 4480, loss 0.771506
epoch 4608, loss 0.724867
epoch 4736, loss 0.529473
epoch 4864, loss 0.725287
epoch 4992, loss 0.789081
epoch 5120, loss 0.537763
epoch 5248, loss 0.874449
epoch 5376, loss 0.763223
epoch 5504, loss 0.768787
epoch 5632, loss 0.925124
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.121243 0.055195
0.105506 0.0652061
0.0251231 0.0567607
0.038792 0.0535494
0.112932 0.056939
0.100097 0.0587983
0.0897923 0.0570289
-0.0716686 0.00505884
-0.0529682 0.00653602
0.0592247 0.0520942
0.10665 0.0542872
-0.00236348 0.0548513
0.0562648 0.0532675
0.0496683 0.0594963
0.081527 0.0559576
-0.00733491 0.0548602
0.0594397 0.0594698
0.0394802 0.00984633
-0.0385424 0.00754284
0.062595 0.0582589
0.0941706 0.0534868
-0.00459356 -0.00101757
0.125873 0.0564168
0.101814 0.0584298
0.0197009 0.0560489
0.0278229 0.0593802
-0.0188028 0.00225253
-1.99995e-06 0.011219
-0.00968441 0.0535974
0.0131848 0.00289618
0.0527575 0.0582456
0.0532677 0.00747975
0.043628 0.0509007
-0.0189824 0.00779892
-0.0532643 0.0119995
0.0691986 0.0606134
0.121243 0.0617644
0.0204028 0.01431
-3.76332e-06 0.00137943
0.0707062 0.00873842
0.0194981 0.00370348
0.0594398 0.0592414
-0.0189887 0.00593155
-0.000835377 0.00156166
0.05627 0.0532273
0.0329021 0.0584781
0.0286795 0.0502869
0.0261394 0.0535324
-0.0207342 0.00164453
0.130756 0.0603895
0.0310444 0.0526477
0.0204119 0.00675212
0.0122404 0.0517014
0.0599612 0.0492031
-0.0417425 0.0029136
-0.00142797 0.0083286
0.0375381 0.0512247
-0.0113373 0.0037969
-0.0707068 0.00129151
0.0594396 0.059224
0.0396776 0.0505734
0.0594349 0.0558017
0.0207193 0.0538744
-0.0462169 0.00171166
-0.0340213 0.0142962
-0.0109774 0.00422369
0.0716773 0.00187261
0.0865812 0.0584975
0.135017 0.0566479
0.00856342 0.00173705
0.0676335 0.0544924
-0.0248069 0.0160509
0.0734322 0.011193
0.0626054 0.0512549
0.0707072 0.004899
0.0594349 0.053708
0.124311 0.049919
-0.0494953 0.00793072
-0.0272995 0.00768217
-0.00735889 0.0527114
0.0152591 0.00634629
1.74905e-07 0.0031968
0.0661223 0.0555994
0.0937752 0.0596824
0.000834798 0.00318834
0.0594397 0.0571161
0.00378707 0.00344897
0.0857794 0.0546224
-0.00735889 0.0467269
0.0593133 0.0542393
0.0131845 0.00426236
0.0594396 0.0567285
0.0594376 0.0521741
0.0310444 0.0528262
0.022444 0.0513223
-0.0131855 0.00160414
0.034034 0.00621651
0.0271109 0.00353541
0.0813321 0.0532289
-0.0707068 0.00519998
0.0594397 0.059579
0.0261394 0.0527409
-0.0385424 0.00503844
0.0983621 0.0523202
0.102035 0.0542049
0.0396776 0.052511
0.0594397 0.0593356
0.046218 0.0108034
-0.014485 0.00372426
0.0594396 0.0598813
-0.0776845 0.0133618
0.0122504 0.0534403
0.0224432 0.0530194
-0.00628221 0.0464107
-0.0152556 0.00666916
0.0857794 0.0543379
0.0496683 0.0601101
0.0496683 0.058021
0.0412776 0.00652252
-0.0716686 0.00505884
0.0368959 0.0538357
0.13219 0.0637555
0.0529666 0.00176241
-0.0210896 0.00396599
0.0971047 0.0569095
0.0677309 0.0535801
0.0337182 0.0562776
0.0310444 0.0530329
parameters: [ 9.449  2.195  2.382  0.582  5.674]. error: 451628556.512.
----------------------------
epoch 0, loss 1.13414
epoch 128, loss 0.672838
epoch 256, loss 0.692177
epoch 384, loss 0.745103
epoch 512, loss 0.647101
epoch 640, loss 0.847147
epoch 768, loss 0.689361
epoch 896, loss 0.577943
epoch 1024, loss 0.771299
epoch 1152, loss 0.59428
epoch 1280, loss 0.757199
epoch 1408, loss 0.687298
epoch 1536, loss 0.859105
epoch 1664, loss 0.473899
epoch 1792, loss 0.721241
epoch 1920, loss 0.947571
epoch 2048, loss 0.828447
epoch 2176, loss 0.721436
epoch 2304, loss 0.810265
epoch 2432, loss 0.665595
epoch 2560, loss 0.661426
epoch 2688, loss 0.653085
epoch 2816, loss 0.795666
epoch 2944, loss 0.771115
epoch 3072, loss 0.701374
epoch 3200, loss 0.671361
epoch 3328, loss 0.803602
epoch 3456, loss 0.536956
epoch 3584, loss 0.663612
epoch 3712, loss 0.958801
epoch 3840, loss 0.546748
epoch 3968, loss 1.01419
epoch 4096, loss 0.750452
epoch 4224, loss 0.676185
epoch 4352, loss 0.932683
epoch 4480, loss 0.735971
epoch 4608, loss 0.780837
epoch 4736, loss 0.656086
epoch 4864, loss 0.520973
epoch 4992, loss 0.894016
epoch 5120, loss 0.605048
epoch 5248, loss 0.842741
epoch 5376, loss 1.0459
epoch 5504, loss 0.749972
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0941706 0.0552459
0.080392 0.058333
0.0594397 0.0571809
0.0631163 -0.00103281
-0.0210947 0.00152644
0.0937752 0.060049
0.0855721 0.0614099
0.0594116 0.0535326
0.0707072 0.022281
0.0093143 0.0541268
0.0910569 0.0571653
0.0329021 0.0611725
-0.129457 -0.00296442
0.0910569 0.055234
0.081527 0.0598515
0.106634 0.059179
-0.000795203 0.00489474
0.0691068 0.0569134
0.0300726 -0.00037068
0.00863029 0.0505028
0.059181 0.0519552
0.0594162 0.0531381
-0.0340329 0.0140998
-0.0495734 0.00379182
-0.0144843 0.00399125
0.00131924 -0.00306666
0.0594494 0.0655438
0.0730674 0.0601396
0.0991788 0.0582334
0.0122352 0.0535154
0.0868848 0.0514485
0.0234681 0.000688263
0.0317696 0.0537011
0.0662934 -0.00049965
0.0327313 0.0532033
0.0397476 0.0559107
0.0106903 0.00148227
-0.00968441 0.0546962
0.0605996 0.0526241
-0.0362226 0.00666929
-0.0462049 0.0141347
-5.97989e-06 0.00163006
-0.0631232 -0.00469659
0.0207331 -0.0014061
0.0188016 0.0621657
-0.0857226 0.00140211
0.0317699 0.056252
-0.0385421 0.0150011
0.0859467 0.0557236
0.121243 0.0572862
-0.014471 0.00557896
-0.0659495 0.00715553
0.0910569 0.0556576
0.0710835 0.0562857
0.0691985 0.0576757
0.0907408 0.0626324
0.0477957 0.0627053
0.0532535 0.0557822
0.121243 0.0589873
-0.0144843 0.0160168
0.0477959 0.0573299
0.0494968 0.00402221
0.0405236 0.0595888
0.0800819 0.0572088
-0.0631232 -0.000488519
0.0541426 0.0551001
0.0611271 0.00205602
0.0991789 0.0602695
0.0286789 0.053565
0.0878299 0.0568351
-0.0385424 0.00682369
0.0589765 0.0608394
0.0594395 0.0558574
0.0477959 0.0581642
0.0945151 0.0519915
0.0859962 0.0623434
0.059435 0.0648693
0.0594396 0.0557312
-0.0188599 0.0174875
0.0661223 0.056296
0.0594376 0.0565648
0.0582236 0.0533962
0.00378707 0.00027977
-0.0131854 0.00350114
-0.0860382 -0.00280103
0.0968257 0.0604501
0.060558 0.0531434
0.0945151 0.0519915
0.0589763 0.0601403
-0.0385424 0.000542131
0.0650733 0.0514456
0.0201879 0.0571635
0.0631163 0.00677842
0.0851612 0.0573226
0.0752321 0.058215
0.0594398 0.0564236
0.090741 0.0567639
0.0594396 0.054093
0.0390889 0.00680309
-0.000801651 0.00307671
0.0734439 0.000613128
0.044975 0.0606622
0.0730674 0.0570943
0.0707072 0.00737385
0.00894166 0.00533415
0.121243 0.0573882
0.0170933 0.0619322
0.133575 0.0537546
0.0867119 0.0582561
0.0594397 0.0594011
0.0589765 0.0604213
0.0461383 0.060741
0.021525 0.00376647
0.0346353 -0.00353881
0.0865811 0.058163
0.0941706 0.0561801
0.0192074 0.0564762
0.0122352 0.0558755
-0.0177194 -0.00131396
0.0412752 0.00695378
2.5668e-06 0.00627184
-0.0631232 -0.000197279
0.0417583 -0.00133711
0.0851612 0.0619603
0.0174335 0.0555062
-0.0131855 0.00554635
0.0764272 0.057066
0.0920961 0.0562227
parameters: [ 9.449  2.195  2.382  0.582  5.579]. error: 2969566.43701.
----------------------------
epoch 0, loss 1.15157
epoch 128, loss 0.815844
epoch 256, loss 0.876838
epoch 384, loss 1.05411
epoch 512, loss 0.732674
epoch 640, loss 0.750204
epoch 768, loss 1.08475
epoch 896, loss 0.853223
epoch 1024, loss 0.741788
epoch 1152, loss 0.69884
epoch 1280, loss 0.781281
epoch 1408, loss 0.996873
epoch 1536, loss 0.756543
epoch 1664, loss 0.518745
epoch 1792, loss 0.886257
epoch 1920, loss 0.55283
epoch 2048, loss 0.513271
epoch 2176, loss 0.729648
epoch 2304, loss 0.550783
epoch 2432, loss 0.845582
epoch 2560, loss 0.589903
epoch 2688, loss 1.07306
epoch 2816, loss 1.01793
epoch 2944, loss 0.586005
epoch 3072, loss 0.681269
epoch 3200, loss 0.456985
epoch 3328, loss 0.841522
epoch 3456, loss 0.66448
epoch 3584, loss 0.72984
epoch 3712, loss 0.809941
epoch 3840, loss 0.854163
epoch 3968, loss 0.502364
epoch 4096, loss 0.807169
epoch 4224, loss 0.605165
epoch 4352, loss 0.659186
epoch 4480, loss 0.66478
epoch 4608, loss 0.581594
epoch 4736, loss 0.800763
epoch 4864, loss 0.478904
epoch 4992, loss 0.674709
epoch 5120, loss 0.886826
epoch 5248, loss 0.780251
epoch 5376, loss 0.849661
epoch 5504, loss 0.782885
epoch 5632, loss 0.786719
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0675205 0.0429541
-0.0412685 0.00995776
0.0594399 0.0493603
-0.0326437 -0.00168335
-0.0412685 0.0125561
0.0997468 0.0555605
0.0495801 0.00526581
0.0691985 0.0548984
0.0781223 0.0504801
3.36628e-05 0.0147865
0.0807993 0.0191949
0.0611271 -0.00106672
0.0920508 0.049167
-0.0189824 0.00831191
0.0710837 0.0560131
-0.0272977 0.00651076
-0.063123 -0.000776704
1.01848e-06 4.06251e-05
0.0495801 0.00420701
0.0866799 0.0523918
0.0117847 0.0050803
0.0217931 0.0576384
0.057604 0.0568403
0.0594562 0.0474487
3.5013e-07 0.0100298
0.0724741 0.0489987
0.0188016 0.0514445
-0.0716686 0.00764025
0.0661223 0.0530827
0.0901948 0.0511478
0.0327313 0.0536398
0.0527576 0.0533614
0.0204028 0.0175835
0.0109517 0.00179166
0.0106905 0.00678476
0.05937 0.0435877
0.0261395 0.0503258
0.080392 0.0520041
0.0594397 0.0586991
0.0691987 0.0545392
0.0188597 0.0178036
-0.000835392 0.00283185
-0.00236347 0.0570402
-0.0390898 -0.00116952
6.39156e-06 -0.00179576
0.0594297 0.0502614
-0.0207367 0.00165732
0.0541426 0.0447262
1.13647e-06 0.000167292
-0.0412731 0.00789345
0.0867119 0.0541918
0.0424316 0.0514809
0.0594449 0.0487132
0.01684 0.0517791
-0.0271152 0.00752953
0.0860428 -0.000210623
0.0851511 0.0448415
0.0593758 0.0489744
-0.0529659 0.000312147
-0.0295559 0.00215155
0.023648 0.048537
0.0857789 0.0529413
0.0907408 0.0563832
0.0594397 0.0548898
0.0861486 0.0558832
0.0920969 0.0504793
0.0945153 0.0499951
0.0267781 0.0470564
-0.0532547 0.000388891
0.0861488 0.0541494
0.0882822 0.0523913
-0.0390898 0.00613834
0.0907408 0.0541494
0.0659505 0.00290374
0.041765 0.0125828
0.0295687 0.000310254
0.125863 0.0538731
0.0492474 -0.00270337
-0.0707055 0.00208891
0.0482639 0.0541857
0.0362233 0.00303442
-0.0662843 0.0048751
0.0330542 0.0427372
-0.0215356 0.0154895
0.0494968 8.95717e-05
-0.000835392 0.00879778
0.0496681 0.0567339
0.0236481 0.0568248
0.0599031 0.0545878
-0.0234699 0.00783254
0.0783558 0.0548371
0.0730676 0.0564031
0.0375381 0.048272
0.0414051 0.00263417
-0.0495737 -0.00215574
0.0983621 0.0490548
0.0589765 0.0555959
0.0319945 0.0521939
0.0290868 0.0524484
0.0582236 0.0457644
-0.00728242 0.045433
-0.0210896 0.00152218
0.032902 0.0537431
0.0964307 0.0528101
0.130756 0.0564844
0.0538015 0.0488007
0.0594397 0.0557562
0.0604514 0.0445487
2.9023e-05 -8.016e-05
1.95996e-07 0.00928669
0.0594399 0.0496586
0.090741 0.049209
0.0495796 -0.00120303
0.0964766 0.049807
0.0716781 -0.00171562
0.129916 0.0485531
0.0414075 0.00832678
0.000786701 0.0108931
-0.0188605 0.0110832
0.022444 0.0533221
0.0724734 0.0519795
-0.0234699 0.00882802
-0.053257 0.0101938
0.0449618 0.00161634
0.0439209 0.0510815
0.0593312 0.0500092
0.0424428 0.0505105
0.0594397 0.0547854
parameters: [ 9.449  2.195  2.382  0.582  5.639]. error: 4886984080.01.
----------------------------
epoch 0, loss 1.00508
epoch 128, loss 0.840762
epoch 256, loss 0.865168
epoch 384, loss 1.06624
epoch 512, loss 0.732902
epoch 640, loss 0.662888
epoch 768, loss 0.626661
epoch 896, loss 0.880079
epoch 1024, loss 0.790762
epoch 1152, loss 0.671097
epoch 1280, loss 0.426267
epoch 1408, loss 0.832314
epoch 1536, loss 0.543216
epoch 1664, loss 0.378441
epoch 1792, loss 0.683545
epoch 1920, loss 0.783055
epoch 2048, loss 0.68802
epoch 2176, loss 0.651505
epoch 2304, loss 0.702292
epoch 2432, loss 0.745495
epoch 2560, loss 0.755105
epoch 2688, loss 0.738831
epoch 2816, loss 0.533304
epoch 2944, loss 0.791395
epoch 3072, loss 0.667408
epoch 3200, loss 0.594674
epoch 3328, loss 0.703007
epoch 3456, loss 0.841296
epoch 3584, loss 0.534929
epoch 3712, loss 0.598488
epoch 3840, loss 0.779354
epoch 3968, loss 0.750223
epoch 4096, loss 0.696823
epoch 4224, loss 0.765713
epoch 4352, loss 0.645146
epoch 4480, loss 0.833176
epoch 4608, loss 0.513762
epoch 4736, loss 1.11096
epoch 4864, loss 0.737186
epoch 4992, loss 0.6483
epoch 5120, loss 0.501449
epoch 5248, loss 0.765671
epoch 5376, loss 0.732047
epoch 5504, loss 0.684323
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.00728242 0.0519607
0.032933 0.0571607
-0.0188605 0.0121746
0.0911183 0.0574857
-0.000805986 -0.000708311
-0.0414059 0.00379199
0.0271187 0.00679709
0.049499 0.016855
0.0907408 0.0596598
0.0819546 0.0566141
0.0268143 0.0539571
0.0594273 0.0538013
0.0207331 0.00135171
0.125146 0.0501644
-0.0716682 0.00745578
0.0329332 0.0610485
-0.0271157 -0.00600329
0.0477959 0.0549078
0.0327313 0.0599012
-0.0495799 -0.00259879
0.0752321 0.0541066
0.05943 0.0524835
0.0593346 0.0557735
-0.0385437 0.00846274
0.0464005 0.0500256
0.0267779 0.0565186
0.0188016 0.0579069
-0.0412731 0.00493114
0.0676335 0.0557283
0.0594273 0.0572235
0.0278228 0.0580664
0.0305923 0.0506816
0.0375434 0.0547371
0.0594398 0.0580691
0.0268142 0.0490925
0.0727412 0.0542682
0.0707066 0.0200987
4.17498e-06 0.00568277
-0.00894125 0.00210136
-0.0385421 0.00381446
0.0764328 0.0539937
0.0394799 0.00307176
-0.0662867 -0.00221131
0.106619 0.0569195
0.0201879 0.050713
0.0477956 0.0617891
-0.00701261 0.0464007
0.0724734 0.0536254
0.0860364 0.00457085
0.0861486 0.0606552
0.101814 0.059786
-0.0385434 0.00391531
0.0234638 -0.00181896
0.0865811 0.0599433
-0.0417792 -0.000410891
0.101814 0.0579818
0.0424428 0.0507481
0.0122404 0.0495694
-0.0857226 -0.00484883
-0.032317 0.00274165
0.0594324 0.0552833
0.0317699 0.0615888
0.00894003 0.000511945
-0.0346352 -0.00351634
0.0144846 0.000235409
-0.00460347 -0.000102027
0.0861488 0.0613605
0.0109586 0.00140591
0.0211182 0.0579834
0.0188018 0.0582295
-0.0532547 0.00914578
0.0950804 0.0623935
0.0199307 0.0586267
0.0964305 0.0567818
0.0133907 0.0627761
-3.17974e-05 0.00636219
-0.000835048 0.00706494
-0.0707055 0.00368083
0.132184 0.0637867
0.0222152 0.058846
0.0730676 0.0618747
0.101441 0.0547918
-0.0209708 0.0056244
0.101441 0.0570755
0.0387924 0.0514411
-0.00142797 0.00355422
0.0449618 0.00402673
0.0650733 0.055684
0.124354 0.0551181
0.046218 1.22608e-05
0.102034 0.0536259
0.0416434 0.00289972
0.0871098 0.0589324
0.0527577 0.056221
0.0594398 0.057844
0.0301505 0.0572654
0.0435639 0.0540857
0.0247806 0.0167929
0.0122352 0.050008
0.0594397 0.058527
0.076422 0.0543045
0.10665 0.057031
-1.32922e-07 0.0103285
-0.000781954 0.000563619
0.0234681 0.000174642
0.0867335 0.0606675
0.0327313 0.0614604
0.0327313 0.0612098
0.0594116 0.0560272
-0.000805986 0.00867449
-0.014471 0.00529342
-0.00701261 0.0536378
0.0589765 0.0591874
0.0199306 0.0602962
0.0243586 0.0510636
0.100097 0.0602422
0.0477958 0.0537326
0.135027 0.057639
-0.00701261 0.0502891
0.0168394 0.0558894
0.0310444 0.054087
0.0416434 0.00476835
0.0133907 0.0631942
-3.17974e-05 -0.00133107
0.0188609 0.00942677
0.038544 0.00536577
-0.0776845 0.00969854
-1.1614e-09 0.00823476
parameters: [ 9.449  2.195  2.382  0.582  5.602]. error: 9407909.66897.
----------------------------
epoch 0, loss 3.54839
epoch 128, loss 0.958461
epoch 256, loss 1.45603
epoch 384, loss 2.75349
epoch 512, loss 2.24329
epoch 640, loss 0.682637
epoch 768, loss 0.896304
epoch 896, loss 0.306655
epoch 1024, loss 1.07123
epoch 1152, loss 0.592754
epoch 1280, loss 0.193152
epoch 1408, loss 2.0603
epoch 1536, loss 1.08369
epoch 1664, loss 0.935035
epoch 1792, loss 0.229224
epoch 1920, loss 0.174074
epoch 2048, loss 0.175647
epoch 2176, loss 1.00463
epoch 2304, loss 0.980273
epoch 2432, loss 0.943361
epoch 2560, loss 1.47916
epoch 2688, loss 2.20348
epoch 2816, loss 2.19865
epoch 2944, loss 3.08897
epoch 3072, loss 1.84261
epoch 3200, loss 0.462653
epoch 3328, loss 0.359915
epoch 3456, loss 0.559498
epoch 3584, loss 0.784574
epoch 3712, loss 1.31327
epoch 3840, loss 0.0846561
epoch 3968, loss 0.285317
epoch 4096, loss 1.9246
epoch 4224, loss 0.398454
epoch 4352, loss 0.787528
epoch 4480, loss 0.275121
epoch 4608, loss 0.0377707
epoch 4736, loss 0.0511817
epoch 4864, loss 0.878506
epoch 4992, loss 0.671031
epoch 5120, loss 0.93857
epoch 5248, loss 1.58458
epoch 5376, loss 0.388661
epoch 5504, loss 0.117705
epoch 5632, loss 1.21012
epoch 5760, loss 1.29071
epoch 5888, loss 1.93701
epoch 6016, loss 0.514192
epoch 6144, loss 1.6758
epoch 6272, loss 0.585787
epoch 6400, loss 0.6853
epoch 6528, loss 1.53224
epoch 6656, loss 0.0343695
epoch 6784, loss 1.20282
epoch 6912, loss 1.16497
epoch 7040, loss 1.37534
epoch 7168, loss 0.603255
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0267779 0.0564051
0.0210212 0.0696072
-0.10011 0.0238065
0.0113515 0.0282763
-0.0807986 0.025404
0.0177216 0.0424795
0.0310444 0.0517351
0.0305923 0.0579045
0.118684 0.0705191
-7.68032e-07 0.0219519
0.109566 0.0547901
0.0594473 0.0685543
-0.0462049 0.0285802
0.0267781 0.0260618
0.0278229 0.0293119
0.0621334 0.0265793
0.0458117 0.0308789
0.0594349 0.0601891
-0.0207292 0.045893
-0.0346416 0.0275533
0.0851609 0.0477358
0.0210209 0.0644783
0.0776735 0.0367092
0.032933 0.0387673
-0.0210896 0.0391721
0.00932522 0.0361701
0.0867119 0.0663705
0.0562648 0.0648193
0.0818743 0.0555262
0.0271187 0.0407048
0.0776855 0.0341526
0.0211182 0.0726432
0.0496683 0.037327
-0.0204122 0.0369694
-0.10012 0.0272116
0.00894166 0.0413696
0.0907409 0.039765
0.0594396 0.0312794
0.0813268 0.0654278
0.0677309 0.0554965
0.0594204 0.0738018
0.0237988 0.0464982
3.36628e-05 0.0417816
0.0170933 0.0639443
0.0211031 0.0323751
0.0199306 0.074803
-0.129457 0.0683267
0.0977802 0.0692082
0.0594325 0.0700612
0.0952317 0.0574252
0.0224432 0.0628138
-0.0860447 0.0268227
0.000213351 0.0699189
0.0882823 0.0381332
0.0594396 0.0408306
0.0140549 0.0461393
0.0496681 0.0368785
0.00596587 0.0699933
0.0596272 0.0609624
0.0449639 0.0301422
0.0594399 0.0279064
0.0861488 0.029841
0.0435741 0.0644362
0.0321897 0.0714836
0.09274 0.0462255
0.0248197 0.0205537
-0.000835377 0.0443219
0.0643397 0.0610749
0.0496681 0.0559995
-0.0860366 0.0510002
0.0706155 0.0374926
0.019701 0.0334416
0.0532535 0.061868
-0.0207367 0.0440588
-0.0417422 0.036732
0.0950802 0.0362906
-0.0495737 0.0435291
0.0952315 0.0439509
0.0791268 0.0576688
-0.0707068 0.0405545
-0.0385421 0.0279745
0.0662934 0.0256969
0.0927397 0.0474945
0.0594379 0.0713795
0.0562851 0.0436291
0.12432 0.0667748
3.40452e-06 0.0394103
0.0461382 0.0523509
-0.0621274 0.0248837
0.0594273 0.0670189
0.0131851 0.0398814
0.0594349 0.064969
0.100115 0.0232153
0.059435 0.0515032
0.0234754 0.0273349
0.0594399 0.0655381
-0.0152533 0.0428828
0.0204119 0.0354657
0.0267419 0.0300755
0.0323166 0.0416236
0.0594397 0.0491682
0.0730676 0.024557
0.0220537 0.0299087
-0.0394796 0.0561947
-0.0111732 0.072466
0.106619 0.0438523
0.0594399 0.0655381
-0.0492483 0.029731
-0.0161248 0.0581366
0.0594426 0.0664122
-0.0271111 0.0211382
0.118684 0.0690331
0.0621302 0.0255002
0.0661223 0.0315663
-0.00249422 0.0355135
0.0859466 0.0403679
-0.0385424 0.0206329
0.0604514 0.0573677
-0.0177216 0.0358603
0.0997462 0.0626473
0.0093143 0.052258
0.0710838 0.0542789
0.0267781 0.0504732
0.0753057 0.0703092
0.0168394 0.0507903
0.0414075 0.026521
0.0783558 0.0529579
0.0477959 0.0335056
parameters: [ 9.898  3.39   2.764  0.036  7.236]. error: 144173120.206.
----------------------------
epoch 0, loss 1.31352
epoch 128, loss 1.0849
epoch 256, loss 0.833729
epoch 384, loss 0.889963
epoch 512, loss 0.959239
epoch 640, loss 1.37161
epoch 768, loss 0.781528
epoch 896, loss 0.75353
epoch 1024, loss 0.927179
epoch 1152, loss 0.620918
epoch 1280, loss 0.932287
epoch 1408, loss 0.551823
epoch 1536, loss 0.614878
epoch 1664, loss 0.836364
epoch 1792, loss 0.582104
epoch 1920, loss 0.867052
epoch 2048, loss 0.877442
epoch 2176, loss 0.993614
epoch 2304, loss 0.537471
epoch 2432, loss 1.15831
epoch 2560, loss 0.617542
epoch 2688, loss 0.767155
epoch 2816, loss 0.749479
epoch 2944, loss 0.469029
epoch 3072, loss 0.699614
epoch 3200, loss 0.802633
epoch 3328, loss 0.80613
epoch 3456, loss 0.597521
epoch 3584, loss 0.535772
epoch 3712, loss 0.716513
epoch 3840, loss 0.649997
epoch 3968, loss 0.738046
epoch 4096, loss 0.78246
epoch 4224, loss 0.724147
epoch 4352, loss 0.63667
epoch 4480, loss 0.928263
epoch 4608, loss 0.603259
epoch 4736, loss 0.626779
epoch 4864, loss 0.722761
epoch 4992, loss 0.764346
epoch 5120, loss 0.620965
epoch 5248, loss 0.849101
epoch 5376, loss 0.873108
epoch 5504, loss 0.715915
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0867335 0.0591101
0.0144707 0.0116033
0.109566 0.0594387
-3.40966e-05 -0.000252008
0.0661222 0.0573185
-0.0860382 0.00249028
0.0267369 0.00237262
0.126171 0.0549491
0.0330542 0.0514179
0.0941706 0.0549377
-0.0412711 0.0018746
0.0417583 0.00242134
0.0866747 0.0546328
-3.91322e-05 0.0015459
0.0764428 0.056265
0.059435 0.0531951
-0.00700272 0.0567471
-0.0117788 -0.000665552
-0.0468045 0.0102739
0.0527575 0.0559508
-0.129457 0.0070398
0.0911183 0.0569552
0.00863092 0.055599
0.0337178 0.0598554
0.088729 0.0588945
-0.0161248 0.0536828
0.0509553 0.0506342
0.0495801 0.00702864
0.0594273 0.052705
-0.0189864 0.00822863
0.0385431 0.00333736
0.081527 0.0558299
-0.0631101 0.00349725
0.00931417 0.0578102
0.130756 0.0581351
0.0594396 0.0577025
-0.014471 0.00393663
0.0989675 0.0586623
0.0859465 0.0595061
0.0189868 0.00253113
0.0220538 0.0556977
0.0859865 0.0585853
0.0691986 0.058124
0.0818743 0.0519595
0.00894166 0.00370642
0.0907409 0.0582483
-0.0248182 0.0140427
-0.0662877 -0.00554611
0.0803921 0.0575485
0.0188023 -0.00143027
0.033718 0.05656
-0.0210896 -0.000910352
-0.00737805 0.0522374
0.0412776 -0.00302545
0.0318693 0.0537249
0.0594325 0.0553387
0.05937 0.0527442
-0.0204028 0.00945141
-0.0271152 -0.00612673
-0.0189848 0.0141592
0.0236482 0.0581346
0.0133878 0.0623878
0.0390889 -0.000664874
0.0208321 0.0118497
0.0511643 0.0532529
-0.0707068 -0.00297421
0.0223968 0.0521425
-0.00236373 0.0594367
0.046799 0.0107319
-0.0414014 0.0118062
0.0449618 -0.00118394
0.0529729 -0.000875267
0.0317699 0.0585765
-0.0385434 0.00337241
0.0562851 0.0566752
0.0594424 0.0534514
0.0661222 0.0573185
0.0113418 0.00464728
0.0281391 0.0563193
0.0907409 0.0593681
0.0496681 0.0570363
-0.0494953 0.0149842
-0.0417422 0.000381552
0.0220537 0.058161
0.0346348 0.000456543
0.126222 0.0534377
0.0177241 -0.000201265
-0.000835048 0.00215949
0.026809 0.055758
0.0594427 0.0553052
-0.0417792 0.000915919
0.0997462 0.0552748
0.0131855 0.00220881
0.0887289 0.0581859
0.0529729 0.00563577
0.0594398 0.0591094
0.0268143 0.0542184
0.0621334 -0.00682679
0.093775 0.0593652
0.081527 0.0548645
0.0691985 0.0577102
0.01684 0.0558371
-0.0177216 -0.00414931
0.0964766 0.0551494
0.0589133 0.0545679
0.100667 0.0565212
0.0286789 0.0552008
0.0188018 0.0599949
0.0859466 0.057068
0.0538177 0.0517339
-0.0194897 0.0144641
0.0417448 -0.00238556
0.0871098 0.0582353
0.0174339 0.0514124
0.0424215 0.0569962
0.0201879 0.0532243
0.0532677 0.00679883
0.0624794 0.0521201
0.01684 0.0535135
0.0952315 0.0566258
0.0394799 0.0033739
0.0414051 0.014077
0.0822309 0.0524126
0.0752422 0.0533095
0.00459996 0.003795
0.0594397 0.060029
0.0188023 0.00484825
0.0329331 0.0577271
parameters: [ 9.449  2.195  2.382  0.582  5.618]. error: 502.761305609.
----------------------------
epoch 0, loss 0.691084
epoch 128, loss 1.61521
epoch 256, loss 1.9033
epoch 384, loss 1.59797
epoch 512, loss 1.20048
epoch 640, loss 0.900491
epoch 768, loss 0.0548122
epoch 896, loss 0.343752
epoch 1024, loss 0.671571
epoch 1152, loss 2.7507
epoch 1280, loss 2.04079
epoch 1408, loss 0.0617191
epoch 1536, loss 0.305377
epoch 1664, loss 1.29365
epoch 1792, loss 0.528415
epoch 1920, loss 2.39914
epoch 2048, loss 1.10228
epoch 2176, loss 0.495823
epoch 2304, loss 1.94002
epoch 2432, loss 0.478325
epoch 2560, loss 0.304936
epoch 2688, loss 1.50966
epoch 2816, loss 2.14652
epoch 2944, loss 0.448558
epoch 3072, loss 0.7729
epoch 3200, loss 2.49955
epoch 3328, loss 1.45549
epoch 3456, loss 0.903918
epoch 3584, loss 1.4784
epoch 3712, loss 0.526559
epoch 3840, loss 0.382695
epoch 3968, loss 0.907175
epoch 4096, loss 0.845311
epoch 4224, loss 0.968055
epoch 4352, loss 0.549912
epoch 4480, loss 0.70913
epoch 4608, loss 0.129535
epoch 4736, loss 0.316544
epoch 4864, loss 0.505451
epoch 4992, loss 0.163731
epoch 5120, loss 0.365955
epoch 5248, loss 0.0436213
epoch 5376, loss 0.708234
epoch 5504, loss 2.13913
epoch 5632, loss 1.58871
epoch 5760, loss 1.26774
epoch 5888, loss 0.181693
epoch 6016, loss 0.137551
epoch 6144, loss 0.368794
epoch 6272, loss 0.0965362
epoch 6400, loss 0.697795
epoch 6528, loss 2.41476
epoch 6656, loss 1.33786
epoch 6784, loss 0.0813963
epoch 6912, loss 0.234119
epoch 7040, loss 0.98977
epoch 7168, loss 1.12257
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0707062 0.0199905
-0.00737805 0.0199328
-0.0132848 0.0643096
0.0188088 0.0210494
0.0595095 0.0489697
-0.0716686 0.02038
0.0631163 0.0898273
-0.0131858 0.0204692
0.0319947 0.0193568
-0.0272904 0.022324
-0.0827278 0.0193317
0.0385437 0.0862724
0.0594449 0.0756907
0.0594398 0.019876
0.0861488 0.043126
-0.0113495 0.0193874
-0.0267379 0.0206014
0.0659505 0.0190444
0.100668 0.100158
0.0248097 0.0777316
-0.00236343 0.0199761
-3.40966e-05 0.0195486
0.0800821 0.0198075
0.12614 0.0195204
0.0152545 0.0196641
0.0439209 0.090948
-0.0118764 0.109682
0.0217933 0.0190184
0.101814 0.0330731
0.0317696 0.0585241
0.0625949 0.01966
-0.0210947 0.0267025
0.0384777 0.0195652
0.0117847 0.0243417
1.95996e-07 0.019885
0.0329332 0.0645056
0.09274 0.0421463
0.0508582 0.019255
0.0318766 0.053292
0.0346447 0.0194089
0.0781227 0.0193088
0.0871097 0.0199074
0.0950803 0.023299
0.059407 0.0193394
0.0594396 0.0623882
0.00142251 0.0423958
0.0482639 0.078059
-0.0390966 0.0374676
0.0611271 0.0190365
0.0927396 0.0221043
-0.0111692 0.0193554
0.121243 0.0797267
0.0594397 0.0621515
0.0217933 0.0551698
0.0907408 0.0186975
0.0496682 0.0834808
-0.0207342 0.0199524
0.0855721 0.0197319
0.0170933 0.0203662
6.39156e-06 0.0196378
0.0133907 0.0197792
0.034034 0.0194118
4.57767e-08 0.0252789
0.0188597 0.0220656
0.129458 0.019469
0.0599612 0.0196501
0.0676335 0.0189542
0.0966595 0.0195486
0.0964772 0.0198285
0.0373484 0.0197689
0.0706156 0.0394436
0.0594481 0.0190215
-0.0207292 0.0508071
0.0901952 0.048369
0.00131924 0.0194475
0.0594399 0.0207811
0.0330949 0.0192938
-0.0215238 0.0194327
0.00856342 0.0239041
0.0594398 0.0950494
0.0327313 0.0429344
0.0791969 0.0764222
0.0310444 0.0258728
0.0436381 0.01967
0.0716781 0.0885415
0.130757 0.0938409
0.0710837 0.0186892
0.0194912 0.0921793
0.0384676 0.0195642
-0.0117817 0.0197438
0.0496682 0.0949983
-0.0111732 0.0248706
0.0396777 0.0808547
0.0365698 0.0207705
0.0424529 0.0189213
0.0223972 0.0195041
0.0211031 0.0984796
0.0122504 0.0478265
0.092056 0.0190661
0.00250001 0.019219
0.109566 0.0194388
0.0582589 0.0191691
0.0495801 0.0194549
0.0310444 0.0529487
-0.027288 0.0203128
0.0458117 0.0194058
-0.0860366 0.0348171
0.0594349 0.0223424
0.0412776 0.0197717
-3.17974e-05 0.0251047
0.043628 0.019675
0.0278229 0.0218972
0.0594397 0.0195129
0.0182076 0.100094
0.0467944 0.0794138
0.0211031 0.0233957
-0.00968441 0.019612
0.0594397 0.0954905
0.0594427 0.0193037
-0.0414033 0.0195637
0.0589765 0.0520806
-0.000781954 0.0192448
0.0968256 0.0755016
0.0907408 0.0672918
-0.0248165 0.0990444
0.0859962 0.0482953
0.0964766 0.0829713
0.0310444 0.02952
parameters: [ 9.898  3.39   2.764  0.036  7.236]. error: 2.26611490105e+12.
----------------------------
epoch 0, loss 1.45615
epoch 128, loss 0.687883
epoch 256, loss 0.623838
epoch 384, loss 0.694633
epoch 512, loss 0.659417
epoch 640, loss 0.748474
epoch 768, loss 0.724925
epoch 896, loss 0.732758
epoch 1024, loss 0.655259
epoch 1152, loss 0.608297
epoch 1280, loss 0.584725
epoch 1408, loss 0.733595
epoch 1536, loss 0.605373
epoch 1664, loss 0.621591
epoch 1792, loss 0.582646
epoch 1920, loss 0.529106
epoch 2048, loss 0.588807
epoch 2176, loss 0.54364
epoch 2304, loss 0.522691
epoch 2432, loss 0.764526
epoch 2560, loss 0.674694
epoch 2688, loss 0.53791
epoch 2816, loss 0.639656
epoch 2944, loss 0.497443
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.000782366 0.00563093
0.0815268 0.0564132
0.0749536 0.0420969
0.0321467 0.0423791
0.110244 0.0634619
0.0853255 0.0451503
0.0983621 0.0711765
-0.0161572 0.0340107
-0.0111821 0.0554459
0.0496681 0.0633667
0.106634 0.0649837
0.0594325 0.04364
0.0261394 0.0332353
0.0594396 0.0550872
0.0188088 0.000226905
0.126222 0.056417
0.0706156 0.0579241
0.0776855 0.00866808
0.0327312 0.0492891
0.100116 0.0019067
0.074953 0.0494755
0.0247038 0.0521011
0.0910569 0.0524054
0.0594251 0.0455033
-0.00378521 -0.00518127
0.0482639 0.0551306
-0.0131841 0.00255453
0.121243 0.0753656
0.0317697 0.0575077
0.042448 0.0551001
0.044975 0.0550304
0.0482639 0.0675865
-0.0346416 -0.00744244
-0.0621343 -0.000240426
-0.000807624 -0.00697141
0.0803921 0.0490274
0.0458117 0.0492926
0.000213351 0.0640363
-0.00236368 0.0388221
0.0594399 0.0536744
0.0643397 0.0598438
-0.00891927 0.00654544
0.0594349 0.0592433
0.0589765 0.0451117
-0.0109842 0.000575927
0.0223972 0.0459351
0.0192074 0.0502926
0.0594449 0.0663314
0.0594398 0.0443517
0.0394802 -0.00336922
1.01848e-06 0.00253445
0.0188603 0.00192757
0.000782366 -4.02807e-05
0.132184 0.0847774
0.0594397 0.0566832
0.0865812 0.0623421
0.0461384 0.0448465
0.0659615 -0.0031143
-0.0152583 -0.00443438
-0.0611267 -0.00342212
0.0224432 0.0488527
0.000782366 0.0130494
0.0362234 0.00537321
-0.0204028 -0.0112106
0.0207377 -0.00639805
0.0106903 0.000979529
0.0989677 0.0588026
-0.0188062 0.000491057
-0.0271121 0.000153304
0.0950804 0.0616742
-0.0495734 -0.00735238
0.0857788 0.0567058
0.0496681 0.0690407
0.101814 0.0597533
0.0461385 0.0502399
-0.0611171 0.00560171
0.0278229 0.0498214
0.00894003 0.00825119
0.0301504 0.0521646
0.044975 0.0617025
0.118684 0.0620476
0.110244 0.0610795
0.0964305 0.0597136
0.00863029 0.0442355
0.0677309 0.0545826
3.5013e-07 0.00228864
0.0321468 0.0672497
-0.0272904 -0.00177465
-0.0215356 -0.00682972
0.0739227 0.0531156
-0.0532547 -0.00591019
0.0594398 0.0571502
-0.0631232 -0.00553537
-1.92327e-05 0.0171829
0.0859467 0.0707749
0.0727409 0.0670181
0.0327314 0.0479921
0.0727409 0.0692293
0.0594116 0.0467578
0.01684 0.0356579
-3.76332e-06 0.00562371
0.0417583 0.000623607
0.034034 -0.00416111
0.0191276 0.0319874
0.0131855 -0.00718575
0.0859467 0.0727753
0.0464004 0.0442528
-1.48505e-07 0.00588386
0.0497672 0.045065
0.0248197 -0.00588769
0.0281391 0.0409115
0.0594452 0.0599812
0.0977799 0.0673885
-3.91322e-05 -0.00670726
-0.0118764 0.0365549
0.027823 0.0602535
-0.0204101 0.0100571
0.0210915 -0.000627524
0.0871096 0.0752528
0.0495796 0.00196497
-0.0188599 -0.000381771
0.0927399 0.0502597
0.0859962 0.0694733
0.0594399 0.0542077
0.028139 0.0444151
0.0271289 0.00738326
0.135017 0.0619389
0.0857795 0.0568132
parameters: [ 8.722  0.262  1.764  1.582  3.   ]. error: 34554397.1669.
----------------------------
epoch 0, loss 1.35219
epoch 128, loss 0.785917
epoch 256, loss 1.13443
epoch 384, loss 0.832574
epoch 512, loss 0.727594
epoch 640, loss 0.596509
epoch 768, loss 0.714622
epoch 896, loss 0.600581
epoch 1024, loss 0.739613
epoch 1152, loss 0.734863
epoch 1280, loss 0.62063
epoch 1408, loss 0.4909
epoch 1536, loss 0.633704
epoch 1664, loss 0.643367
epoch 1792, loss 0.579033
epoch 1920, loss 0.679411
epoch 2048, loss 0.658957
epoch 2176, loss 0.722585
epoch 2304, loss 0.795846
epoch 2432, loss 0.871396
epoch 2560, loss 0.720189
epoch 2688, loss 0.707154
epoch 2816, loss 0.787843
epoch 2944, loss 0.905593
epoch 3072, loss 0.600389
epoch 3200, loss 0.587291
epoch 3328, loss 0.735174
epoch 3456, loss 0.718565
epoch 3584, loss 0.666639
epoch 3712, loss 0.690818
epoch 3840, loss 0.837951
epoch 3968, loss 0.660515
epoch 4096, loss 0.722299
epoch 4224, loss 0.972102
epoch 4352, loss 0.516055
epoch 4480, loss 0.595211
epoch 4608, loss 0.828194
epoch 4736, loss 0.539426
epoch 4864, loss 0.529675
epoch 4992, loss 0.849574
epoch 5120, loss 0.728971
epoch 5248, loss 0.562236
epoch 5376, loss 0.600936
epoch 5504, loss 0.925941
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0461383 0.0539058
0.0989677 0.062071
0.038792 0.0524042
0.0368964 0.0553913
0.0271109 0.00508708
0.0207193 0.0506236
-0.0250779 0.0463149
-0.039089 -0.00268798
0.0305923 0.0516931
0.0133907 0.0701959
-0.0247793 0.0103765
0.0594395 0.0572353
0.0333261 0.0604877
0.0822309 0.0531295
0.0210915 -0.000969435
0.0707072 0.00987393
0.0734322 0.0164452
-0.000795203 0.000100997
0.0271115 0.00481927
0.060558 0.0553506
0.0508582 0.0524347
0.109566 0.0525731
0.0337178 0.0570702
-0.00628221 0.0484465
0.0964307 0.0541445
0.125873 0.060016
0.0562648 0.0502152
0.0527578 0.0512219
-0.049248 0.00390537
0.090741 0.0551529
0.000782366 0.0101747
0.088282 0.0510371
-0.0118764 0.0441958
0.0594397 0.0554383
0.0752422 0.0588077
0.0261393 0.05258
0.0188597 0.00797123
-0.041769 -0.000537663
0.0706155 0.0577192
0.0753057 0.0580841
0.0182076 0.0378245
0.0861487 0.0566662
0.0594466 0.071566
-0.000793636 0.0115943
0.0911183 0.0530101
-0.10011 -0.00313677
0.085161 0.048183
0.0791961 0.0589556
0.0318766 0.00752293
0.0199307 0.0640682
-0.0529722 -0.00128888
-0.0131848 -0.000546462
-0.0340213 0.0106935
0.0477956 0.0477809
0.0800819 0.0524338
0.0871097 0.0557189
-0.00701261 0.0550508
-0.0188605 0.00823216
0.0209836 0.000631534
-0.0412731 -0.000602844
-0.0131841 0.00958544
0.0204119 -0.000935583
0.01684 0.0523098
0.0870053 0.0506125
0.0208245 0.00806495
0.0764328 0.0497822
0.0248197 -7.82042e-05
0.0329331 0.0497316
0.0317696 0.0476056
0.0977799 0.0632694
0.0278229 0.0512121
0.0220537 0.049689
8.40829e-06 0.00838121
0.0605534 0.0585443
-0.00894944 0.00774217
0.0511643 0.0579646
0.0220537 0.0610371
0.0477959 0.0618162
0.0204028 0.000377682
0.0907408 0.0533369
-0.0131848 0.00870672
0.0871098 0.0465452
0.106624 0.0553868
-0.0807974 0.0101789
-0.0416468 0.00680123
0.0417448 -0.00261621
0.121243 0.0556738
-0.0106908 -0.00261712
0.0385434 0.0111659
0.0223972 0.0436753
0.0538297 0.0560925
0.0267779 0.0518914
0.101441 0.0564844
0.0319945 0.0483119
-0.0131854 0.00938979
0.0529686 -0.00161736
-0.00628221 0.0569428
0.0346348 -0.00180327
0.0237989 0.0562868
0.0867335 0.0617079
0.0920961 0.051265
2.05834e-07 -0.000804237
0.0496681 0.0539784
0.0594027 0.0533276
0.0301504 0.0510363
0.0310444 0.0518545
0.0492512 0.00230242
0.0952318 0.0501219
0.081527 0.0567581
0.0189852 -0.00105166
-0.000801651 0.00911048
-0.0247813 0.0109433
0.0317699 0.0552817
-0.00726943 0.0595685
0.0375381 0.0554743
-0.0210969 -0.00109048
0.121243 0.0556738
-0.0248069 -0.00124149
0.0290868 0.0503266
0.0594396 0.0585934
0.132178 0.0679562
-0.00131975 0.00489672
-0.000835048 -0.000730205
-0.00131656 0.00378786
7.08095e-06 -0.00136306
-0.0161572 0.0545168
0.000796249 0.00757405
0.0867333 0.0565523
parameters: [ 9.449  2.195  2.382  0.582  5.618]. error: 314219.180219.
----------------------------
epoch 0, loss 1.12587
epoch 128, loss 0.884304
epoch 256, loss 0.860266
epoch 384, loss 0.593738
epoch 512, loss 0.828414
epoch 640, loss 0.740879
epoch 768, loss 0.685967
epoch 896, loss 0.815386
epoch 1024, loss 0.688867
epoch 1152, loss 0.667811
epoch 1280, loss 0.660285
epoch 1408, loss 0.719767
epoch 1536, loss 0.724331
epoch 1664, loss 0.763474
epoch 1792, loss 0.735754
epoch 1920, loss 1.04875
epoch 2048, loss 0.576014
epoch 2176, loss 0.582214
epoch 2304, loss 0.688367
epoch 2432, loss 0.730665
epoch 2560, loss 0.649601
epoch 2688, loss 0.867464
epoch 2816, loss 0.747385
epoch 2944, loss 0.518038
epoch 3072, loss 0.474349
epoch 3200, loss 0.895223
epoch 3328, loss 0.629645
epoch 3456, loss 0.569356
epoch 3584, loss 0.532859
epoch 3712, loss 0.633753
epoch 3840, loss 0.549843
epoch 3968, loss 0.689069
epoch 4096, loss 0.750292
epoch 4224, loss 0.912786
epoch 4352, loss 0.52647
epoch 4480, loss 0.844269
epoch 4608, loss 0.541003
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0152533 0.00879624
0.0997468 0.05113
0.00460075 -0.00307083
0.0907409 0.0600275
0.0851613 0.06074
0.0861486 0.0570667
-0.0776729 0.00137959
0.0594273 0.0531668
0.0673482 0.0515019
-0.0177216 0.000772298
0.0870053 0.0541545
0.126169 0.0508378
0.0201879 0.054635
0.0327313 0.0591704
0.0207193 0.0531636
-0.00856301 -0.000403191
0.0461385 0.0570508
-0.0271126 0.0029803
0.0435741 0.0520398
0.0631173 -0.00239077
0.0234681 0.00303486
0.0991788 0.059278
0.0337179 0.0563263
-0.0300731 -0.00504842
-0.0385421 -0.000145002
0.0589764 0.0556351
-0.0109774 0.0030922
0.0385434 0.00387195
0.0301504 0.0565816
-0.0300731 -0.00067133
0.0272988 -0.0024235
0.0739226 0.0618913
2.96104e-08 -0.00241394
0.020735 0.00476195
0.000834798 -0.000849164
0.0191276 0.0548499
0.0870053 0.0531181
0.0208321 0.008918
0.0871095 0.055737
0.0117845 -0.00405022
0.0290868 0.0573414
0.0594397 0.0588361
-2.15513e-06 0.00783038
-3.6833e-05 -0.000289618
0.00142251 0.00333526
-0.00378521 0.000743051
0.0329331 0.0595229
-0.0716682 -0.00310766
-3.11149e-07 0.00117312
-0.0189848 0.00288643
0.0594395 0.0597924
0.0323163 -0.00393645
0.129453 0.0098469
0.0310444 0.0552749
0.0861488 0.0596708
0.0596272 0.0529752
0.0197008 0.0593617
0.0281392 0.0575869
0.0208321 0.00138614
0.0868849 0.0610688
0.0594398 0.0583465
0.00856342 -0.0020326
0.081797 0.0535569
-0.0807974 0.00511637
0.0192074 0.0593263
0.0727411 0.0587359
0.0659505 -0.000872317
-0.0631101 -0.000778179
-0.0132904 0.068125
-0.0272995 -0.00160107
0.0248097 -0.000356144
0.106655 0.0540357
-0.014471 0.00288361
0.0871096 0.0585069
0.0594482 0.0684544
0.0730674 0.0560344
0.0599031 0.0567774
-2.15513e-06 0.0027921
0.0594398 0.0594944
0.0188015 0.0605682
-0.065961 -0.000783476
0.0594376 0.0550031
-0.0189848 0.00394065
0.0385291 0.00390102
0.0234638 0.00710394
0.0662801 -0.00433496
0.0952316 0.0566716
0.0237986 0.0582912
0.0168394 0.053095
0.126127 0.0530211
1.13647e-06 -0.00109295
0.0243594 0.0545541
0.0920969 0.0544356
-0.0152556 0.00341498
0.0594273 0.0531668
0.126086 0.0528454
0.0594349 0.0565512
0.0243594 0.0550431
0.125146 0.0537104
0.0589765 0.0572763
0.0217931 0.0630804
0.032902 0.060399
0.0867199 0.0601753
0.0861488 0.0565658
-0.0272904 -0.000878378
0.0301504 0.0574136
2.5668e-06 -0.000564958
0.0691986 0.0585216
0.0701141 0.0568626
-0.129457 0.00169078
0.0945151 0.052345
0.032933 0.0565759
0.0594424 0.0514034
0.0251233 0.0620386
0.0594399 0.0577045
0.0739226 0.0605361
0.038544 0.000171603
-0.00700272 0.0541566
0.0093143 0.0579182
0.0414001 -0.00034188
0.0318766 0.00342381
0.0368959 0.0516552
0.0209719 0.00141881
0.0251233 0.0608396
0.0394802 0.00277115
0.0037846 0.000911885
0.014484 0.00416128
-0.0468045 0.00347295
parameters: [ 9.172  1.456  2.146  0.964  4.618]. error: 11520.3280276.
----------------------------
epoch 0, loss 1.42431
epoch 128, loss 0.803762
epoch 256, loss 0.628893
epoch 384, loss 0.576772
epoch 512, loss 0.572581
epoch 640, loss 0.70326
epoch 768, loss 0.707184
epoch 896, loss 0.596591
epoch 1024, loss 0.663535
epoch 1152, loss 0.47936
epoch 1280, loss 0.722776
epoch 1408, loss 0.706928
epoch 1536, loss 0.538253
epoch 1664, loss 0.590061
epoch 1792, loss 0.786499
epoch 1920, loss 0.772887
epoch 2048, loss 0.517589
epoch 2176, loss 0.718539
epoch 2304, loss 0.628358
epoch 2432, loss 0.534637
epoch 2560, loss 0.650171
epoch 2688, loss 0.6163
epoch 2816, loss 0.668322
epoch 2944, loss 0.618231
epoch 3072, loss 0.42321
epoch 3200, loss 0.668945
epoch 3328, loss 0.718572
epoch 3456, loss 0.527292
epoch 3584, loss 0.593603
epoch 3712, loss 0.702825
epoch 3840, loss 0.533385
epoch 3968, loss 0.636967
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.126171 0.0532829
0.0897923 0.0612271
0.0991789 0.0690511
-0.0707062 -0.00311367
0.044975 0.0578887
0.0734322 -0.00286942
-0.0161473 0.0578303
0.0247806 0.00909665
0.0594324 0.054744
0.0310444 0.0589499
0.0461383 0.06133
0.0538177 0.0563841
0.0271204 -0.00329467
0.0209836 0.0118006
0.0911173 0.0556053
-0.00728242 0.0530507
-0.0152556 -0.00313636
0.032933 0.0571384
0.0477957 0.0672477
0.0322983 0.0653984
0.133575 0.0520714
-0.0414059 -0.0049102
-0.129449 -0.00260513
0.043628 0.0545158
0.0594395 0.0637176
-0.0267379 0.0100688
0.0394802 0.0170867
-0.0247813 0.00856878
0.105503 0.0725426
0.0855721 0.06004
0.0860587 0.0107816
0.090741 0.063294
0.0752955 0.0503837
0.0207377 0.0118327
0.0861488 0.0688943
0.0981549 0.0591266
0.0676335 0.0555016
0.0860587 -0.0010768
0.019701 0.0630909
-0.000792737 -0.0029855
0.0815268 0.0535587
0.0267419 -0.00339912
0.0867335 0.0618223
0.0458117 0.0613141
0.0662931 -0.00426872
-0.0857218 0.00287442
0.0251231 0.0613088
0.0211182 0.0625646
0.0867067 0.0515534
-0.00891927 0.0134336
0.0594398 0.0648083
0.0333261 0.0582849
0.0210958 -0.00327612
0.0236479 0.059976
-0.0340213 0.00695339
0.0927398 0.0668419
-0.0215356 0.00267682
0.0706156 0.0652856
0.0370775 0.0570537
0.0594397 0.0677231
0.0659505 -0.00536786
-0.0716782 -0.00584213
0.028139 0.0628332
0.0855721 0.0622829
0.0871096 0.0567465
-0.0132848 0.0638829
0.0677309 0.0532969
0.10665 0.0615307
-0.0131845 -0.0028405
0.0781227 0.0515416
0.0283783 0.0492767
0.0716773 -0.00524848
-0.0346416 -0.00513682
0.0851609 0.0623508
0.0673482 0.0559059
0.0800819 0.0584358
0.0384776 0.0537083
-0.0161473 0.0520461
0.029557 -0.00459369
0.0859467 0.0598753
0.0605996 0.0540644
0.0937753 0.0640986
0.0122452 0.048106
0.0853255 0.0503188
0.0337182 0.0690501
0.0134334 0.0690885
0.081527 0.0623912
0.0800819 0.0585604
0.0867201 0.0677179
-0.0449623 0.00739548
-0.00699273 0.0561925
0.0134225 0.0694345
0.00863029 0.0509872
0.0952316 0.0654015
0.0385441 -0.00263055
0.13219 0.0716795
0.0477958 0.0610093
0.0710837 0.0664114
0.0827282 -0.000713225
0.0106905 -0.00511141
0.0734366 0.0119017
-0.0412685 0.0149806
0.0594396 0.0636014
0.0373484 0.0512026
0.0594396 0.0631322
0.121243 0.0648725
0.0286795 0.0502431
0.0911173 0.0503159
0.0594398 0.061539
0.0511643 0.0585913
0.0261391 0.0666286
0.0449618 -0.00192915
0.0243586 0.0612839
0.0436381 0.054135
0.0384776 0.0594229
0.0594399 0.0658675
0.0920508 0.060858
-0.0188062 -0.00468576
0.0997462 0.0586188
0.0870049 0.0589129
0.0901953 0.0538933
0.033718 0.0669481
-0.049248 0.00151932
0.124311 0.0562609
0.129453 0.0131388
0.0594398 0.0643057
0.0496683 0.0693379
-0.0250779 0.0494833
parameters: [ 9.   1.   2.   1.2  4. ]. error: 174.897551074.
----------------------------
epoch 0, loss 1.03415
epoch 128, loss 0.742954
epoch 256, loss 0.852756
epoch 384, loss 0.7922
epoch 512, loss 0.741539
epoch 640, loss 0.603312
epoch 768, loss 0.630889
epoch 896, loss 0.666
epoch 1024, loss 0.665916
epoch 1152, loss 0.783892
epoch 1280, loss 0.924723
epoch 1408, loss 0.686447
epoch 1536, loss 0.698549
epoch 1664, loss 0.636271
epoch 1792, loss 0.628316
epoch 1920, loss 0.758566
epoch 2048, loss 0.684607
epoch 2176, loss 0.499884
epoch 2304, loss 0.559667
epoch 2432, loss 0.603337
epoch 2560, loss 0.841843
epoch 2688, loss 0.622834
epoch 2816, loss 0.794285
epoch 2944, loss 0.587805
epoch 3072, loss 0.68239
epoch 3200, loss 0.519177
epoch 3328, loss 0.75519
epoch 3456, loss 0.765774
epoch 3584, loss 0.698181
epoch 3712, loss 0.545976
epoch 3840, loss 0.563065
epoch 3968, loss 0.60778
epoch 4096, loss 0.654255
epoch 4224, loss 0.607689
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0511643 0.0434746
0.081527 0.0575496
0.0800819 0.0486923
0.0496683 0.0584785
6.39156e-06 0.00117362
0.032195 0.054278
0.0268142 0.0490591
-0.00378521 0.000174058
-0.0144706 -0.00739801
0.0188018 0.0559612
0.0168394 0.0507199
0.0594027 0.0471383
0.0271109 -0.00659166
0.081797 0.0483718
0.0819546 0.0400439
-0.0117822 -0.00580924
0.0964307 0.0448872
0.0716781 -0.00290938
0.0170834 0.0566124
0.0346353 -0.00891236
3.62028e-05 0.000474282
-0.0272995 -0.00093542
-0.000795203 -0.00736689
0.0520969 0.0598996
0.0188603 -0.00582284
0.0188596 0.00101749
0.0177241 0.0016208
0.129916 0.0428075
0.0109586 -0.000230747
0.0210981 0.00272508
0.0594396 0.0575842
0.0867199 0.052223
-0.0267379 0.000811086
0.0594397 0.0599865
-0.0215356 -0.00797446
-0.0194973 0.00041321
-0.0734384 -0.00847097
0.0237989 0.0611685
0.0407515 0.05525
0.0210209 0.0598934
0.0594452 0.0588895
0.0783558 0.0607851
0.0223968 0.0512329
0.00596546 0.0568417
0.0691072 0.0556753
0.0109517 -0.0072698
-0.041635 0.00416004
0.0495801 -0.0097932
0.0373484 0.0524342
-0.0234699 -0.00789203
-0.0271111 -0.00783623
0.0385441 0.00150035
-0.0495734 -0.00944275
-0.0416401 -0.00203427
0.0734439 -0.00681662
-0.0106908 -0.00283361
0.0305923 0.0358061
-0.0385434 -0.00750159
-0.00891927 -0.000975668
-0.0716787 -0.00886629
0.0208321 -0.000897328
-0.0177194 -0.00704993
0.0326469 -0.00746971
0.0317697 0.059953
-0.0208232 3.42809e-06
0.0920613 0.0562872
0.0593346 0.0452811
-0.0267425 -0.00695889
0.104819 0.0534358
0.0317697 0.0622599
0.125873 0.0593287
0.0611204 -0.00242938
0.0424268 0.0539125
0.0222151 0.0556875
-0.0494977 -0.00116648
-0.0111018 0.0351
-0.0394796 -0.00816918
0.0330542 0.0397789
0.0867334 0.0603045
0.0326469 -0.00256047
0.0807993 -0.00817448
0.0191272 0.0405042
0.121243 0.0542961
0.0271289 -0.000553235
0.0346452 -0.00672278
0.0541426 0.0482097
0.0272894 -0.00645406
0.0375434 0.0535518
0.029557 -0.00699139
0.0295687 -0.00869583
0.0385441 0.0057606
0.135027 0.0470254
0.0243594 0.0550291
0.0152545 0.00136648
0.0385434 0.0034975
-0.0707068 -0.00731806
-7.68032e-07 0.00072166
0.0594027 0.0401984
-0.0318748 -0.0077027
0.0538177 0.0426019
-0.0215356 -0.00753338
0.0800821 0.0529021
0.0482639 0.0606076
0.0496683 0.0579209
0.0144707 0.000155185
0.025123 0.0568918
0.000835111 0.00194274
-0.0390959 -0.00949714
-0.00733491 0.046793
-0.000835063 -0.00733693
0.0859467 0.0476066
0.000807032 0.00303859
-0.0131858 -0.00787775
-0.0734426 -0.00332235
0.0860364 -0.0049723
0.0594349 0.0482039
0.0435741 0.045741
0.0853255 0.0447917
0.0594399 0.0486065
0.0414075 -0.00762848
-0.0056768 0.038952
0.0593665 0.0467615
-1.63011e-07 -0.00685048
0.0851612 0.0447195
0.0594466 0.0628669
-0.0367964 -0.00223545
0.0621308 -0.00495992
0.0271204 -0.00692313
parameters: [ 9.071  1.19   2.061  1.102  4.257]. error: 3620410369.78.
----------------------------
epoch 0, loss 1.11353
epoch 128, loss 0.610249
epoch 256, loss 0.689531
epoch 384, loss 0.71337
epoch 512, loss 0.578143
epoch 640, loss 0.731335
epoch 768, loss 0.53292
epoch 896, loss 0.638031
epoch 1024, loss 0.579013
epoch 1152, loss 0.622391
epoch 1280, loss 0.675863
epoch 1408, loss 0.709726
epoch 1536, loss 0.627064
epoch 1664, loss 0.568941
epoch 1792, loss 0.545817
epoch 1920, loss 0.634951
epoch 2048, loss 0.541755
epoch 2176, loss 0.626163
epoch 2304, loss 0.567248
epoch 2432, loss 0.649046
epoch 2560, loss 0.627176
epoch 2688, loss 0.572093
epoch 2816, loss 0.712357
epoch 2944, loss 0.595995
epoch 3072, loss 0.638712
epoch 3200, loss 0.497624
epoch 3328, loss 0.596019
epoch 3456, loss 0.569858
epoch 3584, loss 0.642348
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0118764 0.0572672
-0.0734312 -0.00220669
-0.0111018 0.0572787
0.0468064 -0.000502817
0.094504 0.0581764
0.0621302 -0.00379819
-0.0207292 -0.00217825
0.0390889 -0.00336521
0.10664 0.0633705
0.0866799 0.0629969
0.10955 0.0556802
0.05937 0.0601467
0.0477958 0.05867
0.0321466 0.0604666
0.00459996 0.0014441
0.00863092 0.0593571
0.027823 0.0630165
0.126086 0.0614437
0.0991788 0.0601118
0.0397476 0.0607355
-0.0857218 0.00140809
0.13219 0.0722033
-0.0449623 -0.00550771
-2.64683e-07 0.0108841
0.0964307 0.0612915
0.0594398 0.0625787
0.085161 0.0597195
0.0594349 0.0624002
-0.016115 0.058562
0.0207193 0.0546386
-0.0662833 -0.0069656
0.0305919 0.0592038
0.0927396 0.0640728
-0.0267398 0.00242433
0.0857794 0.0552511
0.0749536 0.0563302
0.121243 0.0639575
0.0594376 0.0633101
-0.0416423 -0.00484708
0.0882822 0.0564734
0.0144707 -0.00189304
0.0207377 0.0145545
0.000213351 0.0651028
-0.041769 -0.000737439
0.0594399 0.0634941
-0.013282 0.0708086
0.0192074 0.0587136
-0.0318771 0.00638734
-0.0807986 -0.000896785
0.0857789 0.0577125
0.0594759 0.0561355
0.0170834 0.0657436
0.0424268 0.0620801
-0.0707068 -0.00125762
-0.0300731 -0.0050409
0.076417 0.0618244
0.0188016 0.0648947
-0.000795203 0.00176088
0.0594297 0.0612008
0.102035 0.0573347
0.032158 0.0598289
-0.00701261 0.0583558
-0.0131858 -0.00144072
0.0317697 0.0608499
0.0594396 0.0599884
0.0321468 0.060962
-0.0412711 0.00148739
0.0871095 0.0616137
-0.0272977 -0.00431178
0.0661223 0.0586273
-0.129449 0.00910106
0.0385427 -0.00337572
0.0248097 -0.00495753
0.0781227 0.0588549
0.0210958 0.000617773
0.0424369 0.0614424
-0.00628221 0.0593531
0.129458 0.0125475
0.0964772 0.0564105
0.0362233 -0.00119844
0.0710837 0.0555086
0.0281392 0.0595704
0.0897923 0.0601552
-0.000801651 -0.00281661
0.0271109 0.00726347
0.101441 0.0553331
0.0197009 0.0587077
0.0189892 -0.000895455
0.0449639 0.0084286
0.0326439 -0.00341766
0.0417629 0.00634726
0.0594324 0.063943
0.060558 0.0601746
0.0271109 -0.000432647
0.0819546 0.0576585
0.0131845 -0.000303182
0.0182076 0.0578159
0.0122452 0.0614655
0.046218 -0.00254026
-0.0417624 0.0123312
-0.0412757 -0.00164366
0.0495768 -0.000616557
0.0286789 0.0579321
-0.0385424 0.00512473
-0.0177194 0.0163862
-3.76332e-06 0.00299282
0.0631196 -0.000436026
0.0538009 0.0555207
0.0594398 0.0608668
-0.000792737 0.00114589
0.0857183 -0.0023608
-0.0734361 0.000653424
0.0117785 0.00153667
0.0594494 0.0713083
0.062595 0.0602109
0.0152616 -0.00556275
-0.0144843 0.0109624
0.0318693 0.0540674
-0.0367964 0.00497472
0.0322983 0.0584262
0.090741 0.0549795
0.0106903 -0.00143102
0.0222151 0.0534827
-0.0131845 -0.00303866
0.0562849 0.0610428
0.0624794 0.0559626
0.0594399 0.0604636
0.0449618 0.0102163
parameters: [ 8.894  0.718  1.91   1.346  3.618]. error: 17035.7797491.
----------------------------
epoch 0, loss 1.38298
epoch 128, loss 0.619449
epoch 256, loss 0.768973
epoch 384, loss 0.565057
epoch 512, loss 0.695928
epoch 640, loss 0.767994
epoch 768, loss 0.775448
epoch 896, loss 0.660251
epoch 1024, loss 0.618846
epoch 1152, loss 0.68545
epoch 1280, loss 0.697842
epoch 1408, loss 0.478618
epoch 1536, loss 0.604777
epoch 1664, loss 0.541421
epoch 1792, loss 0.645403
epoch 1920, loss 0.60636
epoch 2048, loss 0.470602
epoch 2176, loss 0.607853
epoch 2304, loss 0.644052
epoch 2432, loss 0.837117
epoch 2560, loss 0.716983
epoch 2688, loss 0.726453
epoch 2816, loss 0.812406
epoch 2944, loss 0.624911
epoch 3072, loss 0.523977
epoch 3200, loss 0.772466
epoch 3328, loss 0.46407
epoch 3456, loss 0.481965
epoch 3584, loss 0.517246
epoch 3712, loss 0.598086
epoch 3840, loss 0.607269
epoch 3968, loss 0.655253
epoch 4096, loss 0.681476
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0301504 0.058935
0.076417 0.0609348
-0.0631232 0.00212327
0.0222152 0.0555639
0.0791961 0.0478104
0.10546 0.0709458
0.0330949 0.0553302
0.0496681 0.0657471
0.0859467 0.0697126
0.0897923 0.0571164
-3.76332e-06 0.0102049
0.0594396 0.0590132
0.0318693 0.0530401
-0.0416423 0.00917064
-0.000793636 0.0113502
0.0989675 0.0632675
0.0152545 -0.00280159
0.0271204 -0.00083797
0.0813321 0.0540245
0.0977799 0.0610693
0.0594399 0.0571125
-0.0390966 0.000497718
0.022444 0.0495543
0.0468012 0.019012
0.0859466 0.0703684
0.0327313 0.0655948
-7.68032e-07 0.00726944
0.0174335 0.0484006
0.102034 0.0509816
0.0204028 0.0100933
0.0113491 0.00655735
0.0594349 0.0591476
0.0329332 0.0550646
0.0373484 0.0581913
0.0966596 0.0596676
-0.0207367 -0.00395996
0.0300726 0.00660557
0.0387924 0.0570361
0.0594397 0.0627998
0.0594397 0.0622454
-0.0326437 -0.00141217
-0.0417491 -0.00374507
0.0220537 0.0648686
2.41162e-06 0.00073534
0.081797 0.0532389
-0.0857218 -0.00199726
0.0439209 0.0503014
0.028139 0.0622221
0.0199306 0.061418
0.032195 0.052361
0.0330948 0.0601375
0.0188086 0.00536261
0.0594402 0.0519364
0.034034 -0.00349928
-0.0385434 0.0106356
-0.0118764 0.0627507
0.0295687 -0.00404197
0.0730676 0.0645916
0.0589765 0.0569874
0.01684 0.0485266
-0.0188605 0.000788384
0.0131849 -0.000844839
0.0106905 -0.00093672
0.0373484 0.0520742
0.0397474 0.0557658
-0.0188062 -0.0013927
0.0950803 0.0625656
-3.11149e-07 0.00714197
0.129916 0.048694
-0.014471 0.0122432
0.0482639 0.0659823
-0.0161572 0.0570732
-0.0131858 -0.00327846
6.39156e-06 -0.000531438
0.0417583 0.0203843
-0.0611267 0.00217889
0.0305919 0.0526486
0.0952318 0.0681039
0.106634 0.0644506
0.0594273 0.0535096
0.0236479 0.0658034
0.00459996 -0.00270261
0.0327312 0.0547488
0.0144846 0.0150192
6.7959e-07 -0.00105423
0.0983621 0.0586921
0.0144846 -0.00251602
0.0871095 0.0652529
0.0278229 0.0570772
0.0964772 0.0628956
0.000834798 0.0187981
0.0248097 -0.00284231
0.0251231 0.05868
0.0594396 0.0642356
-3.91322e-05 0.00684738
0.102035 0.0576894
-0.0023637 0.0642736
0.0482639 0.0622642
0.0897923 0.0562007
0.0594399 0.0556179
0.0131848 -0.00270262
-0.0110929 0.0443763
0.028139 0.0623926
0.0224432 0.049335
0.0207331 -0.00213814
0.0375382 0.0538117
-0.0188606 0.0119823
0.0968257 0.0583211
0.0621339 -0.00341073
-2.95455e-07 -0.0023177
0.0691985 0.0613729
-0.0611171 -7.46838e-05
0.0268195 0.0599997
0.124354 0.058276
0.0562648 0.0585658
-0.0111018 0.0420513
0.0283785 0.0532454
0.0594349 0.0614814
0.0215366 -0.00147049
0.0248076 0.0114204
-0.0295746 0.0122196
-0.0132904 0.0652655
-0.0367964 0.00706811
-0.0132848 0.0664711
-0.0462076 -0.00250922
0.0964766 0.0590583
0.143948 0.0578345
-4.4075e-07 -0.00156584
parameters: [ 9.045  1.12   2.038  1.138  4.162]. error: 54694125.6211.
----------------------------
epoch 0, loss 1.09193
epoch 128, loss 0.685158
epoch 256, loss 0.611516
epoch 384, loss 0.644818
epoch 512, loss 0.573446
epoch 640, loss 0.643881
epoch 768, loss 0.660666
epoch 896, loss 0.72546
epoch 1024, loss 0.664682
epoch 1152, loss 0.64225
epoch 1280, loss 0.653167
epoch 1408, loss 0.772147
epoch 1536, loss 0.667168
epoch 1664, loss 0.67687
epoch 1792, loss 0.64521
epoch 1920, loss 0.644213
epoch 2048, loss 0.718086
epoch 2176, loss 0.525478
epoch 2304, loss 0.7021
epoch 2432, loss 0.454751
epoch 2560, loss 0.523753
epoch 2688, loss 0.740826
epoch 2816, loss 0.466083
epoch 2944, loss 0.70717
epoch 3072, loss 0.683462
epoch 3200, loss 0.678636
epoch 3328, loss 0.584907
epoch 3456, loss 0.666326
epoch 3584, loss 0.720153
epoch 3712, loss 0.638578
epoch 3840, loss 0.546881
epoch 3968, loss 0.597163
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594397 0.0616268
0.0286795 0.0548077
0.0868849 0.0659782
0.0204045 -0.00309043
0.0234754 0.0122065
0.0964307 0.0507167
0.0594426 0.0609433
0.0952317 0.0646515
-0.0412711 -0.00547711
-0.00131656 -0.00279201
0.0301505 0.0576248
-0.000795203 0.0140271
0.0927397 0.0588366
0.0964307 0.0585503
-0.0248165 -0.00380356
0.0594396 0.0655572
-0.0109489 0.0111553
-4.71368e-07 0.0114296
-0.0326435 0.00061017
0.025123 0.0645306
0.0897923 0.0651079
0.0416484 0.00436048
0.0204028 -0.000170681
0.0859962 0.0693976
0.0477956 0.0556249
0.0706155 0.0589817
0.059435 0.0813747
-1.63011e-07 0.0124339
0.0461383 0.0545476
0.0594402 0.0585816
0.0191272 0.0440836
0.0511643 0.0528441
0.0952315 0.0613812
0.0594396 0.0626822
-0.00737805 0.0500151
-0.0234677 -0.00487019
0.0417484 -0.00258415
0.014484 -0.00136792
0.0593346 0.0559594
0.0981549 0.0509251
0.13219 0.0835406
-0.0346416 -0.00457656
0.0594919 0.0619655
0.0385437 -0.0019894
0.0326439 0.00242305
0.0217931 0.0625572
0.0851612 0.0597493
0.0494968 -0.00447087
0.105471 0.0819387
0.0267779 0.0453945
-0.0394796 0.0145017
0.0594397 0.0601579
0.0800819 0.0547085
0.0189892 0.0103948
0.0897923 0.0591991
-0.000807624 0.0019127
0.0375382 0.0556748
1.13647e-06 0.013188
3.82209e-05 0.00447921
0.0277564 0.0553112
-0.0271193 0.00492208
0.0211183 0.0685446
0.0964766 0.0539633
-0.0495737 -0.00223693
0.0865811 0.0639189
-0.0495737 -0.0029537
0.000835111 0.00298588
0.0920561 0.0635426
0.125863 0.0666548
0.124277 0.0649786
-0.0295823 0.00340229
-7.99663e-06 -0.00185287
0.0268195 0.0518642
0.0211182 0.0688586
-0.0495796 -0.00161394
0.0267779 0.0557006
0.0373475 0.0517155
0.00894003 0.00674341
-0.0113495 -0.000370341
0.0385437 -0.00272999
0.0272916 0.00126175
0.0594494 0.0807881
0.0210212 0.063407
0.0594398 0.0588297
0.0964305 0.0518205
0.101814 0.0714586
0.0106905 -0.00195853
-0.0188062 -0.00282778
0.0631163 -0.000662002
0.000213351 0.0721865
-0.00735889 0.0588604
0.0594398 0.0612275
0.0414075 -0.00389432
0.129453 0.00506782
0.0464004 0.0461018
0.0594325 0.0651084
0.0594397 0.0567154
0.0870053 0.0601386
0.0435741 0.0631928
0.100097 0.0701047
0.0594273 0.060951
0.0701141 0.0618051
0.0594494 0.0816535
0.0631207 0.00407099
0.0582589 0.0609133
-0.00459356 -0.00371162
0.0326469 0.00152104
0.0278229 0.0576483
0.0109517 -0.00426143
0.0593312 0.0675835
0.0477959 0.0573285
0.0329332 0.0588265
0.0611204 -0.00078972
-0.0385431 0.00524146
-0.0111821 0.0548806
-0.0492483 -0.00295188
0.094504 0.0565164
0.0495763 0.00365944
0.0330953 0.0482058
0.028139 0.0496837
0.0281391 0.0560808
0.062595 0.0633721
-0.0194973 -0.00264348
-0.0467928 -0.00487252
-0.10011 -0.00271446
0.0387924 0.0511383
0.0941706 0.0648606
-0.0131858 0.00828219
parameters: [ 8.996  0.988  1.996  1.206  3.984]. error: 145060127.732.
----------------------------
epoch 0, loss 1.36337
epoch 128, loss 1.01137
epoch 256, loss 0.724433
epoch 384, loss 0.948009
epoch 512, loss 0.686427
epoch 640, loss 0.795928
epoch 768, loss 0.600191
epoch 896, loss 0.667485
epoch 1024, loss 0.565581
epoch 1152, loss 0.809309
epoch 1280, loss 0.642079
epoch 1408, loss 0.576344
epoch 1536, loss 0.581838
epoch 1664, loss 0.544468
epoch 1792, loss 0.683896
epoch 1920, loss 0.727888
epoch 2048, loss 0.63895
epoch 2176, loss 0.747841
epoch 2304, loss 0.676301
epoch 2432, loss 0.519535
epoch 2560, loss 0.718942
epoch 2688, loss 0.572219
epoch 2816, loss 0.777166
epoch 2944, loss 0.669454
epoch 3072, loss 0.672036
epoch 3200, loss 0.448037
epoch 3328, loss 0.659689
epoch 3456, loss 0.677245
epoch 3584, loss 0.667583
epoch 3712, loss 0.587683
epoch 3840, loss 0.547421
epoch 3968, loss 0.713749
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.124354 0.0573546
0.124277 0.0545863
1.01848e-06 -0.00174704
0.121243 0.062626
-0.0468045 0.00829923
-0.00553123 0.0514094
0.0417489 -0.000543337
0.0093143 0.0618149
0.0710837 0.0598741
0.0691984 0.0615528
0.0243586 0.0586886
0.0464005 0.0545979
0.0122142 0.0593976
-1.48505e-07 0.0114428
0.0318695 0.0577828
-0.0734312 0.00653072
0.026809 0.056549
0.022215 0.0581204
0.0964766 0.0562005
0.0243585 0.05727
0.0168394 0.0572047
-0.0152583 0.00719412
0.0396778 0.0579255
0.0278229 0.0605214
0.085161 0.0586808
0.0532657 -8.52264e-06
0.0871097 0.0635202
0.0871096 0.0621652
0.0859466 0.0620554
0.0927398 0.0589067
-0.00236368 0.0573637
0.0201879 0.0581942
0.0496682 0.0590377
0.0449639 0.0128158
0.0321466 0.0642703
0.0416484 0.0110755
0.0594395 0.0595797
0.0650734 0.0598706
-0.0131845 0.014418
0.0317698 0.0628934
0.0251233 0.0612508
-0.0300728 0.00357873
0.0904958 0.0609694
0.0407513 0.0550424
0.121243 0.0592185
0.0594273 0.0604154
0.0462066 -0.0014414
0.0827282 0.00100248
-2.23337e-05 0.0107761
0.0281391 0.0613455
0.0204119 0.0126901
0.100116 -0.000343735
0.00460075 0.00522745
-0.00737805 0.0512414
0.0857789 0.0568836
0.0594376 0.0589271
0.0594397 0.061222
0.0412752 0.00292384
0.0707072 0.000113928
0.105503 0.0645771
0.0174335 0.0547587
-0.0611171 -0.00395514
0.105506 0.0650628
0.0781227 0.0554094
0.101441 0.0556276
0.0950805 0.0579723
-0.0827278 0.00511137
0.0497672 0.0603988
0.0707072 -0.00291176
-3.76332e-06 0.00942715
0.0412702 0.0175293
0.0317699 0.0604408
-0.0248069 0.0140033
0.0261393 0.0614855
0.0904958 0.0603493
0.10665 0.0611981
-0.0144836 0.000281633
0.081527 0.0571484
0.0330948 0.0566114
-0.0385431 0.0111172
0.079197 0.0555655
0.0589127 0.0551892
0.0813321 0.0582934
0.0594759 0.0521195
0.0236479 0.0593321
0.000834798 0.00194463
0.0853245 0.0581833
0.130756 0.057924
-0.00894944 0.0146964
0.0113491 0.0130644
0.00931421 0.0636907
0.0387924 0.0581203
0.0131848 0.00140556
0.135027 0.0595215
0.0318695 0.0550459
-0.0495734 0.00519257
0.062595 0.0605263
0.000835111 0.0110504
-1.32922e-07 0.0105075
0.0594397 0.0608247
0.112932 0.0638855
0.0290868 0.0610735
-0.00249422 -0.00399486
0.0209719 0.00777607
0.0152591 -0.00272362
0.0594396 0.0597447
-0.0194897 0.000597295
-0.00236343 0.061395
0.0594494 0.0656202
0.121243 0.0592964
0.0594398 0.0610374
-0.0414033 0.00171809
-0.0295676 -0.00189027
0.0261391 0.0580434
0.0901948 0.0582804
0.0855721 0.0628776
0.0122142 0.0558972
0.0435741 0.054821
0.038792 0.0554485
0.0626054 0.0579611
-0.00236343 0.061395
0.0860523 0.00910849
-0.0412757 0.00900529
-0.00968441 0.0553227
0.0439209 0.0588138
0.0093143 0.0585345
0.0781227 0.0570365
-0.014471 0.000841007
parameters: [ 9.017  1.046  2.015  1.176  4.062]. error: 95965.5219446.
----------------------------
epoch 0, loss 1.05023
epoch 128, loss 0.728623
epoch 256, loss 0.762327
epoch 384, loss 0.86458
epoch 512, loss 0.772564
epoch 640, loss 0.928965
epoch 768, loss 0.737955
epoch 896, loss 0.590381
epoch 1024, loss 0.606708
epoch 1152, loss 0.694873
epoch 1280, loss 0.743057
epoch 1408, loss 0.735112
epoch 1536, loss 0.634392
epoch 1664, loss 0.464375
epoch 1792, loss 0.697801
epoch 1920, loss 0.633874
epoch 2048, loss 0.677697
epoch 2176, loss 0.647568
epoch 2304, loss 0.639063
epoch 2432, loss 0.651379
epoch 2560, loss 0.585078
epoch 2688, loss 0.51339
epoch 2816, loss 0.74965
epoch 2944, loss 0.677086
epoch 3072, loss 0.670966
epoch 3200, loss 0.613759
epoch 3328, loss 0.580159
epoch 3456, loss 0.656186
epoch 3584, loss 0.65423
epoch 3712, loss 0.467429
epoch 3840, loss 0.57026
epoch 3968, loss 0.691047
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0261392 0.0528974
-0.0188599 0.00179883
0.0248097 -0.00688981
0.038792 0.0518308
0.0236482 0.0570871
0.0920561 0.0550521
-0.00236368 0.0547136
0.0991789 0.0595693
0.0859465 0.0561402
-0.0113401 -0.00220812
0.0621308 -0.00610836
-0.0210947 -0.00280655
0.0271163 0.00606716
-2.23337e-05 -0.00302669
0.0625949 0.0570013
0.0706156 0.0579996
0.0997468 0.0476494
0.0390991 -0.00624434
0.0511643 0.0526157
0.0318695 0.0538155
0.0871098 0.0564806
0.0497672 0.0529399
0.0346348 -0.00114031
0.0861486 0.0582993
0.102034 0.0538933
-0.0340213 0.00611619
0.0532677 -0.00269825
0.0927399 0.0584229
0.0267419 0.00250045
0.00378707 -0.00402929
-0.0417573 -0.00599052
-0.0271111 -0.000819112
0.0594396 0.0578373
0.0144846 -0.00717948
0.0945153 0.0504969
0.0594452 0.0682955
0.0800819 0.0523195
0.074953 0.0479473
-0.0210947 -0.00455945
0.0724741 0.0538778
-0.0318748 0.00233347
0.0477959 0.0578506
-0.00700262 0.0461308
0.0407513 0.0505602
0.015257 0.00317784
-0.0662833 -0.00694537
0.0417454 -0.00260925
-6.66928e-06 -0.00396632
0.132184 0.0681452
0.0901953 0.0525461
4.57767e-08 0.0037157
0.043628 0.0514787
0.0592406 0.0535809
0.0867335 0.0569701
0.129445 -0.0046816
0.0739227 0.0631648
-0.0857226 -0.00574925
0.0407515 0.0534792
0.0920961 0.0511735
-0.0323135 -0.00401646
1.74905e-07 0.00658771
0.0781223 0.0526306
4.17498e-06 -0.00163769
0.0594397 0.0587638
-0.0659495 -0.00161364
0.0168394 0.048409
0.0243594 0.0506466
0.000793148 -0.00293388
0.0207193 0.0472962
0.133575 0.0564174
0.0435639 0.0557534
-0.0189864 0.00105092
-1.63011e-07 -0.00774174
0.0650734 0.052391
0.046799 0.00377486
0.121243 0.0570702
-0.0161473 0.0514972
0.126169 0.0535823
0.0272988 -0.001162
0.0594396 0.0580415
0.019701 0.0577927
-2.23337e-05 0.0063781
-0.041769 0.00500307
0.0329331 0.054757
0.0594325 0.0551103
0.0318693 0.0497281
0.0237988 0.0549258
0.0997468 0.0499487
0.0327313 0.0555998
-0.0468045 0.00157844
1.74905e-07 -0.00376585
0.0562851 0.0561642
0.0194981 -0.00533148
0.0209719 -0.00585804
-0.0318771 -0.0048116
0.0346452 0.00154266
0.0968256 0.0584524
0.00080867 -0.000648628
0.12432 0.0577463
0.0594397 0.0587275
0.0706155 0.0577549
0.125873 0.0529845
0.059435 0.0526936
0.0234638 0.00197684
0.0286789 0.0502554
0.0594397 0.0562669
0.106645 0.0527484
-0.0295823 -0.00810584
0.0861488 0.059019
0.059407 0.0564822
-0.0367964 -0.0057902
0.0621334 -0.00532695
0.0594399 0.0575273
0.126127 0.0573707
0.00596587 0.0591247
0.0152545 -0.00128394
-0.0468001 -0.00443058
-0.0346416 -0.00402046
-0.0707062 0.0017457
0.0594396 0.0579127
-0.00484737 0.0569991
0.0188016 0.0586537
0.121243 0.059832
0.0631207 -0.00107594
-0.0412731 -0.00431839
0.0594397 0.0562644
0.00863032 0.0525499
0.0199306 0.0593905
parameters: [ 9.007  1.017  2.006  1.191  4.024]. error: 18771126238.4.
----------------------------
epoch 0, loss 1.24513
epoch 128, loss 0.787701
epoch 256, loss 0.868525
epoch 384, loss 0.525538
epoch 512, loss 0.679273
epoch 640, loss 0.707472
epoch 768, loss 0.648095
epoch 896, loss 0.6001
epoch 1024, loss 0.684662
epoch 1152, loss 0.671058
epoch 1280, loss 0.663088
epoch 1408, loss 0.597142
epoch 1536, loss 0.548416
epoch 1664, loss 0.630069
epoch 1792, loss 0.567335
epoch 1920, loss 0.628749
epoch 2048, loss 0.692403
epoch 2176, loss 0.695073
epoch 2304, loss 0.625901
epoch 2432, loss 0.76007
epoch 2560, loss 0.563982
epoch 2688, loss 0.647054
epoch 2816, loss 0.564649
epoch 2944, loss 0.872596
epoch 3072, loss 0.468887
epoch 3200, loss 0.554347
epoch 3328, loss 0.652433
epoch 3456, loss 0.585379
epoch 3584, loss 0.492003
epoch 3712, loss 0.588889
epoch 3840, loss 0.604843
epoch 3968, loss 0.596084
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.00459996 0.000702449
0.0189892 0.0022884
0.0497671 0.0464939
-0.00728242 0.0368054
0.0333262 0.0600414
0.0197009 0.055461
-0.0248069 0.0031902
0.0464005 0.0485235
-0.0131854 0.00983109
0.0599031 0.0544107
0.0464004 0.048095
0.0508582 0.0453757
0.0791969 0.058569
0.0604514 0.0485481
-0.00484737 0.0490524
0.0318695 0.0516808
0.135027 0.0515181
0.0322982 0.0616054
0.0337179 0.0625258
0.0248169 -0.00767344
-0.0494977 0.00531989
-0.0131854 -0.00721853
0.0945151 0.0436962
0.0997468 0.0538837
0.072741 0.0622088
0.0952315 0.0676248
0.0803921 0.0514175
0.0791961 0.0556472
0.0626054 0.048299
0.061289 0.0681313
0.0594398 0.058273
0.0317699 0.0643311
0.0860587 -0.00703029
0.0599029 0.056899
0.0495801 -0.0080534
-0.0189848 -0.0057562
-0.0412685 0.00737039
0.022215 0.0369326
0.0749536 0.0522507
0.129445 0.00827617
-0.0417792 0.00633442
0.0207304 -0.00782889
0.0385437 -0.00669057
0.0174339 0.0364691
0.0385431 0.00204914
0.0594399 0.0617972
0.0937752 0.0600034
-0.0111692 0.0399314
0.0319944 0.0449714
0.0860523 -0.00626803
0.0520969 0.0614235
0.0871097 0.067246
-0.0118767 0.0603412
0.0868848 0.0534788
0.0496683 0.0630908
0.0362233 -0.00690123
0.0907408 0.0639252
-0.0106876 -0.00853944
8.39808e-07 0.00251377
0.0414025 0.00995599
0.0764118 0.0489501
0.0335492 0.0354765
-0.0109774 0.00256131
-0.0707062 -0.00703951
0.042448 0.0509691
0.0477957 0.0523504
0.0538177 0.0504409
0.0594402 0.044696
-0.129453 -0.00776413
0.0261391 0.0524458
-0.00131975 -0.000508203
0.0468064 -0.0081135
-0.0209824 -0.000868815
0.0910567 0.0683642
0.0321466 0.0555048
-0.000801651 0.0017858
0.125157 0.0478403
0.0243585 0.0366322
0.0594473 0.0518599
0.0201879 0.040161
0.132184 0.0722723
0.0867119 0.0512266
0.0691985 0.0549091
0.0594473 0.055202
-0.0248165 -0.00853605
0.101441 0.0547949
0.143948 0.0542823
0.0776855 0.00756014
0.00250001 -0.00280914
-6.30661e-07 -0.00627022
0.0978866 0.0662637
0.0648671 0.0446584
0.0271187 -0.00799041
0.0868849 0.053568
0.0791268 0.0553795
0.0220537 0.0484061
0.000802697 -0.00561466
-0.0412685 -0.00718813
0.0340225 -0.00864882
0.029557 0.00961569
0.126169 0.0608165
0.0407515 0.0397083
0.00891968 -0.00561749
0.0734322 0.00760572
-0.080798 -0.00103162
0.0977801 0.0610154
8.39808e-07 -0.00754372
0.00250079 -0.00823557
-0.0707055 0.00367207
0.0734366 0.00435355
-0.000786289 -4.88154e-05
0.0764328 0.0539376
0.0412776 0.0121534
0.0290868 0.0585948
0.0594273 0.0476296
0.0210212 0.0566933
0.0977802 0.0652747
0.00863032 0.0369157
0.0991789 0.0658971
0.0710835 0.0538567
-0.0248069 -0.00808343
-0.0215356 -0.00827408
0.0131845 0.00998237
0.0337179 0.0457402
0.0267779 0.0375321
0.0625949 0.0622202
-0.0716787 -0.0074369
0.0188603 -0.00759451
parameters: [ 9.   1.   2.   1.2  4. ]. error: 98564201.8416.
----------------------------
epoch 0, loss 0.901899
epoch 128, loss 0.941202
epoch 256, loss 0.602574
epoch 384, loss 0.648857
epoch 512, loss 0.664306
epoch 640, loss 0.719563
epoch 768, loss 0.912779
epoch 896, loss 0.714553
epoch 1024, loss 0.683832
epoch 1152, loss 0.657528
epoch 1280, loss 0.842724
epoch 1408, loss 0.740063
epoch 1536, loss 0.648334
epoch 1664, loss 0.774082
epoch 1792, loss 0.746605
epoch 1920, loss 0.741143
epoch 2048, loss 0.829318
epoch 2176, loss 0.690115
epoch 2304, loss 0.677791
epoch 2432, loss 0.61255
epoch 2560, loss 0.595079
epoch 2688, loss 0.748543
epoch 2816, loss 0.728407
epoch 2944, loss 0.666995
epoch 3072, loss 0.705281
epoch 3200, loss 0.655024
epoch 3328, loss 0.715591
epoch 3456, loss 0.592464
epoch 3584, loss 0.511087
epoch 3712, loss 0.462936
epoch 3840, loss 0.691512
epoch 3968, loss 0.529695
epoch 4096, loss 0.682871
epoch 4224, loss 0.582295
epoch 4352, loss 0.755014
epoch 4480, loss 0.72112
epoch 4608, loss 0.563873
epoch 4736, loss 0.529719
epoch 4864, loss 0.659076
epoch 4992, loss 0.509467
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.027823 0.059927
0.059367 0.0546712
0.0819546 0.0589106
0.032933 0.0632451
0.0594481 0.0730258
0.023648 0.057273
0.125157 0.0555219
0.0211185 0.0629218
0.0589765 0.0650897
0.0594395 0.0600388
0.132184 0.0775019
0.101441 0.058669
0.0661225 0.0601345
-0.10011 0.00493048
0.0927397 0.0613978
0.088282 0.0606733
0.0327314 0.0589604
0.0373475 0.0548234
0.0691986 0.0618029
0.0482639 0.064449
0.0329331 0.0609495
0.0327314 0.0636131
0.0764272 0.0584744
-0.0385437 -0.00118192
0.0394802 -0.00168287
0.0220538 0.0610571
-0.0131858 0.000102905
0.0197009 0.0601165
0.0223972 0.0536317
0.0983621 0.052771
0.0691068 0.0557214
0.088282 0.0574021
-4.71368e-07 0.0181175
3.62028e-05 0.0165997
0.0986862 0.0588328
0.0589127 0.0565661
0.0966587 0.0583849
0.0851511 0.0581006
0.0851613 0.0602522
-0.0734426 -0.000257003
-0.065961 0.00646186
0.046799 -0.00117305
0.0464005 0.0564386
-0.0462049 0.00856255
-3.17974e-05 0.0133974
0.0283785 0.0527377
0.014484 0.00899452
0.106629 0.0625677
0.0326469 -0.00312615
-0.0152606 0.0114381
-0.0860382 0.00983846
0.0397476 0.0536065
0.0271204 0.0137609
-0.0267398 -0.00377172
0.0248076 0.00831788
0.081527 0.0582756
0.0920961 0.0598792
0.0394803 0.0229306
0.0791969 0.0575588
0.094504 0.0572091
0.0416418 -0.00223206
2.27454e-05 0.00985864
0.0424369 0.054547
0.0691068 0.0557896
-3.76332e-06 0.0134789
-0.0152556 0.0118152
0.0897923 0.0619997
0.0318695 0.0548165
0.0370775 0.0514228
0.00250079 0.0107234
-2.23337e-05 0.013703
-0.016115 0.0493031
-0.00968441 0.0489052
-0.00726943 0.0471569
0.121243 0.0654295
0.0286795 0.0527628
-0.0106908 -0.00299776
0.130756 0.0682108
0.0321897 0.0530144
0.0330948 0.0536707
0.0122404 0.0497724
0.0278228 0.0590732
0.104819 0.0628013
0.0594349 0.0559703
-0.014485 0.000372075
0.079197 0.0526366
0.0464004 0.0559699
0.000794682 0.00689686
0.0207193 0.0510646
0.028139 0.0615273
0.0927398 0.0608526
-0.0631232 -0.00277368
0.121243 0.0619498
0.0927396 0.0625592
-0.0860382 0.00796469
0.0662934 0.00701749
-0.0860382 0.00926853
0.0673482 0.0584777
0.121243 0.0619498
-0.014471 0.0157769
-0.0707068 0.0148147
0.0807987 0.0228965
0.0109781 -0.00167511
0.0594397 0.0648406
0.0211185 0.0639909
-0.0860382 0.0099871
0.0497671 0.0550073
0.0122352 0.0573118
-0.0707055 -0.00175942
0.0267781 0.0565086
-0.0111692 0.0496594
0.0594116 0.0553219
0.0562849 0.0630384
0.0416368 0.00817091
-0.0210947 -0.00149745
0.0237987 0.0580474
0.0317699 0.0649507
0.0416368 -0.000896513
0.0144859 0.0145511
0.0904958 0.0566323
0.0295843 -0.00145616
0.0857183 -0.00411368
0.0495801 0.00950394
0.0861487 0.0622969
0.0131855 0.0144802
0.072741 0.0647874
0.0650734 0.0554367
0.0791961 0.05765
parameters: [ 9.   1.   2.   1.2  5. ]. error: 58823.7473453.
----------------------------
epoch 0, loss 1.1475
epoch 128, loss 0.742041
epoch 256, loss 0.757353
epoch 384, loss 0.683016
epoch 512, loss 0.723627
epoch 640, loss 0.755196
epoch 768, loss 0.777936
epoch 896, loss 0.759821
epoch 1024, loss 0.70081
epoch 1152, loss 0.578579
epoch 1280, loss 0.801355
epoch 1408, loss 0.657084
epoch 1536, loss 0.613868
epoch 1664, loss 0.786221
epoch 1792, loss 0.733223
epoch 1920, loss 0.619562
epoch 2048, loss 0.606395
epoch 2176, loss 0.679284
epoch 2304, loss 0.63833
epoch 2432, loss 0.544376
epoch 2560, loss 0.575905
epoch 2688, loss 0.665219
epoch 2816, loss 0.602818
epoch 2944, loss 0.723108
epoch 3072, loss 0.642331
epoch 3200, loss 0.481468
epoch 3328, loss 0.574048
epoch 3456, loss 0.626747
epoch 3584, loss 0.719497
epoch 3712, loss 0.597261
epoch 3840, loss 0.581093
epoch 3968, loss 0.558005
epoch 4096, loss 0.708285
epoch 4224, loss 0.581496
epoch 4352, loss 0.661609
epoch 4480, loss 0.599655
epoch 4608, loss 0.497914
epoch 4736, loss 0.658846
epoch 4864, loss 0.576301
epoch 4992, loss 0.563018
epoch 5120, loss 0.595194
epoch 5248, loss 0.70613
epoch 5376, loss 0.541357
epoch 5504, loss 0.518978
epoch 5632, loss 0.47987
epoch 5760, loss 0.692921
epoch 5888, loss 0.575805
epoch 6016, loss 0.67066
epoch 6144, loss 0.514646
epoch 6272, loss 0.590755
epoch 6400, loss 0.592788
epoch 6528, loss 0.656744
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0317699 0.0582997
0.118684 0.0740337
0.086885 0.0680145
0.0362237 -0.000871851
0.076422 0.0593716
0.0370775 0.0468395
0.0140549 0.0464688
-0.00700262 0.0474227
-0.0340213 -0.00258305
0.0594562 0.0548896
0.0859467 0.0671697
0.0595095 0.0551138
9.99101e-07 0.0106428
0.032933 0.0591582
0.0305923 0.0433216
0.0594324 0.0558403
0.0861486 0.0689357
0.0867334 0.0644016
0.0131855 -0.00103696
0.0871096 0.0664833
0.0482639 0.0644177
-0.00699273 0.0431082
0.0396776 0.0491768
0.028139 0.0603783
0.0910567 0.0688771
0.0134334 0.0753585
0.0439209 0.0496375
-0.0776845 -0.00417589
-0.0113401 0.00655408
-0.00236348 0.0473192
0.0243594 0.0463123
0.0604514 0.0561228
0.057604 0.0787586
-0.0707062 -0.00298954
0.0594397 0.0621104
0.0716773 0.00944558
0.0803921 0.0637493
-0.0318748 0.000884321
-0.0234699 -0.00150615
0.0594473 0.0590448
0.0188015 0.0619375
-0.0807986 -0.00167314
0.0594398 0.064143
0.0642209 0.0539925
0.0496681 0.0599
0.0904958 0.0564726
0.0594396 0.0624107
0.0416484 0.00877271
-0.0188062 0.000524198
6.39156e-06 0.000995778
0.0594482 0.0797138
-0.0113373 -0.000304542
0.0599031 0.0614748
0.0989675 0.0718372
0.0278231 0.0617095
-0.0394796 0.000585508
0.00863088 0.0455805
0.0807977 0.0104608
0.0964307 0.0578364
-0.0417624 0.00562585
-0.0462148 0.000576218
0.0236481 0.0565556
0.0236479 0.0568755
0.0290868 0.058096
-0.032317 0.00240542
0.0271289 0.0121893
3.62028e-05 0.00754228
0.0867199 0.0628685
0.00863029 0.0403804
0.0199307 0.0634211
0.0236482 0.0589507
-0.00236373 0.0494835
0.0910567 0.067395
0.0868849 0.0595473
-0.00484737 0.0494144
0.0283783 0.0487483
-0.0390966 -0.00313805
0.0593758 0.0526456
0.0594398 0.0648338
0.121243 0.0709042
0.101441 0.0613455
-0.00249422 0.00442559
0.0385434 -0.00116709
0.125873 0.057123
-0.0271278 0.000866921
0.0496681 0.0638537
0.0317698 0.0588289
0.0295843 0.00760075
0.0691986 0.0650594
0.0131849 0.00368282
-0.0734312 -0.00321333
0.0326469 0.00112899
-0.0188062 0.00241165
0.0182076 0.0526654
0.0867332 0.0626633
0.0594325 0.0537973
0.0321897 0.0515761
0.00250079 0.00700926
0.097887 0.0731946
-0.049248 0.00242877
0.143948 0.0629775
0.0562851 0.0617869
-0.0271152 0.000938007
0.0236482 0.0569199
0.041765 -0.00240954
-0.0417422 0.00398761
-0.00142797 -0.00292718
0.0385441 0.00480796
0.0855722 0.0682119
0.0210209 0.0638555
4.07609e-05 0.00791379
0.0317699 0.0595038
0.032933 0.0612917
0.0989675 0.0711811
0.0904958 0.0579125
-0.0131845 5.06312e-05
0.0152616 -0.00118523
0.0267393 2.33285e-06
-0.0611171 -0.00136958
0.0224432 0.0472464
0.0323163 0.00509878
0.0605996 0.0572234
0.0268143 0.0533963
0.0594449 0.054695
0.00131924 0.00282854
0.0199307 0.0645752
0.0267369 0.00862684
0.0920961 0.0552276
parameters: [ 9.     1.     2.     1.2    6.618]. error: 257771921.664.
----------------------------
epoch 0, loss 1.38081
epoch 128, loss 0.948581
epoch 256, loss 0.729021
epoch 384, loss 0.814203
epoch 512, loss 0.829236
epoch 640, loss 0.525105
epoch 768, loss 0.641621
epoch 896, loss 0.650755
epoch 1024, loss 0.796512
epoch 1152, loss 0.45968
epoch 1280, loss 0.718472
epoch 1408, loss 0.583419
epoch 1536, loss 0.575225
epoch 1664, loss 0.702648
epoch 1792, loss 0.677396
epoch 1920, loss 0.731829
epoch 2048, loss 0.531741
epoch 2176, loss 0.850275
epoch 2304, loss 0.654256
epoch 2432, loss 0.661691
epoch 2560, loss 0.652605
epoch 2688, loss 0.754767
epoch 2816, loss 0.776846
epoch 2944, loss 0.64998
epoch 3072, loss 0.824776
epoch 3200, loss 0.54159
epoch 3328, loss 0.64165
epoch 3456, loss 0.522283
epoch 3584, loss 0.602701
epoch 3712, loss 0.580877
epoch 3840, loss 0.483086
epoch 3968, loss 0.763889
epoch 4096, loss 0.499572
epoch 4224, loss 0.591216
epoch 4352, loss 0.571393
epoch 4480, loss 0.544553
epoch 4608, loss 0.673812
epoch 4736, loss 0.603382
epoch 4864, loss 0.664411
epoch 4992, loss 0.72196
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.101814 0.0738888
-0.0188599 0.00195009
0.0594325 0.0597922
0.0319944 0.0642339
0.0859465 0.0704475
0.00931426 0.061869
0.0310444 0.0545239
0.0319944 0.0609769
0.0496681 0.0669915
0.0859466 0.0698947
-0.0207342 0.0181874
-0.00331175 0.0494999
0.0964307 0.0552077
0.0477957 0.0664787
0.0594398 0.0668637
-0.032317 0.00912483
0.0791961 0.0560496
0.0878297 0.0604763
0.0865812 0.0724604
0.088729 0.0724894
-0.00236343 0.0596297
0.0857794 0.0617815
4.17498e-06 0.00132392
0.0189868 0.00685355
0.0271185 0.0104312
0.0373475 0.0440929
0.0927399 0.0661335
0.0606162 0.0577677
0.0977799 0.0723763
0.100097 0.0672036
0.0538009 0.0632451
0.0596272 0.0553401
0.0477959 0.0567725
0.0323163 0.00689165
0.0197009 0.0543079
0.0310444 0.0553642
0.0626054 0.0636902
0.0271115 0.0065642
0.0109857 0.0342408
0.0439209 0.0506496
0.0631207 0.0014443
0.0492474 0.0039566
0.0730675 0.0709683
0.0707062 0.00182175
0.028139 0.063861
0.0037846 0.00269923
0.0676335 0.055408
0.0861488 0.0714379
0.0562597 0.0619052
0.0589765 0.0726685
0.0701141 0.0648041
0.0197009 0.0570775
0.0675205 0.0585115
-0.0417624 0.00724349
0.121243 0.0771354
-0.0188605 0.00718396
0.0625951 0.0689895
0.0594396 0.0600449
0.0237987 0.0603065
0.0237986 0.0541941
0.0037846 0.00269923
0.104819 0.0643134
0.0626054 0.0644936
0.0815268 0.0547831
-0.0394796 0.020435
0.0319945 0.0657636
-0.0340329 0.0209097
0.0904958 0.0668668
-0.0462049 0.0108332
0.0191276 0.0488438
0.0910569 0.0711324
-0.0318771 0.00717466
0.0527577 0.0604959
0.0964305 0.0558268
0.0496682 0.0651604
0.0659615 0.0131534
0.019701 0.0558239
0.129925 0.069261
0.0375381 0.0528968
0.0593105 0.0597272
1.13647e-06 0.00130283
-0.0716787 0.00108071
0.0868851 0.0726708
0.10665 0.0683545
0.0991789 0.0725139
0.10664 0.0656138
1.95996e-07 0.00159806
0.0800821 0.0587887
-0.053266 0.00159166
0.0594397 0.0714374
0.0867119 0.0651358
2.96104e-08 0.00301129
0.0283783 0.056875
-5.97989e-06 0.00180936
0.0492474 0.0110323
0.0937753 0.0720742
0.0594398 0.0663952
0.0189868 0.000903659
-0.0131858 0.0118641
0.0407513 0.0593377
-0.0394792 0.0116096
0.0267781 0.0568647
0.0199309 0.0607229
0.0322982 0.0646748
0.0248169 0.0119036
0.0803921 0.0711301
0.0174339 0.0494894
0.0870049 0.064626
-0.0267379 0.00224171
0.0582389 0.0543247
0.0317699 0.0624533
0.0920969 0.0654617
0.0140549 0.0548527
0.080382 0.0653946
0.05937 0.0554399
0.0197009 0.0552236
-0.0152583 0.0274328
0.00931426 0.0642039
-0.0631101 0.00555475
-0.0621272 0.000774142
0.129453 0.0013036
0.0390889 0.0112361
0.123121 0.0646924
-0.0144713 0.00171202
0.100097 0.0689205
0.0223968 0.048257
0.0910569 0.0700658
-0.000835377 0.0013407
parameters: [ 9.   1.   2.   1.2  5. ]. error: 32981566917.2.
----------------------------
epoch 0, loss 1.36395
epoch 128, loss 0.646287
epoch 256, loss 0.678261
epoch 384, loss 0.632075
epoch 512, loss 0.704805
epoch 640, loss 0.620069
epoch 768, loss 0.612575
epoch 896, loss 0.661976
epoch 1024, loss 0.553798
epoch 1152, loss 0.731249
epoch 1280, loss 0.62795
epoch 1408, loss 0.651173
epoch 1536, loss 0.463712
epoch 1664, loss 0.615689
epoch 1792, loss 0.755441
epoch 1920, loss 0.850123
epoch 2048, loss 0.559696
epoch 2176, loss 0.578691
epoch 2304, loss 0.618447
epoch 2432, loss 0.754755
epoch 2560, loss 0.596732
epoch 2688, loss 0.550227
epoch 2816, loss 0.797086
epoch 2944, loss 0.646495
epoch 3072, loss 0.525423
epoch 3200, loss 0.588965
epoch 3328, loss 0.504746
epoch 3456, loss 0.782776
epoch 3584, loss 0.50635
epoch 3712, loss 0.621627
epoch 3840, loss 0.55355
epoch 3968, loss 0.508849
epoch 4096, loss 0.756025
epoch 4224, loss 0.520552
epoch 4352, loss 0.6143
epoch 4480, loss 0.794894
epoch 4608, loss 0.711932
epoch 4736, loss 0.540733
epoch 4864, loss 0.837945
epoch 4992, loss 0.597833
epoch 5120, loss 0.597387
epoch 5248, loss 0.559322
epoch 5376, loss 0.73908
epoch 5504, loss 0.475272
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594395 0.0527751
0.0496683 0.0431433
0.00894922 -0.00558321
0.101441 0.054367
0.0305919 0.0497004
-0.0267379 0.0128804
0.0901953 0.0548211
0.129916 0.0693986
0.0204143 -0.0047947
0.0594398 0.0501983
0.0271109 0.00357922
-0.000781954 -0.00559458
0.0251231 0.0716818
0.0927396 0.058264
0.0594396 0.049541
-0.0177216 0.0044339
0.106655 0.0568181
0.0594397 0.0523105
-0.0210969 0.00416013
0.0394802 0.0256777
0.0937752 0.0709606
0.0716711 0.000247599
0.0977802 0.0811653
0.0188597 -0.00294189
0.0496681 0.0554319
-0.0417425 0.00259505
-7.99663e-06 0.0153604
0.0321632 0.05272
0.0113515 0.00359438
0.0594397 0.0460539
0.015257 0.0009158
-0.00553123 0.0564038
0.0978866 0.0788099
0.0983621 0.0607205
-0.0365701 -0.00487224
0.0594396 0.0560749
-0.0394792 0.00445065
0.0927397 0.0607676
0.0317699 0.0415638
0.046218 0.00810474
-0.0631101 0.00165533
-0.0210896 -0.00420578
0.0211183 0.0744073
0.0133907 0.0830897
0.0865811 0.0488631
0.018802 -0.00564069
-0.0734361 -0.00425464
0.0753057 0.0582028
0.0496682 0.0435769
0.0207193 0.0472102
0.000796249 -0.00411029
0.0131861 0.0204386
0.0986864 0.0431783
0.132184 0.0876348
0.0752955 0.0560134
0.0631207 -0.00593623
0.0494968 -0.00386052
0.033718 0.0430517
-0.0267398 0.00781328
0.0414001 -0.0044397
0.0964766 0.0581086
0.0321467 0.0384557
0.0599612 0.0458025
0.0859467 0.0612669
0.0611204 -0.00494085
-0.0144713 -0.00383605
-3.17974e-05 0.0134333
0.0853255 0.0601617
-0.0152606 -0.00593901
0.0189828 0.00320799
0.0594398 0.0469918
0.059435 0.0886638
0.0417484 -0.00505784
0.129916 0.0717726
0.0222151 0.0448792
0.0594399 0.058472
0.105471 0.0893854
0.0283783 0.038755
-0.0300728 -0.00139137
0.0605534 0.0655706
0.0295687 0.0212533
0.0594473 0.0581138
0.0594397 0.0475363
-0.065961 -0.00358764
0.0529729 0.0101977
0.0281393 0.0364442
4.07609e-05 -0.00509229
0.0397476 0.0470591
0.0706156 0.047796
0.0594482 0.084951
0.125883 0.0662472
0.029557 -0.00334019
0.0727411 0.0457133
0.0134225 0.0854603
0.0594398 0.0529261
0.05627 0.056579
0.0220538 0.0508628
-0.0462148 0.000280195
0.0594426 0.0545345
0.0131851 0.00313474
0.0424316 0.0475878
0.0520969 0.0464109
-0.0734426 0.00378395
0.0594397 0.0569308
0.0261394 0.029291
0.0346447 -0.00618006
-0.0144856 -0.00491429
0.0209719 0.00154341
0.0271187 -0.00138339
0.0417583 0.00813882
0.0318693 0.0369125
0.0467944 -0.00358945
0.100097 0.0720935
0.0458116 0.0336597
0.0271115 0.00194207
0.0807987 -0.00359504
0.0373484 0.0422242
0.101441 0.045294
0.0194981 0.00781403
0.0964305 0.0447859
0.0281391 0.0325209
0.0152545 0.00152708
0.0477958 0.0363279
0.0594398 0.0403814
0.0278231 0.0454335
-0.0144836 -0.00467458
-0.00893962 -0.00370869
-0.0152556 -0.00559145
parameters: [ 9.     1.     2.     1.2    5.618]. error: 14257.1250662.
----------------------------
epoch 0, loss 1.01099
epoch 128, loss 0.723876
epoch 256, loss 0.714031
epoch 384, loss 0.60379
epoch 512, loss 0.665486
epoch 640, loss 0.563692
epoch 768, loss 0.802421
epoch 896, loss 0.818604
epoch 1024, loss 0.657801
epoch 1152, loss 0.727074
epoch 1280, loss 0.502507
epoch 1408, loss 0.75307
epoch 1536, loss 0.642217
epoch 1664, loss 0.614896
epoch 1792, loss 0.61479
epoch 1920, loss 0.737218
epoch 2048, loss 0.821828
epoch 2176, loss 0.633519
epoch 2304, loss 0.581746
epoch 2432, loss 0.601081
epoch 2560, loss 0.659157
epoch 2688, loss 0.662786
epoch 2816, loss 0.727162
epoch 2944, loss 0.578969
epoch 3072, loss 0.625904
epoch 3200, loss 0.663511
epoch 3328, loss 0.638938
epoch 3456, loss 0.642865
epoch 3584, loss 0.559479
epoch 3712, loss 0.601
epoch 3840, loss 0.651032
epoch 3968, loss 0.492605
epoch 4096, loss 0.552037
epoch 4224, loss 0.605759
epoch 4352, loss 0.767301
epoch 4480, loss 0.561048
epoch 4608, loss 0.550996
epoch 4736, loss 0.668927
epoch 4864, loss 0.556197
epoch 4992, loss 0.590355
epoch 5120, loss 0.736403
epoch 5248, loss 0.657156
epoch 5376, loss 0.591878
epoch 5504, loss 0.479692
epoch 5632, loss 0.698351
epoch 5760, loss 0.67441
epoch 5888, loss 0.670044
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.109549 0.0595918
-0.0385437 -0.00458377
0.106624 0.0619214
0.032933 0.0365093
-0.0716787 -0.00321622
0.101441 0.0450672
0.0367961 0.0115663
0.0384877 0.0407011
0.0435639 0.061029
0.0605996 0.0599052
0.0310444 0.0355269
-0.0188606 -0.00448669
0.110244 0.0562017
0.0237989 0.0612646
0.0267781 0.0394228
0.0318695 0.0424039
0.0752422 0.056644
0.0210915 -0.00689545
0.000834798 -0.000510293
0.0131851 -0.00562928
0.0109517 -0.00288607
0.020735 -0.00533369
0.0716719 -0.00470258
-0.0248092 -0.00603073
-0.00699273 0.0385461
-0.0340329 -0.0073043
0.0131848 -0.00423405
0.0594374 0.0472303
2.05834e-07 0.00197274
-0.041769 -0.00744615
0.0870049 0.0524852
0.0857183 -0.00376128
-0.0234626 0.0020282
0.00131924 -0.00341063
-0.0414033 -0.00678681
0.0424529 0.0550177
-0.0161248 0.0431227
0.0037846 -0.00624957
0.00250001 -0.00381816
0.109566 0.0676967
0.0247038 0.0331806
0.0237988 0.0496743
0.0582651 0.0503429
0.0527577 0.0560693
0.0131845 0.00330494
-0.0662877 -0.00590473
0.032933 0.0306172
0.106624 0.0683004
-0.0467928 0.00294784
-0.00856301 -0.00367791
0.0131851 0.000116044
-0.0529698 -0.00511697
0.0710838 0.0536156
0.0859465 0.0579458
0.0791961 0.0404228
0.0871098 0.0477038
-0.0188599 -0.0063224
0.0661224 0.0369019
0.0461383 0.0527973
0.0321897 0.0494875
0.0594376 0.0492989
0.0907408 0.0592895
0.0859467 0.0612909
0.0223972 0.0341538
0.0865812 0.0599083
0.0677309 0.0625018
4.57767e-08 -0.0028178
0.034034 -0.00606855
0.100667 0.043289
0.0412702 -0.00665512
0.0964772 0.0465663
-0.0390966 -0.00474393
0.0271117 -0.00451341
1.01848e-06 0.0026639
-0.00484737 0.0414084
0.0907411 0.0660354
-3.76332e-06 0.0108519
0.024704 0.0315089
-0.0188605 -0.00440256
-1.63011e-07 -0.000125478
0.0109586 -0.00585659
0.0907411 0.046386
-0.0468045 0.00521881
0.025123 0.0726781
0.0495796 -0.00355495
0.0642209 0.0566286
0.0659615 -0.00628353
0.00931426 0.0473215
0.0435741 0.0461076
0.0234638 -0.00656973
0.0594349 0.0367791
0.0321466 0.0324689
-0.00142797 0.00285922
0.0417484 0.000460935
0.0327313 0.0550716
-3.91322e-05 -0.00781744
0.0191276 0.0343171
-0.0204001 -0.0043242
-0.0417573 -0.00755567
0.0267443 -0.00665869
0.0261395 0.0658117
0.0321468 0.0484735
0.10955 0.0560325
0.126127 0.0780192
-0.0132848 0.0859242
-0.0209708 -0.00859129
0.0966587 0.0641546
0.0691986 0.0488091
0.0859466 0.0624302
0.0662934 -0.00177455
0.00931421 0.0480551
-0.0106908 -0.00345797
-0.00700262 0.0438959
0.121243 0.0624142
-1.17339e-07 -0.00461804
-0.053257 0.000706264
0.0978866 0.0766216
-0.0247813 -0.00623482
0.0321468 0.0534585
0.088282 0.0519822
0.0901949 0.0434464
0.0861488 0.0646786
-0.0188028 -0.00345923
0.0362233 0.00209202
-0.0189864 0.000842896
-0.0023636 0.062183
-0.00236347 0.0590274
-0.0494977 -0.0052214
parameters: [ 9.   1.   2.   1.2  6. ]. error: 3072818788.77.
----------------------------
epoch 0, loss 0.992293
epoch 128, loss 0.863907
epoch 256, loss 0.813177
epoch 384, loss 0.807234
epoch 512, loss 0.620155
epoch 640, loss 0.795987
epoch 768, loss 0.739737
epoch 896, loss 0.634975
epoch 1024, loss 0.544249
epoch 1152, loss 0.562874
epoch 1280, loss 0.726697
epoch 1408, loss 0.560924
epoch 1536, loss 0.506712
epoch 1664, loss 0.725724
epoch 1792, loss 0.729175
epoch 1920, loss 0.595558
epoch 2048, loss 0.74849
epoch 2176, loss 0.616919
epoch 2304, loss 0.632998
epoch 2432, loss 0.543953
epoch 2560, loss 0.62138
epoch 2688, loss 0.73339
epoch 2816, loss 0.628013
epoch 2944, loss 0.616912
epoch 3072, loss 0.628662
epoch 3200, loss 0.681827
epoch 3328, loss 0.702004
epoch 3456, loss 0.669632
epoch 3584, loss 0.577538
epoch 3712, loss 0.689549
epoch 3840, loss 0.791736
epoch 3968, loss 0.576426
epoch 4096, loss 0.663141
epoch 4224, loss 0.67369
epoch 4352, loss 0.666611
epoch 4480, loss 0.586115
epoch 4608, loss 0.602145
epoch 4736, loss 0.675262
epoch 4864, loss 0.581562
epoch 4992, loss 0.689774
epoch 5120, loss 0.529421
epoch 5248, loss 0.767196
epoch 5376, loss 0.559927
epoch 5504, loss 0.596651
epoch 5632, loss 0.588911
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0819546 0.0589166
0.0907409 0.0534827
0.101441 0.0495448
0.0117847 -0.00501976
0.0593312 0.0396908
0.0691984 0.045696
0.0941706 0.0449773
0.0170933 0.0601768
0.0791969 0.0518443
0.060558 0.0626556
0.0927396 0.0258262
0.0594398 0.0502107
0.0593133 0.0441765
0.0624794 0.0533499
0.088282 0.0397253
0.109566 0.0521883
0.0318693 0.0337399
0.0599029 0.0494802
-0.00236347 0.0374003
0.0117782 0.00459261
0.0271109 -3.61345e-05
-0.0611267 -0.00124678
0.00131924 -0.00518212
0.0966587 0.0515406
0.080402 0.0622023
-0.0346352 -0.00534363
0.0495768 0.000587088
0.0871096 0.0630069
-0.014471 -0.0012863
-0.0611267 -0.00372619
0.0861486 0.060464
0.0467944 -0.00145369
0.0734439 -0.00537001
0.0462066 -0.00427719
0.0174339 0.0438654
0.110244 0.0674637
0.000796249 0.013061
-0.0144713 -0.00481748
0.0267781 0.0238317
0.13219 0.0902458
0.0991788 0.0566399
0.0710835 0.0450745
0.0710835 0.0566347
0.0188086 -0.00517585
0.093775 0.0809501
0.126086 0.0783237
0.0407515 0.0326869
0.0337179 0.044208
0.126086 0.0570449
0.0197008 0.0450021
0.0791268 0.053442
0.0594325 0.0546138
-0.0272977 -0.005482
0.0461383 0.031046
0.110244 0.0450069
-0.014471 0.00423737
0.0412725 -0.00463278
-0.00131975 -0.00502246
-0.00893962 -0.00520997
-0.0109489 0.0139777
0.0594395 0.0657742
0.0986864 0.0370561
0.0927399 0.0717778
0.01134 -0.0053283
0.0373484 0.0314864
0.0271187 -0.00501043
0.0966596 0.0552802
0.0477959 0.0315727
0.0861488 0.0604631
0.029557 0.00358714
0.0461383 0.0374622
0.0594395 0.05781
0.106645 0.0520307
0.0497673 0.0462968
0.0286789 0.0300766
0.101441 0.0555932
0.0865811 0.0637311
0.0707072 0.00450794
-3.91322e-05 0.00157708
0.0199309 0.0681162
0.0594399 0.0610455
0.0594396 0.0342829
0.109566 0.0550994
-0.0394792 0.00258236
-0.00378297 -0.00456523
0.05943 0.053459
0.0286789 0.0287631
-0.00378521 -0.00485037
0.0113491 -0.00509839
0.0997468 0.0311307
0.0482639 0.0560796
0.0188023 -0.00277478
0.0373484 0.0473047
0.0248169 0.00441852
0.101441 0.051466
0.0776735 0.000723791
-0.00700272 0.0252848
-0.0204122 -0.00485444
-0.0417624 -0.00498758
-0.00733491 0.0576959
0.125873 0.0596279
0.0859465 0.0309646
0.126222 0.0625748
0.059435 0.0555608
0.0739227 0.0714038
-0.00553123 0.0510021
0.0730676 0.0400674
4.57767e-08 -5.33081e-05
0.0335493 0.0482767
0.0815268 0.0317083
-0.0152533 -0.0046954
0.0541227 0.055729
0.0385291 -0.00432134
0.0606162 0.0570369
0.0290868 0.0581597
0.100097 0.0735487
0.0997468 0.0560451
0.062595 0.046475
-0.0204001 -0.00285288
0.0661223 0.046708
0.0642209 0.0656072
0.0261395 0.0384871
0.0362237 0.00533913
-0.000801651 -0.00503388
0.0710838 0.062746
0.000782366 0.0108383
0.0594301 0.0610888
0.0977799 0.0773575
parameters: [ 9.     1.     2.     1.2    5.744]. error: 1203.91865399.
----------------------------
epoch 0, loss 1.16276
epoch 128, loss 0.912482
epoch 256, loss 1.17577
epoch 384, loss 0.926061
epoch 512, loss 0.680529
epoch 640, loss 0.610908
epoch 768, loss 0.846274
epoch 896, loss 0.765237
epoch 1024, loss 0.69692
epoch 1152, loss 0.74161
epoch 1280, loss 0.759357
epoch 1408, loss 0.705043
epoch 1536, loss 0.663267
epoch 1664, loss 0.80707
epoch 1792, loss 0.73275
epoch 1920, loss 0.675166
epoch 2048, loss 0.74673
epoch 2176, loss 0.622455
epoch 2304, loss 0.694779
epoch 2432, loss 0.733536
epoch 2560, loss 0.564154
epoch 2688, loss 0.614967
epoch 2816, loss 0.613636
epoch 2944, loss 0.687764
epoch 3072, loss 0.849293
epoch 3200, loss 0.871708
epoch 3328, loss 0.750995
epoch 3456, loss 0.649914
epoch 3584, loss 0.584027
epoch 3712, loss 0.698546
epoch 3840, loss 0.794151
epoch 3968, loss 0.554314
epoch 4096, loss 0.637159
epoch 4224, loss 0.800533
epoch 4352, loss 0.558828
epoch 4480, loss 0.448573
epoch 4608, loss 0.672117
epoch 4736, loss 0.647423
epoch 4864, loss 0.537487
epoch 4992, loss 0.631642
epoch 5120, loss 0.64871
epoch 5248, loss 0.70812
epoch 5376, loss 0.557335
epoch 5504, loss 0.393922
epoch 5632, loss 0.60017
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
3.5013e-07 -0.00423842
0.0174335 0.0638452
0.0385437 0.0113719
0.0983621 0.053389
0.0283785 0.0559949
0.043921 0.0569257
0.0977801 0.0733293
0.0730674 0.0622091
-0.00891927 0.0170444
3.40452e-06 0.0216142
0.0134225 0.0732338
0.060558 0.065244
0.0739227 0.0702839
0.0397474 0.0558686
-0.0113373 0.00424571
-0.0189824 -0.00334361
0.0317698 0.0679085
-0.0248069 -0.0048458
-0.00249422 0.00367992
-0.0210969 -0.00381191
0.0243594 0.0611863
0.129916 0.06493
0.0589763 0.062144
0.0152616 -0.0068862
0.0237986 0.066196
0.0416418 0.0120079
0.0853245 0.0677993
0.0529686 -0.00360921
0.121243 0.0614365
0.118684 0.0723081
-0.0267379 -0.00522807
0.0396776 0.060709
0.101814 0.0743963
0.059407 0.0611543
0.0927396 0.0577284
-0.0295746 -0.00495154
0.0272894 -0.00520585
0.0594397 0.0677307
-0.0412711 -0.0060362
-0.0271121 -0.00435178
0.0562597 0.0599596
-0.00142797 -0.00459799
0.0860428 -0.00426434
0.0188597 -0.00518684
0.0412702 -0.00442962
0.130005 0.0683989
0.059435 0.0687706
0.0201879 0.0547689
0.0882823 0.0648487
0.0333262 0.0675981
0.0706155 0.0701598
-0.0413988 0.00867557
0.0867067 0.0656597
0.0234681 -0.00586427
0.0318693 0.0622321
-0.0326435 -0.00595295
-0.0385427 0.00691436
0.00863029 0.0611504
-3.91322e-05 -0.00525027
0.0887289 0.0689485
0.0373484 0.0668857
0.059367 0.0649711
0.0319943 0.0671391
0.0327313 0.0549024
0.081797 0.0636878
-0.0272904 -0.00478113
-0.0323135 -0.000277202
0.0477956 0.0640087
0.13219 0.0850631
0.0220537 0.0580257
0.0286789 0.0559567
0.000834782 -0.00498581
0.088729 0.0681618
0.124354 0.0703412
0.0716781 0.00218153
0.0210915 -0.00489809
0.0188086 0.00265728
0.0977799 0.0734968
-0.0177216 0.00733923
0.12614 0.0695017
0.0237988 0.0517865
0.0283783 0.0611838
0.0589765 0.0687554
0.0449618 -0.00442571
0.0133878 0.0750688
-0.013282 0.0785231
0.118684 0.0753638
0.0197008 0.0592579
0.0271289 -0.00287471
0.0605534 0.0654937
-0.0207292 -0.00484122
-0.0707068 -0.00344628
-0.00236348 0.0440468
0.0867333 0.0711545
0.0384777 0.0636001
0.0870049 0.0554299
0.0594494 0.0758682
0.0416484 0.0281149
0.0867333 0.0657214
3.40452e-06 -0.00304194
0.0859466 0.0674914
-0.053257 -0.0049182
0.0562597 0.065674
-0.00553123 0.0600669
0.126171 0.0653814
0.0182076 0.0677045
-0.0267425 0.0100625
0.100097 0.0733075
0.0424581 0.0645061
0.0177216 -0.00589104
0.00931426 0.0599586
0.0964772 0.0584044
0.0327314 0.055589
-0.036223 -0.00512765
0.0330948 0.0603925
0.061289 0.0738798
0.0211185 0.0708392
0.0384777 0.0560272
0.0870049 0.0599561
0.0373484 0.0571427
0.0631173 -0.00495632
0.0859467 0.0667896
0.102034 0.0623693
0.0477959 0.0593525
0.110244 0.0590048
0.0871095 0.0716135
-0.0271153 -0.00440218
0.018802 0.000444931
parameters: [ 9.     1.     2.     1.2    5.681]. error: 32116284.0335.
----------------------------
epoch 0, loss 0.833823
epoch 128, loss 0.967248
epoch 256, loss 0.667258
epoch 384, loss 0.789018
epoch 512, loss 0.702477
epoch 640, loss 0.751395
epoch 768, loss 0.759875
epoch 896, loss 0.737641
epoch 1024, loss 0.85003
epoch 1152, loss 0.797752
epoch 1280, loss 0.57103
epoch 1408, loss 0.667629
epoch 1536, loss 0.567048
epoch 1664, loss 0.671162
epoch 1792, loss 0.690886
epoch 1920, loss 0.603563
epoch 2048, loss 0.789357
epoch 2176, loss 0.598203
epoch 2304, loss 0.67494
epoch 2432, loss 0.600962
epoch 2560, loss 0.625995
epoch 2688, loss 0.568864
epoch 2816, loss 0.610389
epoch 2944, loss 0.610486
epoch 3072, loss 0.731679
epoch 3200, loss 0.721288
epoch 3328, loss 0.673439
epoch 3456, loss 0.571713
epoch 3584, loss 0.521042
epoch 3712, loss 0.556019
epoch 3840, loss 0.596245
epoch 3968, loss 0.659249
epoch 4096, loss 0.558087
epoch 4224, loss 0.61446
epoch 4352, loss 0.575186
epoch 4480, loss 0.567686
epoch 4608, loss 0.552786
epoch 4736, loss 0.586844
epoch 4864, loss 0.599885
epoch 4992, loss 0.554161
epoch 5120, loss 0.512546
epoch 5248, loss 0.589927
epoch 5376, loss 0.711753
epoch 5504, loss 0.650943
epoch 5632, loss 0.648624
epoch 5760, loss 0.704617
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0170933 0.059963
0.0152616 -0.00945489
0.0210212 0.0672866
0.106634 0.059719
0.0594396 0.0561847
0.0234681 -0.00767262
0.0594397 0.0593558
-0.0492483 -0.000477332
0.0267443 -0.00611538
0.0222151 0.034258
-2.99285e-06 0.00801039
-0.02083 -0.00131937
0.0691986 0.057027
0.0362233 -0.00663414
0.0791969 0.0474637
0.0236481 0.0473009
-0.0207367 -0.00885223
0.0594162 0.0538955
0.0989678 0.0684738
0.0492512 -0.00185949
0.0122295 0.0433143
-0.129453 -0.000219549
0.0189892 0.00307096
0.0134334 0.0716403
-0.0132904 0.0691739
0.0323163 0.000288055
0.0604514 0.061901
0.0199309 0.0585047
-0.000801651 0.00287019
-0.0716782 -0.00451879
-0.049248 -0.00945018
0.0707072 0.0033668
0.0396778 0.0488781
0.0267779 0.0494463
0.0593312 0.0473067
-0.0113401 -0.00828101
0.0326469 7.01294e-05
0.0199307 0.0641542
0.0621339 -0.00853125
0.0594379 0.0727775
-0.00142113 -0.00901379
0.0594398 0.0548765
0.0594324 0.051737
0.12945 -0.00639117
0.121243 0.0567974
-3.11621e-07 -0.00499007
-0.0318771 0.00657468
0.0271163 -0.00814995
-0.0106908 -0.00405856
0.0594397 0.0606928
0.0317697 0.0591938
-0.036223 -0.00683419
0.0407515 0.0469267
0.0458115 0.0552571
-0.0152556 -0.0097458
0.100097 0.0660115
0.0813321 0.0528073
0.0764118 0.0531217
0.0496683 0.0546686
0.081527 0.0516899
0.0950804 0.0502921
0.0243594 0.0559405
0.059435 0.056544
0.0716773 -0.00083316
0.0188088 -0.00182572
0.088282 0.0537069
0.0243594 0.0517391
0.0734322 -0.00773257
0.0192074 0.0516409
0.0661224 0.0604378
-0.0390898 -0.00959438
-0.0113495 -0.00811533
0.0281391 0.0463283
0.0910567 0.0639531
-0.0417573 -0.00991931
0.0268143 0.0484496
-0.00460347 -0.00772644
0.0290868 0.0622368
0.0904958 0.0513693
0.0133878 0.0719781
0.046218 -0.00879852
0.0800821 0.0595408
0.0223972 0.0518508
0.088729 0.0635762
0.0624794 0.0575327
0.0724741 0.0501462
0.0416434 -0.00845767
-0.00731719 0.0578324
0.0966595 0.0451733
0.101441 0.0532364
-0.0161473 0.0463931
0.0144859 0.00191232
0.0593346 0.0523594
0.0273016 -0.00684122
0.000213748 0.0536785
0.0971043 0.0678435
0.0417489 -0.00157145
0.0752955 0.0503185
0.0248197 0.000336916
0.0251233 0.0644827
0.0271187 -0.00183921
0.0189868 0.00259506
0.0497673 0.0468165
0.0346348 -0.00450374
0.0205116 0.0467196
-0.0412731 0.000748847
-0.0295823 -0.00842646
0.0593665 0.0524065
0.0594325 0.0590657
0.0594452 0.073738
0.0396778 0.0461098
0.0424581 0.0591348
0.0594473 0.05522
0.0860428 -0.00662414
0.0327313 0.057187
0.0859466 0.0619859
-2.23337e-05 0.00955448
-0.0385433 -0.0019796
0.0248076 0.00169654
0.0310444 0.0489347
-0.000805986 0.00109181
0.132184 0.0793709
-0.0118767 0.0470264
0.049499 0.0017586
0.0907411 0.0645793
1.01848e-06 0.00180494
-0.0207367 -0.00821096
-0.0210969 0.00187787
parameters: [ 9.     1.     2.     1.2    5.841]. error: 324572967.932.
----------------------------
epoch 0, loss 0.918713
epoch 128, loss 0.748125
epoch 256, loss 0.601962
epoch 384, loss 0.575599
epoch 512, loss 0.684159
epoch 640, loss 0.743829
epoch 768, loss 0.613736
epoch 896, loss 0.779381
epoch 1024, loss 0.696065
epoch 1152, loss 0.759998
epoch 1280, loss 0.672035
epoch 1408, loss 0.625984
epoch 1536, loss 0.472991
epoch 1664, loss 0.813612
epoch 1792, loss 0.743114
epoch 1920, loss 0.627614
epoch 2048, loss 0.697367
epoch 2176, loss 0.567458
epoch 2304, loss 0.631347
epoch 2432, loss 0.540559
epoch 2560, loss 0.615788
epoch 2688, loss 0.691859
epoch 2816, loss 0.649443
epoch 2944, loss 0.698709
epoch 3072, loss 0.61727
epoch 3200, loss 0.59241
epoch 3328, loss 0.634995
epoch 3456, loss 0.6561
epoch 3584, loss 0.459718
epoch 3712, loss 0.742855
epoch 3840, loss 0.591319
epoch 3968, loss 0.563996
epoch 4096, loss 0.588677
epoch 4224, loss 0.547898
epoch 4352, loss 0.570048
epoch 4480, loss 0.666027
epoch 4608, loss 0.621502
epoch 4736, loss 0.575243
epoch 4864, loss 0.56345
epoch 4992, loss 0.553247
epoch 5120, loss 0.582657
epoch 5248, loss 0.626666
epoch 5376, loss 0.584966
epoch 5504, loss 0.491101
epoch 5632, loss 0.6073
epoch 5760, loss 0.567546
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.000796249 0.0005041
-0.00378521 0.000346904
-0.0118767 0.0706778
-0.0326435 -0.000445197
0.0496683 0.0700456
0.044975 0.0699213
0.0920561 0.0587341
0.0396776 0.0525124
-0.00701261 0.0537875
-0.0827278 0.000712138
0.0691072 0.0661206
0.0594449 0.0513968
-0.129457 0.00351976
0.0207377 0.000132066
0.0416418 0.023543
0.0599031 0.0597155
0.0901953 0.0591544
-0.0346424 0.00112008
0.0272894 -0.00105856
0.125883 0.0652648
0.01684 0.0406648
-0.0860447 -0.000544882
0.0327313 0.0641337
-0.0734361 -0.000876437
0.0301505 0.0707928
0.0281391 0.0438684
0.0593665 0.0580297
0.0117847 0.00665357
0.0131849 0.00688185
0.0318766 0.0152256
0.0326439 0.00282117
0.0243594 0.0599739
0.0594562 0.0507923
0.0977799 0.078696
0.0631207 0.00258237
0.000782366 0.00590526
0.0435639 0.071044
0.0870049 0.0558499
-0.0118764 0.0478402
0.0594349 0.0644623
0.0871095 0.0479479
0.126127 0.0596126
0.0318693 0.0539034
0.0321468 0.0511585
-0.0857218 0.00041527
0.00250001 -0.00028007
-0.0532643 0.00627117
-0.0416423 -0.00148987
0.0417484 0.00791668
0.0295687 0.0405786
-0.0131855 0.044767
0.0174335 0.050178
0.0414075 -0.00216289
0.124277 0.0694311
0.0174339 0.0429402
-0.0272977 0.00265375
-0.0462049 0.0215405
0.0317697 0.0497388
0.0495801 0.00316019
-3.11621e-07 0.0196466
-0.0611272 0.00979622
-0.0161572 0.0548555
0.0295774 0.0130689
0.125873 0.0558597
0.0321468 0.0769312
0.0594482 0.0797434
0.000796249 0.00906651
0.0871098 0.0626792
0.0859467 0.0513683
0.0815268 0.0540403
0.0594301 0.0623272
0.0329332 0.0761697
-0.0412685 0.00681908
0.0468064 -0.000832523
-1.63011e-07 0.000572518
-0.0716787 -0.000633485
-0.0495734 -0.0008509
-0.000792737 -0.000324245
0.0594397 0.0630005
0.062595 0.0730918
0.080392 0.0730044
0.0859467 0.0650026
0.036794 0.0535296
0.0920613 0.066594
0.130756 0.0496781
0.0907408 0.0674118
0.0215366 0.0143061
0.0327313 0.0522808
0.00596587 0.0737979
0.0497671 0.0510286
0.0859466 0.06764
0.0710835 0.0496047
0.0594374 0.0603414
0.125146 0.0770468
0.0321467 0.0744712
0.0436381 0.0657015
0.0727409 0.0535421
0.0390895 0.000229561
-0.129457 -0.000775538
-0.0467978 0.0389172
-0.0611177 0.00598825
0.0323166 0.0207392
-0.0204101 0.0245433
0.0267781 0.0616632
0.0907409 0.0690188
0.0851609 0.078649
0.0594398 0.0558039
-0.0194897 0.0200005
0.0224432 0.0337012
0.0594397 0.0685693
0.129453 0.00142597
0.0599031 0.0519413
0.0871098 0.0707835
0.0868851 0.0800916
-0.0117788 0.00176657
0.0860523 0.0374229
0.0449639 0.0106906
0.076422 0.05716
-0.00893962 -0.000222857
-0.00733491 0.0586832
0.022444 0.0680811
0.061289 0.0797121
0.0859962 0.0692046
-0.0385427 0.0429882
-0.0152583 0.00114691
0.043921 0.0634841
-0.0734361 0.00792553
0.0329331 0.0510591
parameters: [ 9.     1.     2.     1.2    5.781]. error: 3788.9859148.
----------------------------
epoch 0, loss 0.937569
epoch 128, loss 0.917039
epoch 256, loss 0.981467
epoch 384, loss 0.852254
epoch 512, loss 0.606933
epoch 640, loss 0.658085
epoch 768, loss 0.765081
epoch 896, loss 0.585538
epoch 1024, loss 0.695069
epoch 1152, loss 0.571682
epoch 1280, loss 0.717709
epoch 1408, loss 0.737105
epoch 1536, loss 0.672537
epoch 1664, loss 0.661111
epoch 1792, loss 0.603842
epoch 1920, loss 0.58917
epoch 2048, loss 0.643209
epoch 2176, loss 0.785703
epoch 2304, loss 0.737317
epoch 2432, loss 0.710931
epoch 2560, loss 0.645828
epoch 2688, loss 0.580373
epoch 2816, loss 0.676063
epoch 2944, loss 0.624426
epoch 3072, loss 0.558579
epoch 3200, loss 0.620171
epoch 3328, loss 0.76896
epoch 3456, loss 0.468817
epoch 3584, loss 0.597956
epoch 3712, loss 0.6091
epoch 3840, loss 0.599168
epoch 3968, loss 0.592219
epoch 4096, loss 0.746764
epoch 4224, loss 0.808157
epoch 4352, loss 0.600611
epoch 4480, loss 0.707235
epoch 4608, loss 0.61307
epoch 4736, loss 0.751693
epoch 4864, loss 0.485099
epoch 4992, loss 0.595571
epoch 5120, loss 0.603536
epoch 5248, loss 0.705077
epoch 5376, loss 0.726904
epoch 5504, loss 0.722844
epoch 5632, loss 0.620521
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0317697 0.058167
0.0867333 0.0601881
0.0462156 -0.00255596
0.0251233 0.0680937
0.0449618 -0.00348837
0.0122352 0.0589337
0.0861488 0.0615407
0.0286795 0.0566268
-0.0267352 -0.00181634
0.0384877 0.0630703
0.027823 0.0584054
0.022215 0.0536172
-0.0271193 0.00322175
1.01848e-06 0.00353359
0.033718 0.0605341
0.0857788 0.0601608
2.05834e-07 0.00711215
0.0605996 0.0589448
0.132184 0.0749368
0.000802697 -0.00173695
0.0397476 0.0570982
2.05834e-07 0.00649091
0.0594397 0.0584266
0.0477956 0.0574959
-0.041769 -0.00308473
0.0691068 0.0575982
0.0920561 0.0624123
0.0237988 0.0553132
0.0800821 0.0582234
0.0662934 -0.00302941
0.0330953 0.0567696
0.0594396 0.0584677
0.0621334 -0.00347466
0.0857183 -0.000106632
0.027823 0.0579071
0.0707066 0.0129104
0.0594397 0.0583391
0.0373475 0.059451
0.0207304 0.00724928
0.0701141 0.0616204
0.0907411 0.0622937
-0.0118767 0.055899
0.0477956 0.0574549
0.0606162 0.0569637
-0.00894944 0.00530488
-0.0495799 -0.0010698
0.0321632 0.0589323
0.0281392 0.0616561
0.0981549 0.0592904
-0.0362226 0.00461107
0.0989677 0.0684332
-0.041769 0.00614296
-0.00733491 0.0590025
0.0589765 0.0603512
0.0236479 0.0554192
0.0300726 -0.00169857
0.0368959 0.0557101
0.0631207 -0.00265713
0.0673482 0.0598719
-0.0132904 0.0731328
-1.48505e-07 -0.00152307
0.00131949 -0.00213172
-0.129445 0.00446138
0.0122142 0.0603669
0.044975 0.0677177
-0.0394792 0.00528484
3.40452e-06 0.00107327
0.0594397 0.0630532
0.0373484 0.0560465
0.100097 0.0681732
0.0373475 0.0570582
0.0188603 -0.00314027
0.0594467 0.075728
0.0791961 0.0597599
0.0362233 0.00709343
0.0989677 0.0697236
-0.00701261 0.0564352
0.0907409 0.063763
0.0113515 0.00555268
0.043921 0.0522184
0.0734394 0.00734488
0.0819546 0.0607103
0.0855722 0.0694013
0.0220537 0.0603795
0.0321466 0.0625008
0.0594399 0.0602459
0.0989677 0.0692464
0.0594374 0.0597584
-2.99285e-06 0.008571
0.0333262 0.0677272
0.01684 0.0549291
0.0716711 -0.00299109
0.0582236 0.0579072
-0.0295676 0.00188902
0.0495768 -0.00439076
0.0859962 0.066869
-0.032317 0.00144838
0.0781223 0.0613365
0.059435 0.0607822
0.0621334 -0.00192434
0.0611204 -0.00133955
0.0321466 0.0625544
0.0724734 0.0553673
0.0691068 0.0593122
0.0730674 0.0597885
0.110244 0.0612774
0.0783558 0.0626873
-1.99995e-06 0.00908858
0.0333262 0.0675718
0.0594297 0.0601912
0.0223972 0.0525316
0.0727412 0.0623468
0.0272894 0.00502049
0.0596272 0.0579412
0.0329332 0.0590565
0.0594396 0.0615822
0.130005 0.062944
0.0197009 0.0589921
0.0477959 0.0575331
0.101814 0.0688754
0.0365698 0.0113872
0.0594379 0.0750536
-0.00378521 0.00650807
0.0319944 0.0598023
0.100097 0.0672732
0.0106905 0.0026134
0.0416368 -0.00307897
0.0991789 0.0633829
parameters: [ 9.     1.     2.     1.2    5.726]. error: 2219279360.2.
----------------------------
epoch 0, loss 0.845581
epoch 128, loss 0.679804
epoch 256, loss 0.752102
epoch 384, loss 0.659435
epoch 512, loss 0.525113
epoch 640, loss 0.702011
epoch 768, loss 0.655299
epoch 896, loss 0.624474
epoch 1024, loss 0.662187
epoch 1152, loss 0.528022
epoch 1280, loss 0.723875
epoch 1408, loss 0.682275
epoch 1536, loss 0.574041
epoch 1664, loss 0.57262
epoch 1792, loss 0.931364
epoch 1920, loss 0.716578
epoch 2048, loss 0.542589
epoch 2176, loss 0.5404
epoch 2304, loss 0.662447
epoch 2432, loss 0.739097
epoch 2560, loss 0.658444
epoch 2688, loss 0.599838
epoch 2816, loss 0.571026
epoch 2944, loss 0.57968
epoch 3072, loss 0.711757
epoch 3200, loss 0.684121
epoch 3328, loss 0.713029
epoch 3456, loss 0.679962
epoch 3584, loss 0.664571
epoch 3712, loss 0.541425
epoch 3840, loss 0.619977
epoch 3968, loss 0.756791
epoch 4096, loss 0.744692
epoch 4224, loss 0.649587
epoch 4352, loss 0.724723
epoch 4480, loss 0.588982
epoch 4608, loss 0.468163
epoch 4736, loss 0.596546
epoch 4864, loss 0.65248
epoch 4992, loss 0.630381
epoch 5120, loss 0.571513
epoch 5248, loss 0.662131
epoch 5376, loss 0.620032
epoch 5504, loss 0.737253
epoch 5632, loss 0.546879
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0727411 0.0611368
0.0417702 0.00588639
0.0131849 -0.00161401
-0.00236347 0.0538179
0.060558 0.0618316
0.0271115 -0.00358926
-0.0215356 -0.00287738
0.13219 0.0854316
0.0435639 0.0579266
0.121243 0.0672665
0.088729 0.0634936
0.0261391 0.0500678
0.0199307 0.0657251
-3.11621e-07 -0.00173099
-0.0492483 0.00539232
0.0867334 0.0571105
0.059181 0.0636981
0.0290868 0.0631332
0.126169 0.0703275
0.0301504 0.0582026
-0.0662833 -0.00328053
0.0868848 0.0594713
-0.0118764 0.0550322
0.0867335 0.06554
0.0594297 0.0620057
0.124311 0.0748719
0.133575 0.0709762
0.0631163 -0.000725712
0.0333262 0.0642142
0.0305923 0.0549264
3.5013e-07 -0.00391951
0.0996717 0.0701609
0.0594398 0.0562137
0.0910567 0.0590668
0.0730674 0.059826
0.033718 0.0594055
0.0477957 0.0568914
-0.02083 -0.00140439
0.0599612 0.0503117
0.0582589 0.0603978
-0.0131855 0.00370282
0.0317699 0.0474724
0.0317699 0.0529767
0.032933 0.0632118
0.0594251 0.0591636
0.0594396 0.062825
0.135027 0.0676653
0.027823 0.0506679
-0.0621274 0.00389086
0.0208245 0.00582254
0.143948 0.0648659
-0.0857226 -0.00383286
0.124311 0.0742713
0.0978866 0.0778334
0.0594116 0.0616893
0.0594398 0.0665472
-0.0529659 0.0159084
0.0301504 0.055087
-0.0390966 0.00466453
0.132184 0.0853773
0.00931421 0.0446007
1.01848e-06 -0.000482748
3.82209e-05 -0.000486742
0.0318695 0.0477226
0.0631196 0.00700894
-7.68032e-07 -0.00192649
0.0319945 0.0489618
0.043628 0.0581028
-0.0611267 -0.0021521
0.0706156 0.070692
0.0323166 -0.00284295
0.0407515 0.0559524
-0.0468045 -0.00294621
0.0813269 0.063907
-0.0215356 -0.00179746
0.0247829 0.00909247
-0.0467928 -0.000764179
8.40829e-06 0.00529305
0.028139 0.0506141
0.0527575 0.0569229
0.0857788 0.0593969
0.0706156 0.0673539
0.0910569 0.0638884
-0.0144713 -0.00175214
0.0209836 -0.00249691
0.0417583 -0.00191927
0.0897923 0.0687099
0.0621302 -0.00277946
0.0724741 0.0584586
0.014471 0.00615218
-0.0416468 -0.00107481
0.0131851 -0.00283881
0.0621334 0.00152955
-0.0807993 0.00454457
-0.00460347 0.00616033
0.022444 0.0556673
0.0271204 0.0167044
0.0321467 0.0509914
0.0865812 0.0623729
0.0329331 0.0523991
-0.0707068 0.00880054
0.0368964 0.0584832
0.093775 0.0775491
0.100668 0.0556587
0.0727409 0.0623541
0.0268143 0.056899
0.0861486 0.0628732
-0.0394796 0.000785106
0.0152591 -0.0024708
0.093775 0.075771
4.17498e-06 0.00488642
0.080402 0.0699772
0.125863 0.0668757
0.0594398 0.0650474
0.0594349 0.0625297
0.0131845 -0.00140037
1.01848e-06 0.0045833
0.090741 0.0710617
0.0496681 0.0629898
0.0594027 0.0645666
0.0327312 0.0586496
0.0593758 0.0605304
0.0594297 0.0598801
0.125157 0.0593703
0.0541426 0.0623784
0.109566 0.0677906
-0.0394792 -0.00138469
-0.0267398 0.0139669
parameters: [ 9.     1.     2.     1.2    5.761]. error: 22997148.6718.
----------------------------
epoch 0, loss 1.43951
epoch 128, loss 1.08753
epoch 256, loss 0.731995
epoch 384, loss 0.817465
epoch 512, loss 0.602577
epoch 640, loss 0.593672
epoch 768, loss 0.820316
epoch 896, loss 0.588033
epoch 1024, loss 0.591826
epoch 1152, loss 0.628057
epoch 1280, loss 0.670951
epoch 1408, loss 0.717293
epoch 1536, loss 0.615146
epoch 1664, loss 0.595393
epoch 1792, loss 0.746268
epoch 1920, loss 0.658184
epoch 2048, loss 0.59663
epoch 2176, loss 0.657918
epoch 2304, loss 0.664359
epoch 2432, loss 0.626079
epoch 2560, loss 0.563166
epoch 2688, loss 0.48623
epoch 2816, loss 0.613577
epoch 2944, loss 0.648731
epoch 3072, loss 0.791339
epoch 3200, loss 0.751216
epoch 3328, loss 0.715658
epoch 3456, loss 0.594658
epoch 3584, loss 0.632936
epoch 3712, loss 0.482114
epoch 3840, loss 0.681479
epoch 3968, loss 0.609857
epoch 4096, loss 0.666444
epoch 4224, loss 0.693158
epoch 4352, loss 0.71125
epoch 4480, loss 0.749641
epoch 4608, loss 0.737395
epoch 4736, loss 0.5294
epoch 4864, loss 0.658321
epoch 4992, loss 0.764963
epoch 5120, loss 0.785358
epoch 5248, loss 0.609129
epoch 5376, loss 0.644271
epoch 5504, loss 0.556512
epoch 5632, loss 0.548608
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.00863032 0.0472096
0.0223968 0.0453471
-0.0827278 0.000838102
0.0964772 0.0495114
0.0317696 0.0532136
0.0131851 0.00506443
0.0865812 0.0598422
0.0496682 0.0531701
0.05627 0.0600986
0.0319945 0.0483087
0.121243 0.0577269
-0.0131841 0.00605838
-0.0631103 -0.00218583
-0.00131975 -0.0014061
-0.053266 -0.00579863
0.0482639 0.0526677
0.124354 0.0620894
0.0191276 0.0540215
0.00932421 0.0493419
-0.0621274 -0.00615967
0.0414025 -0.00390464
-0.0807974 0.00146245
0.0691068 0.0553326
6.39156e-06 -0.00337069
0.0217931 0.0638409
0.0857183 -0.00502971
1.13647e-06 -0.00356409
0.0764428 0.060747
0.0594324 0.0590602
0.0211183 0.0675446
0.086885 0.0557263
-0.0412711 -0.00574299
0.106624 0.0619918
-0.00142113 -0.00457509
0.0326469 -0.00515513
0.0562597 0.0581849
0.0271163 0.00350789
0.0594398 0.0562103
-0.0117788 -0.00254894
0.00596546 0.0658448
-0.0209708 0.00232281
0.0582589 0.0585996
0.023648 0.0527597
0.0853245 0.0504685
0.0710837 0.0483398
0.0319943 0.0504527
-0.000792737 0.00430195
-0.0267352 0.0012775
0.0188019 0.0639331
0.060558 0.0563614
0.0204045 0.00376709
0.0464004 0.0497158
0.0594376 0.058975
0.110244 0.0573998
0.0385437 0.0044288
-0.0194897 0.0020885
0.05627 0.056892
0.0210209 0.0676564
0.0920961 0.0526686
0.0807983 0.00188331
0.0594397 0.056875
0.0210209 0.0668625
0.100115 -0.00161895
0.0144707 0.00441973
-0.0210947 0.001086
0.0207377 -0.00534369
0.124311 0.0595579
0.0327313 0.0528714
0.0131849 -0.00530822
0.0321632 0.059691
-4.71368e-07 0.0057602
-2.95455e-07 -0.00398683
0.0562849 0.0548496
0.0867333 0.0555012
0.0318766 -0.00491536
0.0594396 0.0530548
0.0496682 0.0504011
-0.0118768 0.0451534
0.0997468 0.0503631
-0.000792737 0.000905336
0.124311 0.0620548
0.0945151 0.0495632
-0.063123 -0.00338685
0.0594397 0.0604386
-0.0857218 -0.00222917
-0.0662867 -0.00618629
0.038792 0.0535206
0.0865811 0.0592121
0.0204119 -0.0005799
-0.000835392 -0.0031159
0.000796249 0.00442236
0.0424369 0.0575923
0.0461384 0.0548178
0.0209719 -0.00567351
0.132178 0.0752928
-0.0807986 0.00349279
0.046218 -0.00528068
-0.0023637 0.0485518
0.0920561 0.0597844
0.0592543 0.0561465
0.0191272 0.044455
0.125873 0.0636686
-0.0194897 -0.00111935
0.0868848 0.0593884
0.00863032 0.0502179
0.0327315 0.0564174
0.0131858 0.00256842
-0.0467928 -0.00423165
-0.0716782 -0.00564741
0.0323166 -0.00248025
0.0594396 0.0571401
0.0281391 0.0570643
-0.0234744 0.00283565
0.0945038 0.0530502
0.0407515 0.0528968
0.0468012 0.00223534
0.124311 0.0620548
0.0730674 0.0566855
0.100115 -0.00242733
0.0882823 0.0552557
-0.0394792 -0.00368409
-0.0611177 -0.00583931
0.0677309 0.0545165
0.0385291 -0.00402889
0.0333261 0.0657275
-0.041769 -0.00520427
0.022444 0.0503635
0.00459996 -0.00619914
parameters: [ 9.     1.     2.     1.2    5.744]. error: 211515419.522.
----------------------------
epoch 0, loss 1.1024
epoch 128, loss 0.822169
epoch 256, loss 0.916487
epoch 384, loss 0.886795
epoch 512, loss 0.622047
epoch 640, loss 0.759541
epoch 768, loss 0.738979
epoch 896, loss 0.617421
epoch 1024, loss 0.725512
epoch 1152, loss 0.867095
epoch 1280, loss 0.680538
epoch 1408, loss 0.924352
epoch 1536, loss 0.714964
epoch 1664, loss 0.66377
epoch 1792, loss 0.737891
epoch 1920, loss 0.756345
epoch 2048, loss 0.604413
epoch 2176, loss 0.666578
epoch 2304, loss 0.44378
epoch 2432, loss 0.685168
epoch 2560, loss 0.681798
epoch 2688, loss 0.821893
epoch 2816, loss 0.775097
epoch 2944, loss 0.584939
epoch 3072, loss 0.838966
epoch 3200, loss 0.758165
epoch 3328, loss 0.758641
epoch 3456, loss 0.765115
epoch 3584, loss 0.551076
epoch 3712, loss 0.648277
epoch 3840, loss 0.699002
epoch 3968, loss 0.675257
epoch 4096, loss 0.724558
epoch 4224, loss 0.589045
epoch 4352, loss 0.712696
epoch 4480, loss 0.6716
epoch 4608, loss 0.668076
epoch 4736, loss 0.59991
epoch 4864, loss 0.648146
epoch 4992, loss 0.58883
epoch 5120, loss 0.647562
epoch 5248, loss 0.582769
epoch 5376, loss 0.700638
epoch 5504, loss 0.635166
epoch 5632, loss 0.893798
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0991789 0.0575798
0.090741 0.0607072
0.0966596 0.0506426
0.0477957 0.0536405
0.0330542 0.0520042
0.0134334 0.0664492
0.0968256 0.0584025
-0.0414014 0.00112985
-0.0250779 0.0551921
0.076422 0.0534863
0.0977799 0.0607815
0.0707066 0.0151294
0.0594395 0.0573062
0.0966595 0.0555542
0.0318695 0.0540326
-0.0271193 0.00745327
0.0321632 0.0556432
0.0594116 0.0519877
0.0194981 0.0104947
0.110244 0.0571808
0.0435741 0.0529197
0.121243 0.0538176
0.0594397 0.0567697
0.0326469 0.00165441
0.0907409 0.0555902
0.086885 0.0597901
0.0621339 0.00225175
0.0593133 0.0542071
0.0927399 0.0568789
0.0878299 0.0537871
0.0594398 0.0583564
-0.0132965 0.0658075
0.029557 -0.0019325
0.0271289 -0.00124832
0.0368959 0.0536531
0.0861488 0.057829
0.0496681 0.0564958
0.0977801 0.0631972
-4.6184e-07 -0.00223656
0.0991789 0.0563077
6.39156e-06 -0.00278545
0.0878299 0.0536278
0.0991789 0.0576081
0.0983621 0.0529808
0.0394802 -0.000441189
0.0594398 0.0578966
0.130756 0.0526661
0.134995 0.0555357
2.27454e-05 0.00682403
0.130757 0.0589596
0.0335492 0.0545722
-1.32922e-07 0.00696505
0.0803921 0.0546458
0.0529709 -0.00182003
0.0188018 0.0612969
0.0496683 0.0569343
0.0497671 0.0526778
-0.00378521 0.0049211
0.022444 0.0526757
-0.0394796 -0.00142861
0.0330949 0.0544691
0.0494968 0.000251506
0.00932522 0.0571391
-4.71368e-07 0.00665461
-0.0188605 0.00333882
0.000835126 0.000893594
0.079197 0.0527392
0.0859466 0.053482
0.0865812 0.0585462
0.0396778 0.0555813
-0.063123 -0.0021624
-0.0449599 -0.00242812
0.0191272 0.0551897
0.0882822 0.0539777
0.0407513 0.0513312
0.059435 0.0566592
0.0950802 0.05839
0.0109586 0.00323456
-0.0707068 0.00255761
0.076438 0.056525
0.0335492 0.0545722
0.0882823 0.0533289
0.0857788 0.0567793
-0.0234626 0.00374347
0.130757 0.0613573
-0.0417488 0.00204423
2.96104e-08 0.000871734
-0.0611171 -0.00198838
-1.32922e-07 0.000172269
0.0318695 0.0525931
1.01848e-06 0.00421506
-0.0734312 0.0028974
0.0599029 0.057373
0.0594273 0.0568012
0.0594376 0.0561635
0.0594349 0.0559618
-0.0417491 0.000359353
0.0496682 0.0577616
0.0867119 0.0571415
-0.0271153 0.00613203
0.0859466 0.0571791
-7.99663e-06 0.00352365
0.0207331 -0.000320847
0.0496682 0.0583004
0.0272894 0.00691183
0.046799 0.0040817
0.0594399 0.0571257
-0.0271126 -0.0038867
-0.0204122 -0.000665336
0.0851609 0.0580545
0.0667826 0.0570344
0.0691987 0.0582872
0.062595 0.0555688
0.0290868 0.0559317
-0.00236343 0.0540756
0.0144853 0.00234158
0.0144704 0.000164808
0.0272988 0.00360196
0.0188015 0.0609967
-0.0417573 0.00241795
0.104819 0.0554766
0.0594396 0.0519623
0.0594395 0.0535241
2.41162e-06 0.00242945
-0.014471 -0.000872373
0.0321468 0.0525512
0.0594376 0.0552387
0.0037846 0.00535568
parameters: [ 9.     2.     2.     1.2    5.744]. error: 1487131148.74.
----------------------------
epoch 0, loss 1.35634
epoch 128, loss 0.724237
epoch 256, loss 0.576399
epoch 384, loss 0.833853
epoch 512, loss 0.542027
epoch 640, loss 0.686435
epoch 768, loss 0.62204
epoch 896, loss 0.535299
epoch 1024, loss 0.725617
epoch 1152, loss 0.605054
epoch 1280, loss 0.520466
epoch 1408, loss 0.698331
epoch 1536, loss 0.67095
epoch 1664, loss 0.646484
epoch 1792, loss 0.669563
epoch 1920, loss 0.621496
epoch 2048, loss 0.563918
epoch 2176, loss 0.687275
epoch 2304, loss 0.573852
epoch 2432, loss 0.524395
epoch 2560, loss 0.818872
epoch 2688, loss 0.936438
epoch 2816, loss 0.64793
epoch 2944, loss 0.76112
epoch 3072, loss 0.725723
epoch 3200, loss 0.606461
epoch 3328, loss 0.622577
epoch 3456, loss 0.85634
epoch 3584, loss 0.599086
epoch 3712, loss 0.672367
epoch 3840, loss 0.576831
epoch 3968, loss 0.711813
epoch 4096, loss 0.604807
epoch 4224, loss 0.558608
epoch 4352, loss 0.596694
epoch 4480, loss 0.594359
epoch 4608, loss 0.585348
epoch 4736, loss 0.558871
epoch 4864, loss 0.598076
epoch 4992, loss 0.60511
epoch 5120, loss 0.604667
epoch 5248, loss 0.721895
epoch 5376, loss 0.662088
epoch 5504, loss 0.649865
epoch 5632, loss 0.568212
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0562851 0.0575142
-0.0462169 0.00887572
0.102035 0.0648559
0.0188016 0.0599717
0.12945 -0.0081659
-0.000807624 -0.0051235
-0.0340213 0.00439007
0.0593758 0.0470273
0.0907411 0.0685691
0.0113515 -0.00530642
0.0494968 -0.00800095
0.0594375 0.0667866
0.0626054 0.0502972
0.0706155 0.0628783
0.0318695 0.0526026
-0.000795203 -0.00377123
0.121243 0.0750704
0.130756 0.0566961
0.0997462 0.0528552
0.0271204 -0.00137958
0.0582589 0.0511537
0.0920969 0.0601934
0.0594395 0.0560773
0.0594397 0.0660928
8.40829e-06 -0.000742636
0.0319945 0.0468784
0.0482639 0.0536493
0.0205126 0.062599
0.038792 0.0461922
-0.053266 -0.0126198
0.0301505 0.0598278
-3.91322e-05 -0.0090621
-0.000835392 0.00161629
0.0776735 -0.0101692
0.0734322 -0.0106381
0.0952317 0.0711331
-0.0362233 -0.00102205
0.05943 0.0580111
-0.0234677 -0.0022563
0.0871097 0.0568772
0.0220537 0.0434987
0.0920508 0.0500219
0.100115 -0.00104804
0.0375381 0.0528219
0.0861488 0.0626541
0.0131855 0.00285057
0.0407515 0.0422195
0.0541426 0.0662465
0.0317699 0.0678337
-0.0248165 -0.00760379
-0.0462049 -0.00497057
0.0189868 -0.00279587
0.0853245 0.0500276
0.0538009 0.0547043
0.0189828 -7.40989e-05
0.0594027 0.0453093
0.0910569 0.0724108
0.0414025 -0.0073095
0.0243585 0.0454834
0.0625951 0.0464553
0.00250079 -0.00396343
0.0283783 0.0567261
0.0707072 0.0209895
0.0730674 0.0484992
-0.0210947 -0.00902032
0.0599029 0.0702259
-0.0144706 0.00435539
-0.0204001 -0.00915139
0.0449639 0.00838377
0.132178 0.0847383
0.130756 0.0427546
0.0321468 0.0629724
-2.95455e-07 -0.00176531
0.0305923 0.0524071
0.100097 0.0677641
0.0594398 0.0567453
0.0337182 0.0676858
0.0541227 0.0530493
0.0251233 0.0497177
0.0927396 0.0740394
0.0764118 0.0620261
-0.0468001 -0.00759638
0.0310444 0.0423649
0.109566 0.0487648
0.0716773 -0.0024652
-0.0326437 -0.00243494
0.121243 0.057899
0.0599612 0.0601554
0.0362233 -0.00401685
-0.0385437 0.00233233
0.0407513 0.0479669
0.0234704 -0.00787666
-0.0416423 -0.00865646
0.0222151 0.0229823
0.0251233 0.0602882
-1.17339e-07 0.00281162
0.0412752 0.00980866
0.0860587 0.00464212
0.0867199 0.0592664
0.0964307 0.0445402
0.0477958 0.0630984
-0.00726943 0.0579465
0.0978866 0.0730379
0.0989678 0.0723814
0.0562597 0.0572666
0.0527575 0.0519416
0.0621334 -0.00311683
-3.66166e-06 -0.00350059
0.043921 0.0522212
-0.0414033 -0.00542953
0.0594452 0.0842563
0.0248197 -0.00970356
-0.0188028 -0.00420985
0.000834782 0.00625267
-0.0131848 -0.00607509
0.0237989 0.0298221
0.0887289 0.0722387
0.0140549 0.0409152
0.0236479 0.0401006
0.0997468 0.0434149
0.0857284 -0.00413871
0.0997462 0.0737052
0.0204143 -0.00290164
0.0368959 0.0496367
0.0237989 0.0517187
-0.014471 0.000860293
0.0676335 0.0659216
0.0631196 -0.00468811
parameters: [ 9.     0.618  2.     1.2    5.744]. error: 27594561.8576.
----------------------------
epoch 0, loss 1.44076
epoch 128, loss 0.68718
epoch 256, loss 0.641158
epoch 384, loss 0.530132
epoch 512, loss 0.590583
epoch 640, loss 0.531727
epoch 768, loss 0.628368
epoch 896, loss 0.565318
epoch 1024, loss 0.595977
epoch 1152, loss 0.606823
epoch 1280, loss 0.494064
epoch 1408, loss 0.73589
epoch 1536, loss 0.698383
epoch 1664, loss 0.621738
epoch 1792, loss 0.553137
epoch 1920, loss 0.621606
epoch 2048, loss 0.569159
epoch 2176, loss 0.612232
epoch 2304, loss 0.687587
epoch 2432, loss 0.532177
epoch 2560, loss 0.579326
epoch 2688, loss 0.625564
epoch 2816, loss 0.549899
epoch 2944, loss 0.527313
epoch 3072, loss 0.637974
epoch 3200, loss 0.625357
epoch 3328, loss 0.424121
epoch 3456, loss 0.526837
epoch 3584, loss 0.689774
epoch 3712, loss 0.547179
epoch 3840, loss 0.677876
epoch 3968, loss 0.461682
epoch 4096, loss 0.418719
epoch 4224, loss 0.682836
epoch 4352, loss 0.555573
epoch 4480, loss 0.406085
epoch 4608, loss 0.302944
epoch 4736, loss 0.620676
epoch 4864, loss 0.569826
epoch 4992, loss 0.501391
epoch 5120, loss 0.512494
epoch 5248, loss 0.430864
epoch 5376, loss 0.601362
epoch 5504, loss 0.546046
epoch 5632, loss 0.522055
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.024704 0.0427487
0.0177241 0.00596405
0.0317698 0.0549946
0.0529729 0.0214638
0.0281391 0.047611
0.092056 0.0428553
0.0177216 0.00153105
-0.00894944 0.00528441
0.0424316 0.0272541
0.0295687 0.00182782
0.0661225 0.0670221
0.132178 0.0745758
0.0800819 0.057134
0.00143008 -0.00189062
-1.63011e-07 -0.00887249
0.0271187 -0.00305853
-0.0118768 0.0388403
0.105506 0.0709025
0.0520969 0.0595091
0.0496683 0.0524508
0.0941706 0.0541473
0.0727412 0.0601785
0.0907408 0.0633974
-0.0857226 0.00484385
0.0927399 0.0638176
0.0871097 0.0775379
-0.0776729 -0.0112967
3.40452e-06 0.00553242
0.0385427 -0.000475022
0.0223972 0.0318771
0.022215 0.0251382
-0.0144706 0.00416005
-0.0271126 0.00128208
0.0952315 0.0683454
0.0224432 0.0386585
0.00891968 0.00733469
0.0966587 0.0539475
0.0191276 0.0337721
-0.0207292 -0.00281762
0.101441 0.0597206
0.0205116 0.0387074
0.0414025 -0.00179422
0.126086 0.0737917
-0.0662843 -0.00285824
0.0968256 0.0702028
-0.0204101 -0.00789002
0.0122504 0.0286167
-0.0621343 0.000608151
0.0594759 0.0338208
0.0207304 -0.00336901
0.0261393 0.0481032
0.0691987 0.0567348
0.0497672 0.0465415
0.00863092 0.0358569
0.0599031 0.0638516
0.0594325 0.0308562
0.000835111 0.00281918
0.01134 -0.0037955
0.059435 0.0343847
0.0412776 0.00585219
0.0394803 0.0181722
0.0271185 0.00605319
0.0494968 0.00686687
0.0594396 0.0567455
0.0131845 0.00221738
0.106645 0.0517284
0.0818743 0.0485302
0.0594297 0.0378606
-0.0385427 -0.00257281
-0.0827278 0.00190726
0.0387924 0.0472948
0.0594398 0.0560846
0.0865811 0.071665
0.126206 0.068813
0.0752955 0.0468738
0.0867202 0.0680942
0.0329021 0.0331995
-0.0110929 0.0236424
0.0247829 -0.000736403
0.0220539 0.0498791
0.0211182 0.0274603
0.0424316 0.0312917
-0.014471 -0.00101914
0.0329331 0.0468543
-0.0188606 -0.00158729
0.0234638 0.00186553
0.01684 0.0281673
-0.0295746 0.00283573
0.130756 0.0956528
0.0724734 0.0466325
0.100115 0.00266392
0.032195 0.0349763
0.00931426 0.0461264
0.0327315 0.0513518
0.0867333 0.0718913
0.0594396 0.0635548
0.0329021 0.0258644
-0.0248092 0.00923888
-0.0113473 0.0029911
0.00143008 -0.00754584
0.028139 0.0441512
0.00931426 0.0492594
0.102035 0.0501639
0.0144859 -0.00144995
0.0907408 0.0600685
0.0867335 0.0579928
0.0322983 0.0476526
0.0375434 0.0367207
0.0781223 0.0561026
0.0727409 0.0655877
-3.76332e-06 -0.000742364
0.00596546 0.0295046
0.0991789 0.0693708
-0.0248182 0.00214495
0.0373484 0.0502952
0.034034 -0.00164895
-0.0272904 0.00136133
0.0871097 0.0690773
0.043628 0.0419517
0.0340225 -0.00274675
4.17498e-06 0.00522324
-2.64683e-07 0.000560538
0.0716773 0.000616626
0.0562849 0.0538526
0.0920969 0.060793
0.0387924 0.0472948
0.0201879 0.039772
-0.0117788 0.000653797
parameters: [ 9.     0.063  2.     1.2    5.744]. error: 6911429398.15.
----------------------------
epoch 0, loss 1.12566
epoch 128, loss 0.933078
epoch 256, loss 0.660915
epoch 384, loss 0.697075
epoch 512, loss 0.776426
epoch 640, loss 0.640795
epoch 768, loss 0.728799
epoch 896, loss 0.690849
epoch 1024, loss 0.725628
epoch 1152, loss 0.63443
epoch 1280, loss 0.631217
epoch 1408, loss 0.625445
epoch 1536, loss 0.548913
epoch 1664, loss 0.545459
epoch 1792, loss 0.76239
epoch 1920, loss 0.687316
epoch 2048, loss 0.67738
epoch 2176, loss 0.625662
epoch 2304, loss 0.553744
epoch 2432, loss 0.518375
epoch 2560, loss 0.638451
epoch 2688, loss 0.513463
epoch 2816, loss 0.773627
epoch 2944, loss 0.820038
epoch 3072, loss 0.642094
epoch 3200, loss 0.644654
epoch 3328, loss 0.582833
epoch 3456, loss 0.662326
epoch 3584, loss 0.667642
epoch 3712, loss 0.569302
epoch 3840, loss 0.820738
epoch 3968, loss 0.655375
epoch 4096, loss 0.538975
epoch 4224, loss 0.522736
epoch 4352, loss 0.657415
epoch 4480, loss 0.427284
epoch 4608, loss 0.546391
epoch 4736, loss 0.555692
epoch 4864, loss 0.606381
epoch 4992, loss 0.710098
epoch 5120, loss 0.517662
epoch 5248, loss 0.64526
epoch 5376, loss 0.586211
epoch 5504, loss 0.516976
epoch 5632, loss 0.542045
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-4.71368e-07 0.0124575
0.0907408 0.0613361
0.0594424 0.0580181
0.062595 0.0641701
0.0599031 0.0539154
0.0189868 -0.00254439
-0.0492483 0.00680666
0.0286789 0.0483586
0.0152616 0.0148346
0.0860364 0.0345992
0.0131849 -0.00336274
0.0752955 0.0570761
0.0710835 0.0461337
0.0301504 0.0606514
-0.0707068 0.0109026
-0.00553123 0.0567635
-0.00331175 0.0526175
0.00894922 0.0211665
-0.0412731 0.0063466
0.0267393 -0.00217304
0.0251231 0.0639754
0.0492512 0.00216368
0.0281391 0.0526755
-0.0188593 -0.00349661
0.0412725 0.020546
0.0462156 0.0123227
0.125883 0.0575139
-0.0295746 0.00423724
0.0907409 0.0594914
2.41162e-06 0.00734991
-0.0204122 -0.00281546
0.0131858 -0.000961012
0.0764481 0.0583643
0.0997468 0.0507889
0.0594399 0.0615554
-0.0362226 0.0116516
0.126222 0.0638369
0.0532535 0.0515192
0.0458116 0.062568
0.0594251 0.0535297
0.085161 0.0597898
0.0859466 0.0590519
0.0247038 0.0533637
-0.0117783 0.0101539
0.000793148 0.0124103
0.0271109 0.0108011
0.0562648 0.0562459
0.0346447 -0.00312601
-3.66166e-06 -0.00294256
-0.0318771 0.00392519
0.0188016 0.0616695
0.0417454 0.00336984
0.12432 0.0653779
-6.30661e-07 0.00501808
0.101441 0.0568723
0.0251234 0.0661501
0.0318693 0.0527186
0.049499 0.0185467
-7.68032e-07 -0.00183485
-7.99663e-06 0.0179468
0.093775 0.0664055
0.0210981 0.0102321
0.000802697 0.00829225
0.027823 0.0601624
0.0414075 -0.00298731
0.121243 0.0595671
0.0117845 0.00224149
0.0966596 0.0586189
-0.0495737 0.00130975
0.0247806 0.0146001
-0.0462148 -0.00140006
0.0594398 0.0638021
0.0594398 0.0598027
0.076422 0.0531091
0.0327313 0.0576242
0.0174339 0.0461364
0.0301504 0.0597322
0.0477958 0.0432231
0.029557 -0.0027607
0.0439209 0.046793
-0.0247793 0.00906142
0.0424529 0.0496951
0.0594397 0.0607372
0.0966587 0.0621128
0.0594396 0.0590107
0.00894166 -0.00297125
-0.0207367 -0.00164383
0.0271204 -0.00247036
0.0327313 0.061796
0.0210958 0.010758
0.0605996 0.0579846
-0.0449623 -0.00255567
0.093775 0.0678607
-0.0394796 0.00521074
0.0594396 0.0603149
-0.0707062 -0.00309113
-0.0271153 0.0104483
0.0818743 0.057563
0.130757 0.0577227
0.0414051 0.0086681
0.0594473 0.058158
0.00932421 0.0428133
0.110244 0.0601615
0.0594397 0.0541741
-0.0271193 -0.00333687
0.0394799 -0.00224045
0.0611204 -0.00379271
-0.0326435 -0.00417022
0.0710837 0.0510735
0.0321468 0.0543259
0.106624 0.064372
0.0594397 0.0536904
0.0594398 0.0637041
0.0329332 0.0408937
0.0211182 0.0672067
0.106655 0.0670446
0.0878299 0.0504241
0.038544 0.00739633
0.028139 0.0530002
0.0271117 0.0100927
0.0509553 0.0548058
0.0594449 0.0507419
0.00142251 -0.00198669
0.0337179 0.0535189
0.0626054 0.0579289
0.121243 0.0573589
0.0199306 0.0629907
0.072741 0.0617368
parameters: [ 9.     1.     2.     1.2    5.744]. error: 5127912.49613.
----------------------------
epoch 0, loss 1.1971
epoch 128, loss 0.810341
epoch 256, loss 0.90689
epoch 384, loss 0.658754
epoch 512, loss 0.819672
epoch 640, loss 0.616829
epoch 768, loss 0.591241
epoch 896, loss 0.686181
epoch 1024, loss 0.669386
epoch 1152, loss 0.791632
epoch 1280, loss 0.698745
epoch 1408, loss 0.601431
epoch 1536, loss 0.681034
epoch 1664, loss 0.543302
epoch 1792, loss 0.694055
epoch 1920, loss 0.68585
epoch 2048, loss 0.701824
epoch 2176, loss 0.565745
epoch 2304, loss 0.634917
epoch 2432, loss 0.530415
epoch 2560, loss 0.711824
epoch 2688, loss 0.507563
epoch 2816, loss 0.752296
epoch 2944, loss 0.680945
epoch 3072, loss 0.534163
epoch 3200, loss 0.742608
epoch 3328, loss 0.547764
epoch 3456, loss 0.507202
epoch 3584, loss 0.675865
epoch 3712, loss 0.558534
epoch 3840, loss 0.700692
epoch 3968, loss 0.825516
epoch 4096, loss 0.673501
epoch 4224, loss 0.613908
epoch 4352, loss 0.493627
epoch 4480, loss 0.642357
epoch 4608, loss 0.683728
epoch 4736, loss 0.662852
epoch 4864, loss 0.603847
epoch 4992, loss 0.418458
epoch 5120, loss 0.56137
epoch 5248, loss 0.506504
epoch 5376, loss 0.463104
epoch 5504, loss 0.661903
epoch 5632, loss 0.565719
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0776855 -0.00476687
-0.0857218 -0.00268996
-0.0211014 -0.00489597
0.0857284 -0.00173272
0.0532657 0.00497599
0.0237987 0.0611501
0.0271185 -0.00236414
0.0412702 0.00157309
0.0321632 0.0428629
0.0417583 -0.00488388
0.121243 0.0552069
-3.76332e-06 -0.00200821
-0.0414033 -0.00475972
0.0813268 0.0624378
-0.0131841 0.00333839
0.0321467 0.0353663
0.0223968 0.0280467
4.07609e-05 -0.00209059
-0.0113495 -0.00448674
0.0122142 0.0355262
0.0333261 0.0720871
0.0594398 0.0438725
-1.1614e-09 -0.0020219
0.101814 0.0783158
0.0271185 -0.00223451
-0.00378521 -0.00163624
0.057604 0.0927156
4.07609e-05 -0.00398504
0.0189852 -0.00115403
0.125863 0.0706675
0.093775 0.0822736
0.110244 0.0752725
0.0867332 0.0690486
0.081797 0.0315431
-0.0295559 -0.00437893
0.072741 0.049664
0.0340225 -0.00411114
0.0594467 0.0932756
-0.00236343 0.0288206
0.0541426 0.0436886
0.0497671 0.0329785
0.0122243 0.0202929
0.00931421 0.033624
0.0290868 0.0521085
-0.0271278 -0.00406713
0.0859467 0.062485
0.014471 0.000386544
0.0496681 0.0533884
0.0594397 0.0288497
-0.0194973 -0.00508957
0.0174335 0.0422688
0.0815268 0.0543713
0.059181 0.0556767
-0.0204001 -0.00463376
0.0734439 0.00134342
0.0477956 0.0540659
0.0764428 0.0430585
0.0529709 -0.00232305
0.0188018 0.0614097
-0.034636 0.000794721
0.0281392 0.0247732
0.0330949 0.0209402
0.0594396 0.0488662
0.0318693 0.0450697
-0.0248165 -0.00456041
0.0152591 -0.000681247
0.0941706 0.0587664
0.0109586 -0.00462737
0.0971047 0.0741829
-0.049248 -0.00352138
0.0323166 -0.00209838
0.0494968 0.0142618
0.05627 0.0358463
0.0407513 0.0532376
0.00596546 0.0636972
0.0594481 0.0918816
0.0611271 0.00278439
-0.0857218 -0.00112274
0.0234704 -0.00027801
0.0594397 0.068339
0.00932522 0.0414626
0.0626106 0.0519587
0.0197009 0.0290918
0.0611271 -0.00339366
0.0991789 0.0772819
0.0907408 0.047968
0.0691986 0.0646162
0.133575 0.0815039
-0.0131841 0.010569
0.014484 -0.00237679
4.17498e-06 -0.00213776
0.0691985 0.0404457
0.0594466 0.0919659
0.000213748 0.06283
0.0594395 0.0578364
0.0449639 -0.00429571
-0.013282 0.0853439
-3.40966e-05 -0.00458384
0.0907411 0.0791651
0.0396776 0.0525814
-0.0385421 0.00255505
0.10665 0.0506481
0.126206 0.0616418
0.0716711 -0.00273007
-0.0417425 -0.000867765
0.0855721 0.0772369
0.0247038 0.0233854
-0.00142797 -0.000650204
-0.0390966 -0.00400919
0.0301504 0.0527526
-5.97989e-06 -0.00156115
-2.99285e-06 -0.00218247
0.0589765 0.0654846
0.059181 0.0455678
0.0477957 0.044711
0.0710837 0.0722434
-0.0611177 -0.000475037
0.0416418 -0.00433881
-0.0494977 -0.00419589
-0.0326435 -0.00197362
0.022444 0.0598408
-0.0194973 -0.00146428
-0.0362233 -0.00382214
0.130756 0.0457774
0.0373475 0.0465164
0.0727409 0.0570947
-0.0631232 -0.00368823
-0.0807993 -0.00333875
parameters: [ 9.     1.382  2.     1.2    5.744]. error: 3.99892532678e+13.
----------------------------
epoch 0, loss 1.40378
epoch 128, loss 1.01848
epoch 256, loss 0.688927
epoch 384, loss 0.774492
epoch 512, loss 0.763065
epoch 640, loss 0.726449
epoch 768, loss 0.653723
epoch 896, loss 0.595519
epoch 1024, loss 0.568361
epoch 1152, loss 0.543881
epoch 1280, loss 0.647964
epoch 1408, loss 0.675083
epoch 1536, loss 0.647976
epoch 1664, loss 0.631852
epoch 1792, loss 0.798267
epoch 1920, loss 0.636414
epoch 2048, loss 0.631985
epoch 2176, loss 0.507269
epoch 2304, loss 0.719753
epoch 2432, loss 0.716524
epoch 2560, loss 0.7001
epoch 2688, loss 0.543505
epoch 2816, loss 0.637851
epoch 2944, loss 0.601954
epoch 3072, loss 0.641357
epoch 3200, loss 0.564075
epoch 3328, loss 0.665773
epoch 3456, loss 0.57363
epoch 3584, loss 0.520175
epoch 3712, loss 0.672887
epoch 3840, loss 0.66155
epoch 3968, loss 0.578425
epoch 4096, loss 0.535118
epoch 4224, loss 0.590278
epoch 4352, loss 0.5859
epoch 4480, loss 0.587984
epoch 4608, loss 0.748135
epoch 4736, loss 0.477604
epoch 4864, loss 0.541301
epoch 4992, loss 0.668831
epoch 5120, loss 0.723872
epoch 5248, loss 0.648552
epoch 5376, loss 0.544076
epoch 5504, loss 0.542021
epoch 5632, loss 0.585739
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.121243 0.097093
8.40829e-06 0.00663277
0.0813321 0.0473117
0.0248197 -0.00211939
0.130756 0.0480223
0.0582389 0.0329123
-0.00733491 0.0285444
0.0414001 -0.00798049
-0.0267398 -0.00774394
0.0920961 0.0438109
-0.0468001 -0.00928156
0.0606162 0.033092
0.0497671 0.0552561
0.028139 0.0446175
0.0691986 0.065528
-0.0340213 -0.00160971
0.0394802 -0.00789563
-2.86114e-05 0.010122
0.0964772 0.0586385
0.0724734 0.0371125
0.00378707 -0.00756334
0.0305923 0.0458018
0.0800821 0.0626916
-0.0272977 -0.0031279
-0.0248182 -0.00812996
-0.0132848 0.0557629
0.0532677 -0.00708314
0.0106905 -0.00455169
0.0207331 -0.000614445
0.0283785 0.0389397
0.0234754 0.00584035
0.0317699 0.0604771
0.0594452 0.0523566
0.093775 0.0490916
-0.0807993 -0.00499242
0.0527577 0.0538878
-0.0631232 -0.00899395
0.0417448 -0.00211635
0.079197 0.0527199
0.0868851 0.089295
0.0631196 -0.00796911
0.0710836 0.0852631
0.0317699 0.0694896
0.081527 0.0491513
0.0986864 0.065446
0.0385437 0.00121657
0.0209719 -0.00130185
0.0594204 0.0414619
0.0281393 0.0572837
0.0416484 0.0117814
0.0122352 0.0380943
0.0199307 0.0401484
0.0661223 0.0589558
-1.63011e-07 0.00234726
0.028139 0.0778438
0.0853255 0.0326058
0.0981543 0.0670967
0.0424369 0.0441604
-0.0857218 -0.00365114
-2.15513e-06 0.00551076
0.0859467 0.0806712
0.0592543 0.0383831
0.100097 0.0476253
0.0813321 0.0473117
0.0594396 0.081786
0.0691985 0.0669278
0.0272916 -0.00883783
-0.129457 -0.00901509
0.121243 0.0624388
-0.0210947 0.00296297
0.0321468 0.0641802
0.0113515 -0.00830184
0.13219 0.0724812
0.0589763 0.0492435
0.0594398 0.0748945
0.05937 0.0320136
-0.0495737 -0.00846657
-0.0248165 -0.00835204
0.0867202 0.0771923
0.0594759 0.036969
0.0390991 -0.00386565
-0.0248182 -0.00822525
0.0910567 0.0632526
0.0247038 0.055438
0.0730675 0.0695359
0.0927399 0.0709158
0.0385437 -0.00239917
-0.0194973 0.00206945
-0.0152583 0.00711369
0.0851613 0.0425361
0.0952316 0.0808554
0.0362237 -0.000630773
0.0968257 0.0604983
0.0329332 0.0744884
4.07609e-05 0.00108432
0.0621339 0.00238227
0.0594396 0.0785546
0.0764118 0.049792
0.0321468 0.0566079
0.094504 0.049575
-0.0416401 0.00624846
0.100097 0.0574301
0.088729 0.0645497
0.0594398 0.0762684
0.0272894 -0.0021833
-0.0326435 -0.00838942
0.0188086 -0.00857291
0.0261394 0.0765106
0.0626054 0.0534917
-0.0118764 0.0553349
0.0414025 -0.00870228
0.0321586 0.0698307
0.032933 0.0616365
-0.0462049 -0.00204667
0.0781227 0.061793
0.0174339 0.039955
-0.02083 0.00810657
0.0807993 -0.0083561
-0.000792737 -0.00921916
0.0412702 0.0105324
-0.00628221 0.0493414
0.0981543 0.0670967
0.0739227 0.0478971
0.0920969 0.0512669
0.0920969 0.0547106
-0.0734312 0.00279839
0.0417489 0.00204655
0.0295687 -0.000782828
parameters: [ 9.     0.642  2.     1.2    5.744]. error: 174826.905908.
----------------------------
epoch 0, loss 1.12129
epoch 128, loss 1.01186
epoch 256, loss 0.78264
epoch 384, loss 0.761678
epoch 512, loss 0.653515
epoch 640, loss 0.631629
epoch 768, loss 0.5745
epoch 896, loss 0.641022
epoch 1024, loss 0.685705
epoch 1152, loss 0.635312
epoch 1280, loss 0.563855
epoch 1408, loss 0.839327
epoch 1536, loss 0.812626
epoch 1664, loss 0.759997
epoch 1792, loss 0.719541
epoch 1920, loss 0.755012
epoch 2048, loss 0.625598
epoch 2176, loss 0.54667
epoch 2304, loss 0.676767
epoch 2432, loss 0.651572
epoch 2560, loss 0.546121
epoch 2688, loss 0.644947
epoch 2816, loss 0.729709
epoch 2944, loss 0.622416
epoch 3072, loss 0.632223
epoch 3200, loss 0.815131
epoch 3328, loss 0.653169
epoch 3456, loss 0.656195
epoch 3584, loss 0.681127
epoch 3712, loss 0.496864
epoch 3840, loss 0.666426
epoch 3968, loss 0.720739
epoch 4096, loss 0.602377
epoch 4224, loss 0.699983
epoch 4352, loss 0.791906
epoch 4480, loss 0.618206
epoch 4608, loss 0.51858
epoch 4736, loss 0.547609
epoch 4864, loss 0.621037
epoch 4992, loss 0.623087
epoch 5120, loss 0.5555
epoch 5248, loss 0.688331
epoch 5376, loss 0.772026
epoch 5504, loss 0.894422
epoch 5632, loss 0.5459
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0791961 0.048665
-0.0207342 -0.00799017
0.0373475 0.0487678
-0.0390966 -0.00187461
-0.0131858 0.00319693
0.0611204 -0.000678039
0.0458115 0.049937
0.0174339 0.0556088
-0.0188057 0.00102127
0.123121 0.0509823
0.0201879 0.0501026
0.106624 0.0597848
0.022444 0.0497402
-0.000786289 -0.00194514
0.025123 0.0622812
-0.0734361 -0.00227612
-0.0188062 -0.00232759
-0.0295676 0.0018328
0.0461382 0.0635109
-3.91322e-05 0.00535212
0.0191276 0.0547261
0.125873 0.0643616
0.0691984 0.0619288
0.0243594 0.0556487
0.0631196 -0.000284507
0.0594452 0.0772567
-0.0340213 0.00157043
0.0461385 0.0569513
-0.0462169 -0.00951667
0.0201879 0.0513521
-0.0118765 0.0508437
0.0594349 0.0519069
0.0243586 0.0542438
0.129916 0.0600024
-0.00553123 0.035148
0.0204143 0.00180576
0.0882823 0.0544201
0.022215 0.0529573
0.032195 0.0499752
0.0867333 0.0607111
0.100097 0.067903
0.0813269 0.0567005
0.0414051 -0.0048891
-0.0131858 0.00222356
0.000796249 -0.00751433
-0.0113401 -0.00751951
8.39808e-07 -0.00511639
0.0593312 0.047227
0.0449751 0.0630717
0.0997462 0.0610602
0.0122404 0.0455708
0.0385437 0.00121993
0.0594375 0.0575454
-0.0131855 0.00717856
-0.00733491 0.0336683
0.062595 0.0568751
0.0217931 0.0646781
0.0983621 0.0549246
0.0464005 0.0548588
-0.00378521 -0.00578286
0.0593105 0.0453145
0.0783558 0.061298
0.0191272 0.0559978
0.0631196 -0.00555796
-0.00236348 0.0521447
0.0859467 0.0637161
0.0857789 0.0535369
-0.0234699 -0.0075202
0.0267781 0.0533483
0.080402 0.0534455
-0.0023637 0.0438539
-0.0188028 7.41565e-05
-0.0271153 0.000246541
0.0593346 0.0460244
-0.0295676 -0.00732957
0.0335492 0.0546843
0.0384877 0.0566341
9.99101e-07 0.00519858
-0.014471 -0.00382041
0.0813269 0.0567005
0.0405236 0.056473
0.0417629 -0.00628627
0.0305923 0.0530302
0.0734394 0.0132109
0.102034 0.0530993
0.0986864 0.0598423
1.13647e-06 0.00223689
0.0819546 0.0532053
0.0927396 0.0608753
0.0706156 0.063902
0.0329331 0.0638504
-0.0318771 -0.00693041
0.0593105 0.049628
-0.0367964 -0.00736716
0.0527575 0.0624488
0.105471 0.0764509
-0.0412757 0.000944418
0.0868848 0.0643758
0.0532578 -0.00677863
0.0621308 -0.00559885
0.0594398 0.0629724
0.0853255 0.0579342
-0.0189848 0.0035778
0.0317696 0.0589001
0.0920961 0.0507529
0.0867202 0.0610363
0.0385434 0.0126687
0.0117847 -0.00596278
0.0435741 0.05427
0.0594396 0.0627727
0.0611263 0.00376733
0.0643397 0.0517301
0.0594398 0.0604559
-0.0807974 -0.00857904
0.101814 0.0674001
0.0318695 0.0454172
0.0594297 0.0569157
0.0261395 0.0563406
0.0330953 0.0529248
0.0867334 0.0606172
0.0234704 0.00746016
0.022444 0.0497506
0.0907409 0.0610297
0.081797 0.056673
0.0209836 -0.00796543
0.0182076 0.0560546
-0.0318771 0.000414011
-0.0248069 0.00501601
parameters: [ 9.     0.821  2.     1.2    5.744]. error: 19529421.1775.
----------------------------
epoch 0, loss 1.23048
epoch 128, loss 0.758503
epoch 256, loss 0.626828
epoch 384, loss 0.63327
epoch 512, loss 0.697999
epoch 640, loss 0.561569
epoch 768, loss 0.852089
epoch 896, loss 0.544747
epoch 1024, loss 0.643407
epoch 1152, loss 0.623363
epoch 1280, loss 0.550499
epoch 1408, loss 0.661766
epoch 1536, loss 0.560923
epoch 1664, loss 0.527123
epoch 1792, loss 0.594228
epoch 1920, loss 0.719531
epoch 2048, loss 0.60171
epoch 2176, loss 0.608808
epoch 2304, loss 0.58291
epoch 2432, loss 0.538115
epoch 2560, loss 0.513642
epoch 2688, loss 0.74894
epoch 2816, loss 0.74342
epoch 2944, loss 0.604683
epoch 3072, loss 0.636402
epoch 3200, loss 0.616824
epoch 3328, loss 0.657544
epoch 3456, loss 0.566174
epoch 3584, loss 0.653371
epoch 3712, loss 0.667373
epoch 3840, loss 0.791106
epoch 3968, loss 0.502263
epoch 4096, loss 0.542352
epoch 4224, loss 0.485231
epoch 4352, loss 0.474874
epoch 4480, loss 0.462982
epoch 4608, loss 0.841359
epoch 4736, loss 0.611419
epoch 4864, loss 0.587661
epoch 4992, loss 0.524473
epoch 5120, loss 0.555244
epoch 5248, loss 0.542926
epoch 5376, loss 0.70669
epoch 5504, loss 0.516017
epoch 5632, loss 0.666966
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.00728242 0.0370623
0.0222151 0.0508588
0.034034 0.0245364
0.0818743 0.0701186
0.0538297 0.0560292
0.0335493 0.0464152
-0.053257 -0.00435623
-0.0106908 -0.0010808
0.0122243 0.0609555
0.0188015 0.0524538
0.0659615 -0.00399833
0.0412776 -0.0069986
0.0318695 0.052342
-0.0662877 -0.00470393
0.0508582 0.0527393
0.0204119 0.0170452
0.0753057 0.0691961
0.000834782 -0.00317685
0.000794682 -0.00284334
-4.71368e-07 0.0215948
0.0764328 0.0595069
0.0482639 0.0672792
-6.66928e-06 0.0219542
0.0800821 0.054848
0.0710836 0.0714903
0.0211183 0.0662167
0.0851613 0.0751818
0.0131848 0.00202691
-0.0208232 0.0158784
0.0867119 0.0688645
0.0867199 0.0579037
-0.00968441 0.0425197
0.0910569 0.0594034
0.0594397 0.0619705
0.00250079 -0.0028057
0.028139 0.0398353
0.0140549 0.0468701
0.0971047 0.0757507
0.0458117 0.0360581
0.092056 0.065375
0.0594396 0.0551665
0.0527575 0.0664324
0.0594396 0.0582616
-0.0131848 -0.00212344
0.0224432 0.044965
0.0819546 0.0661956
0.0477956 0.04452
0.0691068 0.0510367
0.0611197 -0.00405119
0.0861487 0.0761196
0.0707072 -0.00508141
0.106645 0.0754077
0.0424369 0.0540895
0.110244 0.0701217
-0.0394792 0.000992418
0.130756 0.077208
0.125863 0.0734074
0.0482639 0.0560211
0.0662803 -0.0031916
0.0594398 0.0498085
0.110244 0.0475237
0.0329021 0.051992
0.0887289 0.0726361
-0.0118764 0.0508036
0.0237986 0.0372432
-0.0611272 -0.00417604
0.0691986 0.064367
0.101441 0.0573523
-0.0132965 0.0608032
0.0593105 0.0632082
0.0621302 0.0055317
-0.0188062 -0.00118833
-0.0132904 0.0514091
0.101814 0.06395
0.0594395 0.0626016
0.0301505 0.0392866
0.0323166 -0.00101738
0.0964772 0.0577917
0.0861487 0.0719501
0.0211182 0.0598965
0.0407513 0.0348193
0.0211031 -0.00446986
0.0529686 0.00177662
0.0594397 0.0592115
0.0859466 0.0523175
0.0367961 0.00226868
0.100668 0.0673052
0.0237986 0.0350661
0.132184 0.0871494
0.125873 0.0694584
0.0243594 0.0509424
0.106634 0.0609622
-0.0707068 -0.0010458
-0.0267379 -0.00490228
0.124354 0.076915
0.0730674 0.0542937
0.0727411 0.0549787
0.0594398 0.0488478
0.0330953 0.0457781
0.038544 0.00167075
0.0277564 0.0345068
0.088729 0.0651922
0.059435 0.0501537
0.0330949 0.0320603
0.0691987 0.0659191
0.0188603 0.0042247
-0.02083 -0.00725248
0.015257 -0.000946245
0.109566 0.0681856
0.0449639 0.00210128
0.0595095 0.0514751
0.0370769 0.0361182
0.0599031 0.0463195
0.0822309 0.0690935
0.0724734 0.0490744
-0.0362233 0.000427478
8.39808e-07 -0.00150648
0.044975 0.0595759
0.0449618 -0.00629989
0.0113418 -0.00300354
-0.0385437 -0.00175504
0.0301505 0.0402882
-0.0131858 0.0230295
0.0243594 0.0495996
-0.00856301 0.00226519
-0.0210947 0.0125249
0.0878299 0.0576041
0.0901952 0.0599664
parameters: [ 9.     0.421  2.     1.2    5.744]. error: 376.417044895.
----------------------------
epoch 0, loss 1.03118
epoch 128, loss 0.887089
epoch 256, loss 0.558968
epoch 384, loss 0.567632
epoch 512, loss 0.492497
epoch 640, loss 0.612272
epoch 768, loss 0.62528
epoch 896, loss 0.495526
epoch 1024, loss 0.641974
epoch 1152, loss 0.459747
epoch 1280, loss 0.572426
epoch 1408, loss 0.75268
epoch 1536, loss 0.590022
epoch 1664, loss 0.575098
epoch 1792, loss 0.572923
epoch 1920, loss 0.599682
epoch 2048, loss 0.448096
epoch 2176, loss 0.54313
epoch 2304, loss 0.493912
epoch 2432, loss 0.574845
epoch 2560, loss 0.741431
epoch 2688, loss 0.566074
epoch 2816, loss 0.694895
epoch 2944, loss 0.613319
epoch 3072, loss 0.67459
epoch 3200, loss 0.591605
epoch 3328, loss 0.786033
epoch 3456, loss 0.667261
epoch 3584, loss 0.596525
epoch 3712, loss 0.5153
epoch 3840, loss 0.631413
epoch 3968, loss 0.604747
epoch 4096, loss 0.732269
epoch 4224, loss 0.552647
epoch 4352, loss 0.516884
epoch 4480, loss 0.570639
epoch 4608, loss 0.66628
epoch 4736, loss 0.642137
epoch 4864, loss 0.715933
epoch 4992, loss 0.602036
epoch 5120, loss 0.634869
epoch 5248, loss 0.589828
epoch 5376, loss 0.715475
epoch 5504, loss 0.502332
epoch 5632, loss 0.544907
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0482639 0.0631478
0.0968257 0.0624736
0.0152616 0.00802556
0.106634 0.059195
0.0594116 0.0505274
0.088729 0.058744
-0.0131858 0.00630102
-0.013282 0.0759845
-0.0495799 -0.00141779
0.0384877 0.0589641
0.034034 -0.00259609
0.0373484 0.0522294
0.0659505 -0.0023944
0.0458115 0.0440093
0.0920961 0.0503085
0.000834798 -0.000543169
-7.68032e-07 0.0250976
0.0194912 -0.00254915
-0.0152583 -0.00105472
-0.0117817 -0.00268221
0.0462066 0.00243716
-0.0417624 0.00263676
0.000835126 0.00551711
0.0496681 0.0574466
0.0815268 0.0600215
0.0117847 -0.0012741
0.0867067 0.0592936
0.0396778 0.0575129
-0.0152556 -0.000779688
0.0140549 0.0516736
-0.0152583 0.00696262
0.0271163 0.00848885
0.0271187 -0.00142784
0.0529709 0.0117305
-0.0161248 0.0532141
-2.95455e-07 0.0106566
0.0520969 0.0585232
-0.00378297 -0.00124834
0.0945151 0.0608118
0.0182077 0.0378239
0.0594398 0.045364
0.0989678 0.0755611
0.0174335 0.0490066
0.121243 0.0548013
0.080382 0.0658278
-0.0449623 -0.000963043
0.0897923 0.0685067
0.0827282 0.0118047
0.0867119 0.0626477
0.0327313 0.0451443
0.0407513 0.0438723
0.0211182 0.0680494
0.0140549 0.0513179
0.0594399 0.0540494
0.110244 0.0720672
0.0234754 -0.00197735
0.0191276 0.0394644
0.0236482 0.0512005
-0.0414033 0.00243005
-5.97989e-06 0.0220437
-0.00737805 0.0398449
0.0384877 0.0575974
-0.0188599 0.00819097
-0.0234744 0.00116635
0.074953 0.0539966
0.0220539 0.0455282
0.0752955 0.0661312
-0.0250779 0.0410016
-0.0131851 -0.00172484
0.0397474 0.0518789
-3.40966e-05 -0.00196978
0.0397474 0.0526463
0.0541227 0.0608883
0.0871098 0.0644604
0.0234681 0.00127601
0.102034 0.0567002
0.022444 0.0579837
0.0497671 0.0546906
0.0803921 0.0679193
4.07609e-05 -0.000347762
0.081527 0.0705851
-0.0189887 -0.00172891
0.0977799 0.0694393
-1.48505e-07 0.00439603
0.0407515 0.0577902
-0.0414059 -0.002511
-0.00131656 0.00170479
0.0594349 0.0634891
-0.000835048 0.00348354
0.0594396 0.0588506
0.0594325 0.0626585
-0.000801651 0.00577879
0.0278228 0.0511011
-0.0152533 0.0158269
-0.0188593 -0.000673897
-0.0467978 0.00373053
0.0595095 0.0618952
-3.91322e-05 -0.00267563
0.0272988 0.00658822
-0.0267425 0.0145248
0.0417484 -0.00135052
0.0248169 -0.0010989
0.0739226 0.0730075
0.0904958 0.0587072
0.0964307 0.069151
0.0907408 0.0540287
0.088282 0.0691453
-0.0412685 0.00711304
0.0271109 0.0230229
-1.99995e-06 -0.00159
-0.129449 -0.00220365
0.0867199 0.0478698
0.0532557 -0.00168869
0.0706156 0.0642099
0.0318695 0.0490845
-7.99663e-06 -0.00194858
-0.0271193 -0.00198163
0.028139 0.047229
0.0897923 0.06045
0.0322983 0.0534829
0.032933 0.0615575
-0.0300728 -0.00192005
0.0706156 0.0633325
0.0562597 0.0619847
0.0781223 0.0554387
0.0707066 -0.0010213
0.0122504 0.0539816
0.125883 0.0758171
parameters: [ 9.     0.514  2.     1.2    5.744]. error: 178481.150156.
----------------------------
epoch 0, loss 1.06137
epoch 128, loss 0.737564
epoch 256, loss 0.63408
epoch 384, loss 0.610598
epoch 512, loss 0.774747
epoch 640, loss 0.631831
epoch 768, loss 0.811245
epoch 896, loss 0.515656
epoch 1024, loss 0.741712
epoch 1152, loss 0.566582
epoch 1280, loss 0.618315
epoch 1408, loss 0.601503
epoch 1536, loss 0.717767
epoch 1664, loss 0.564446
epoch 1792, loss 0.676064
epoch 1920, loss 0.566131
epoch 2048, loss 0.628683
epoch 2176, loss 0.57492
epoch 2304, loss 0.643543
epoch 2432, loss 0.6105
epoch 2560, loss 0.720785
epoch 2688, loss 0.579206
epoch 2816, loss 0.670479
epoch 2944, loss 0.570771
epoch 3072, loss 0.575445
epoch 3200, loss 0.719483
epoch 3328, loss 0.67921
epoch 3456, loss 0.67413
epoch 3584, loss 0.728647
epoch 3712, loss 0.539211
epoch 3840, loss 0.574119
epoch 3968, loss 0.516018
epoch 4096, loss 0.569754
epoch 4224, loss 0.642084
epoch 4352, loss 0.691075
epoch 4480, loss 0.552173
epoch 4608, loss 0.568743
epoch 4736, loss 0.486323
epoch 4864, loss 0.566737
epoch 4992, loss 0.62032
epoch 5120, loss 0.44194
epoch 5248, loss 0.588746
epoch 5376, loss 0.406009
epoch 5504, loss 0.513901
epoch 5632, loss 0.557053
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0482638 0.069199
0.0290868 0.0676984
0.0204143 0.00467461
0.0243594 0.0498974
0.00459996 -0.00107111
0.00131949 2.87414e-05
0.0977799 0.072106
0.0991789 0.0756629
-0.0495737 0.00274363
0.0210981 -0.0017573
-0.0144856 -0.00813843
0.121243 0.0706798
0.059435 0.0584219
0.0594424 0.0531732
0.014484 -0.0015147
0.0532677 -0.000215843
0.028139 0.0595817
0.0106903 -0.0025245
0.0901949 0.0584848
0.0986864 0.0619026
-0.00735889 0.0476829
0.100097 0.0694794
0.0261394 0.0620249
0.0210209 0.0588695
0.0305919 0.0564252
0.000835111 0.000227709
0.0417454 0.0051905
0.0300726 0.00253348
0.0248169 -0.00277981
0.090741 0.066278
0.0317697 0.0650452
0.0945038 0.0661523
0.0966595 0.0592613
0.0691985 0.069994
0.0346447 0.00121509
0.0131845 -0.00933391
0.000807032 -0.00431029
0.0117845 0.00110554
0.0716719 -0.00439326
0.0950803 0.0690307
-0.0131845 -0.0011705
0.0318787 0.00674066
0.0911183 0.0658985
0.027745 0.0435917
0.0964772 0.0704084
0.000793148 -0.00036592
-3.66166e-06 0.0151485
0.0621339 0.00125986
0.101814 0.0739541
0.0417454 0.0014516
0.0327313 0.0625546
-0.0111692 0.036698
0.0871097 0.0620223
0.134985 0.0713072
0.0477958 0.0670632
0.0327313 0.0645717
0.132184 0.0876029
0.0117782 0.000456673
0.0594399 0.0547128
0.0370769 0.0520239
0.0605996 0.0516359
0.0109586 0.00202065
0.0662803 -0.00389331
0.0330948 0.0514211
0.0625951 0.0630417
0.0611271 0.00160905
-0.0662833 0.000109342
0.121243 0.0691139
0.062595 0.06643
-0.0109774 0.000352212
-1.1614e-09 -0.00358681
0.0278228 0.0586357
0.0122243 0.039531
0.10664 0.0613803
-0.0109566 0.00541038
-0.0529698 -0.00254757
0.0710837 0.0625906
0.0907409 0.0673771
0.0910569 0.066757
0.032902 0.0587224
-0.0272995 -0.00606149
-0.0734361 -0.0033105
0.0477959 0.0629606
0.0662934 0.00154532
0.0594349 0.0582687
0.00131924 0.000989136
0.0248197 -0.00468628
0.0511643 0.059222
-0.00378297 0.00175449
0.074953 0.0614092
0.0727411 0.0705625
0.00894922 0.000648663
0.0968255 0.0722118
-0.0215356 -0.0185867
0.0327314 0.0591972
0.0594482 0.0494727
0.0859466 0.0736519
-3.66166e-06 -0.0105583
0.0964772 0.0723414
-0.0495799 -0.000811632
0.060558 0.0555677
0.0594376 0.0556008
-0.080798 -0.00215952
0.12945 -0.00630503
0.132178 0.0799696
-0.0267352 0.000886363
0.0509553 0.0533736
0.080382 0.05277
0.0464005 0.0626052
0.0599029 0.0681543
0.0368959 0.0580178
-0.0110929 0.0363731
0.0952318 0.0737603
0.0594397 0.0588037
0.0174339 0.0542965
0.0966587 0.0606363
0.0122142 0.04215
0.0593105 0.0532743
0.0492512 0.00492722
0.0224432 0.0556099
0.0813321 0.0622655
0.129445 -0.00906923
0.0271187 0.00720053
0.0611197 -0.0102509
0.0594396 0.0587875
0.0384776 0.04519
0.0857794 0.0670622
0.129916 0.0607586
parameters: [ 9.     0.284  2.     1.2    5.744]. error: 1.67615667833e+14.
----------------------------
epoch 0, loss 1.41192
epoch 128, loss 0.642582
epoch 256, loss 0.702685
epoch 384, loss 0.649877
epoch 512, loss 0.67378
epoch 640, loss 0.7178
epoch 768, loss 0.729446
epoch 896, loss 0.711181
epoch 1024, loss 0.646393
epoch 1152, loss 0.675709
epoch 1280, loss 0.679282
epoch 1408, loss 0.643509
epoch 1536, loss 0.699793
epoch 1664, loss 0.508718
epoch 1792, loss 0.699953
epoch 1920, loss 0.583685
epoch 2048, loss 0.685873
epoch 2176, loss 0.532046
epoch 2304, loss 0.633639
epoch 2432, loss 0.577962
epoch 2560, loss 0.538484
epoch 2688, loss 0.724685
epoch 2816, loss 0.637762
epoch 2944, loss 0.573293
epoch 3072, loss 0.640269
epoch 3200, loss 0.538523
epoch 3328, loss 0.600462
epoch 3456, loss 0.526447
epoch 3584, loss 0.591059
epoch 3712, loss 0.686285
epoch 3840, loss 0.793355
epoch 3968, loss 0.629934
epoch 4096, loss 0.599265
epoch 4224, loss 0.587898
epoch 4352, loss 0.663223
epoch 4480, loss 0.630718
epoch 4608, loss 0.62343
epoch 4736, loss 0.566585
epoch 4864, loss 0.57438
epoch 4992, loss 0.57264
epoch 5120, loss 0.752788
epoch 5248, loss 0.634553
epoch 5376, loss 0.454664
epoch 5504, loss 0.403558
epoch 5632, loss 0.593436
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0691984 0.0708402
0.0215366 0.000328553
0.0813321 0.0661016
0.0867334 0.0556486
0.0867332 0.0727084
0.0950805 0.0807813
0.0594396 0.0669119
-0.0234626 0.00110535
0.0496681 0.0716131
0.0594399 0.0684111
0.0140549 0.0632658
0.0971047 0.0745273
0.00131924 0.00450282
-0.0271278 0.00204947
0.0605996 0.0478969
-0.0532547 0.00143558
0.0305923 0.0578447
0.0511643 0.0617607
0.0305919 0.0584493
-0.080798 0.00280353
-0.00142113 0.000418183
0.0364958 0.0563439
0.0882823 0.0581138
0.059435 0.0703042
0.0373484 0.0584593
-0.0716782 0.00316908
-0.0385424 0.000945357
0.0220538 0.0657445
0.041765 -0.00271135
0.0283783 0.0406115
-0.0621343 0.00745825
0.0853255 0.070591
-0.0860366 0.00209741
0.0964766 0.0659772
0.0477956 0.0555116
-0.0300731 0.00407666
0.0927397 0.0686947
0.0188086 0.00506507
0.0594398 0.0631951
0.0562851 0.0646173
0.000786701 0.0029145
0.0237986 0.0653832
0.00460075 0.00424211
0.0920961 0.0591217
0.0277564 0.0531761
0.0271185 0.00276749
0.015257 0.00191111
0.0109857 0.0220119
-0.00701261 0.0535666
-0.0152556 -0.000906057
-0.0860431 0.00187863
0.0776735 -0.00141007
0.0781223 0.0504731
0.106645 0.0663576
-0.0611171 0.00236687
0.0707066 0.00360162
0.0594399 0.073612
0.0117847 0.00309317
0.088282 0.0560896
0.0492512 0.00491305
0.0417484 0.00739689
0.00931426 0.0457087
0.0937752 0.0756594
0.0234638 -0.00136635
2.96104e-08 0.00858582
0.0859467 0.0697865
0.0152616 0.0192029
0.0897923 0.0683398
-0.0161473 0.0574237
0.094504 0.060057
0.036794 0.0608019
0.0592543 0.0599204
0.0301505 0.0598244
0.0122142 0.0568549
0.0435741 0.050752
0.0210981 -0.000647846
0.00863029 0.0548367
0.0261392 0.0723152
-0.0659495 0.00380691
0.0764328 0.064381
0.0594398 0.0598847
0.121243 0.0549224
0.0867333 0.0747041
0.100097 0.0709142
0.0589764 0.0650353
-0.0449623 -0.000226169
0.0337179 0.0647199
1.96444e-05 0.00378092
0.0117845 0.00529105
-0.0210896 -0.00124626
0.0594452 0.0778395
0.0131848 0.0191728
0.00142251 0.00327085
0.0191272 0.0636312
0.0477959 0.0611356
0.0234638 0.0268517
-3.76332e-06 0.00437489
0.0866799 0.0564048
0.094504 0.0564225
-0.0662843 0.00107329
0.0477959 0.0708847
-0.0211014 0.00208891
0.000835111 0.00087563
0.0861486 0.0747538
0.0870053 0.0518345
-0.0300728 0.00477578
0.0662801 0.0025135
0.0727411 0.0642506
0.0318695 0.0470031
0.0477956 0.06988
0.025123 0.0678745
0.0335493 0.0441077
0.059435 0.0630473
0.0781223 0.0554123
0.0897923 0.0746121
0.0781223 0.0504731
0.0950802 0.0624216
0.0730675 0.0658291
0.0594397 0.0610318
0.102035 0.0585391
0.0109781 -0.00340887
0.0449751 0.0614519
-0.00628221 0.0500106
0.0589763 0.0639832
0.0594324 0.0553445
0.0764428 0.0707874
0.0997462 0.0740412
0.0859467 0.0678508
parameters: [ 9.     0.369  2.     1.2    5.744]. error: 452733162674.0.
----------------------------
epoch 0, loss 1.2836
epoch 128, loss 0.976341
epoch 256, loss 0.811374
epoch 384, loss 0.847108
epoch 512, loss 0.764434
epoch 640, loss 0.700438
epoch 768, loss 0.684656
epoch 896, loss 0.582164
epoch 1024, loss 0.541169
epoch 1152, loss 0.634343
epoch 1280, loss 0.464171
epoch 1408, loss 0.494046
epoch 1536, loss 0.537283
epoch 1664, loss 0.660287
epoch 1792, loss 0.700617
epoch 1920, loss 0.707728
epoch 2048, loss 0.606664
epoch 2176, loss 0.865105
epoch 2304, loss 0.603369
epoch 2432, loss 0.620097
epoch 2560, loss 0.695758
epoch 2688, loss 0.614053
epoch 2816, loss 0.637721
epoch 2944, loss 0.641506
epoch 3072, loss 0.621113
epoch 3200, loss 0.737167
epoch 3328, loss 0.659343
epoch 3456, loss 0.557026
epoch 3584, loss 0.436199
epoch 3712, loss 0.725713
epoch 3840, loss 0.619006
epoch 3968, loss 0.577982
epoch 4096, loss 0.696844
epoch 4224, loss 0.7082
epoch 4352, loss 0.702906
epoch 4480, loss 0.492834
epoch 4608, loss 0.554323
epoch 4736, loss 0.596018
epoch 4864, loss 0.569387
epoch 4992, loss 0.373437
epoch 5120, loss 0.649369
epoch 5248, loss 0.524782
epoch 5376, loss 0.615341
epoch 5504, loss 0.577105
epoch 5632, loss 0.590774
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0333262 0.0654317
0.0243594 0.0438159
0.0497674 0.0522739
1.96444e-05 0.0020133
0.0497671 0.0499407
0.0691985 0.0655329
0.046209 0.0237617
0.0373484 0.0470993
-0.00549668 0.0583708
0.0983621 0.0621436
0.080392 0.063292
-0.0106908 0.00105618
0.0417484 0.00257995
0.0582236 0.0583234
0.0997468 0.060213
0.0851609 0.0513863
-1.32922e-07 0.00255168
0.0323163 0.00277245
-0.00726943 0.0471837
0.0204143 0.000840322
0.0417702 0.0164944
0.0368959 0.0599346
0.102035 0.0485269
0.0582389 0.0535327
0.0281393 0.0530315
0.076438 0.0599089
0.0625949 0.0578685
0.0385291 0.00214525
1.74905e-07 0.000844308
0.0596272 0.0569463
0.0952317 0.0603823
0.0739227 0.0725118
-0.0247813 -0.00226915
-1.92327e-05 0.0140311
0.0991788 0.0623769
0.0857795 0.0486318
0.0989675 0.0763381
0.0131861 0.00199541
0.0710835 0.0524999
0.0861488 0.0716455
0.0414025 -7.25492e-05
0.0594399 0.0462099
0.0346452 0.00240948
-0.0109566 0.0125659
-0.0188057 0.00169815
0.0813268 0.051465
0.0191272 0.0360046
0.129445 0.0014469
-0.00894125 -0.00097518
8.40829e-06 0.00436869
0.0859465 0.0529905
0.081797 0.0469782
0.0286789 0.0572837
0.0594452 0.0790487
0.0594398 0.0684871
0.0861486 0.0606302
0.0983621 0.0700155
0.0477959 0.0464523
0.126127 0.0764292
0.102035 0.0673538
0.0278231 0.0616729
0.0477958 0.0479503
0.0223968 0.0438571
0.0414025 0.000299611
-0.0417624 -8.97526e-05
0.088729 0.0713108
0.093775 0.0694412
-0.0118768 0.0269093
0.0997462 0.0515258
0.0321466 0.0529395
0.0989677 0.0674459
0.0301505 0.0647794
0.0261395 0.0665652
0.0449751 0.0720176
0.0781227 0.0612125
0.0589765 0.0624331
0.0384776 0.0500874
0.0859466 0.0713134
-0.053257 -0.00403532
0.0477959 0.0625319
-0.0414059 -0.000309422
0.0317698 0.0663297
0.0739226 0.0743004
0.061289 0.0774378
0.0867119 0.0560534
0.0642209 0.0518617
0.022444 0.0594707
0.121243 0.053412
0.0867199 0.070175
0.0362234 0.00384897
0.0593312 0.0607914
0.110244 0.0736569
0.0424369 0.0531128
0.0532657 0.00478633
0.0907409 0.0727047
0.0594324 0.0613575
0.125873 0.0663435
0.0950803 0.0768141
0.0631173 0.00105114
-0.0271121 0.00565739
0.0204119 -0.00188011
0.0131845 0.0041852
0.0860364 7.08627e-07
0.0319944 0.0463445
-1.99995e-06 0.000302554
0.0133878 0.0806946
-0.00891927 0.00142665
-0.0117817 0.00191126
0.0710836 0.0624329
0.0295687 0.000288551
0.0106905 0.00387778
0.0527576 0.0505727
0.132184 0.0899391
0.0752422 0.0520845
0.0268142 0.0540247
0.0730676 0.0638568
0.0234704 0.00221138
-0.034636 0.000957911
0.0662803 -0.00177973
0.100668 0.0378131
0.0783558 0.0628567
-0.0111018 0.0483613
0.0594399 0.0520411
0.109549 0.0503023
0.0207193 0.033251
-0.0716682 0.0032384
0.081797 0.0557658
0.0593346 0.0511765
parameters: [ 9.     0.456  2.     1.2    5.744]. error: 51380481076.5.
----------------------------
epoch 0, loss 1.14193
epoch 128, loss 0.658993
epoch 256, loss 0.592031
epoch 384, loss 0.635529
epoch 512, loss 0.715102
epoch 640, loss 0.662267
epoch 768, loss 0.567863
epoch 896, loss 0.587333
epoch 1024, loss 0.829428
epoch 1152, loss 0.599548
epoch 1280, loss 0.597877
epoch 1408, loss 0.730607
epoch 1536, loss 0.530711
epoch 1664, loss 0.480065
epoch 1792, loss 0.566539
epoch 1920, loss 0.604518
epoch 2048, loss 0.649611
epoch 2176, loss 0.616559
epoch 2304, loss 0.665965
epoch 2432, loss 0.873817
epoch 2560, loss 0.567711
epoch 2688, loss 0.535251
epoch 2816, loss 0.625391
epoch 2944, loss 0.600861
epoch 3072, loss 0.497215
epoch 3200, loss 0.487837
epoch 3328, loss 0.61177
epoch 3456, loss 0.514559
epoch 3584, loss 0.56637
epoch 3712, loss 0.750793
epoch 3840, loss 0.558028
epoch 3968, loss 0.707751
epoch 4096, loss 0.670819
epoch 4224, loss 0.539031
epoch 4352, loss 0.49621
epoch 4480, loss 0.719553
epoch 4608, loss 0.521621
epoch 4736, loss 0.581714
epoch 4864, loss 0.541133
epoch 4992, loss 0.60138
epoch 5120, loss 0.611617
epoch 5248, loss 0.602534
epoch 5376, loss 0.557242
epoch 5504, loss 0.52586
epoch 5632, loss 0.545232
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0335493 0.0373852
0.0368964 0.0548549
-0.0247793 0.00273829
0.0724741 0.0547949
0.081527 0.048631
0.106634 0.0590793
0.093775 0.073411
0.0910567 0.0706389
0.124277 0.0765452
0.0278229 0.0383015
-0.0131845 0.00743078
-0.00735889 0.0589664
0.0594398 0.0547421
0.112932 0.0835486
-0.0462049 -0.00633068
-0.0860431 -0.00539913
6.39156e-06 -0.00583286
0.0724734 0.0521885
-0.0495799 -0.00397564
0.0727411 0.043769
0.0710837 0.0372937
-0.0529659 0.000805844
-0.0716782 0.000820503
-0.00378521 -0.00289792
0.106624 0.0727226
0.0631196 -0.00478826
0.0204045 -0.00410992
0.072741 0.0459346
0.0318695 0.0517467
0.0113418 -0.00480127
0.00080867 -0.00461057
0.0964772 0.055209
-0.0468001 0.000820194
0.0594399 0.0527815
-0.0271121 -0.00460733
0.080382 0.0635524
0.106629 0.0787314
-7.68032e-07 -0.00453506
0.0594398 0.057965
0.133575 0.0796786
0.0594301 0.0501072
-0.00484737 0.0422307
-1.32922e-07 -0.00325402
0.0384777 0.0623511
0.0594396 0.058396
-0.0177194 0.0152988
0.0911173 0.0547504
-0.0267352 0.00576333
0.0267443 0.0151206
0.0859466 0.0462748
-0.0734384 -0.00496261
0.0853255 0.041881
0.0495801 0.000739419
-0.0776845 -0.000357038
0.0870049 0.0653126
0.0370769 0.0539245
-0.0385433 -0.00323084
0.0815268 0.0477225
0.133575 0.0716823
-0.00737805 0.0478954
0.0650733 0.0514794
-0.0417488 -0.00255971
-0.0111732 0.0506746
0.0532557 -0.00455949
0.088729 0.0690528
0.0867332 0.0615427
0.081527 0.05771
0.0691072 0.0545785
0.0271109 -0.00466746
0.0920561 0.0611697
0.0594466 0.0757945
0.0267779 0.058409
0.0977802 0.0853786
0.0281391 0.0348538
0.0631163 0.00318
0.0248169 -0.00186815
0.0594398 0.0579012
-0.0131855 -0.00422162
-0.0144843 0.00458258
0.0866747 0.0625718
-0.0417792 -0.00383862
0.0764428 0.0514735
0.032933 0.0327387
0.0859465 0.0497372
0.0901948 0.0642945
0.0204119 0.00208478
0.0594397 0.0457667
0.0329331 0.0405074
0.060558 0.062449
-0.0118765 0.0410004
0.0424215 0.0600442
0.0594325 0.0628336
0.0594398 0.0499389
-0.0132848 0.0714592
0.0323166 -0.00107799
-0.00894944 -0.00515353
0.0152591 -0.00293275
0.027745 0.0464606
-0.00331175 0.0400353
0.0727411 0.043769
-0.0318771 0.010922
0.0594379 0.0819568
-0.0132848 0.0728765
0.022215 0.0466184
0.0390895 -0.00440568
0.028139 0.0347577
0.0966587 0.0658311
0.0538297 0.0565762
-0.0492483 -0.00324601
0.0822309 0.0634386
0.0318695 0.0460981
0.0611263 -0.00220286
-3.66166e-06 -0.0049919
0.0594395 0.0442626
0.100097 0.0721209
-0.0109489 -0.00371047
0.0625951 0.062353
1.01848e-06 -0.00357376
0.0477959 0.0374838
3.36628e-05 -0.00371593
0.0223972 0.0416007
0.059181 0.0510683
0.0251233 0.0505495
0.0920961 0.069542
-0.0295676 -0.00192089
0.0220539 0.0408786
0.0122504 0.0586028
2.5668e-06 0.0240334
parameters: [ 9.     0.401  2.     1.2    5.744]. error: 937692682.78.
----------------------------
epoch 0, loss 1.12021
epoch 128, loss 0.545283
epoch 256, loss 0.576703
epoch 384, loss 0.614634
epoch 512, loss 0.770935
epoch 640, loss 0.685007
epoch 768, loss 0.831666
epoch 896, loss 0.713876
epoch 1024, loss 0.628046
epoch 1152, loss 0.53848
epoch 1280, loss 0.634748
epoch 1408, loss 0.604701
epoch 1536, loss 0.778976
epoch 1664, loss 0.78129
epoch 1792, loss 0.636073
epoch 1920, loss 0.660294
epoch 2048, loss 0.569695
epoch 2176, loss 0.598591
epoch 2304, loss 0.609531
epoch 2432, loss 0.69919
epoch 2560, loss 0.703478
epoch 2688, loss 0.533331
epoch 2816, loss 0.675479
epoch 2944, loss 0.540247
epoch 3072, loss 0.577479
epoch 3200, loss 0.597835
epoch 3328, loss 0.666571
epoch 3456, loss 0.637719
epoch 3584, loss 0.733764
epoch 3712, loss 0.780528
epoch 3840, loss 0.448876
epoch 3968, loss 0.632532
epoch 4096, loss 0.4736
epoch 4224, loss 0.538414
epoch 4352, loss 0.720121
epoch 4480, loss 0.573677
epoch 4608, loss 0.610463
epoch 4736, loss 0.549896
epoch 4864, loss 0.40695
epoch 4992, loss 0.54701
epoch 5120, loss 0.666386
epoch 5248, loss 0.517076
epoch 5376, loss 0.601914
epoch 5504, loss 0.603842
epoch 5632, loss 0.646154
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0210947 -0.00301037
0.0174339 0.0451984
0.0594399 0.0710821
0.0734322 0.0377377
0.130005 0.0575089
-0.0271111 -0.00278663
0.00894166 0.00254237
0.0424581 0.0523972
0.0901948 0.0654555
0.12614 0.0574577
-0.0295559 -0.0036299
0.0220539 0.0625659
0.130756 0.0848406
0.00142251 -0.00662117
0.0268142 0.0490739
0.0996717 0.0827582
0.0707072 -0.00123288
0.0367961 0.00410039
-0.0023637 0.0595465
0.0599029 0.0839195
0.0424529 0.0466284
0.0397474 0.0492258
0.10546 0.074093
-0.063123 -0.00275984
-1.17339e-07 -0.00265105
-0.0267425 -0.00598092
4.57767e-08 0.0167709
0.0204143 -0.00601469
0.0752422 0.0560541
3.40452e-06 0.00565151
0.0594919 0.050375
-0.0118765 0.0690717
0.0210958 0.000999157
-0.000807624 -0.00170153
0.0267419 -0.000761441
0.0582589 0.0489493
0.0529729 0.0229407
-0.0207367 -0.00397238
0.0412776 0.0246334
0.0477957 0.0754371
0.080382 0.0637397
0.056285 0.0746728
-0.0734426 -0.00634924
0.0131848 -5.78902e-05
0.0477957 0.0706969
0.0362233 0.00682328
0.0477958 0.0777397
0.0251231 0.0562114
0.0871098 0.0784259
0.0122452 0.0437865
0.0251234 0.0592148
0.102034 0.0735583
0.0594494 0.0701889
0.0373475 0.0497594
0.106619 0.0647091
0.0286795 0.0483966
0.0439209 0.0502378
0.124277 0.0670726
-0.0385437 0.00227574
0.0327315 0.0751937
0.0594397 0.0744194
0.0424428 0.0502864
0.0861486 0.0828334
0.0327314 0.0767087
0.0321466 0.073179
0.0986862 0.0668765
0.0396777 0.0480745
-0.0271121 0.00179876
0.0037846 0.000825931
0.0496683 0.0740583
0.0860523 -0.000129278
0.0952318 0.0808067
0.00891968 0.00846913
0.0593133 0.0500381
0.00378707 0.00286585
-0.0189887 0.00151544
0.0177216 0.018277
0.126222 0.0598349
0.0594399 0.0786634
-0.0204101 0.00237065
0.0594397 0.0749527
0.0859466 0.0847033
0.0594467 0.0833715
-0.049248 -0.00407451
0.0210209 0.0520068
0.121243 0.0840566
0.0495768 0.00137989
-0.0417491 0.00442509
0.0329021 0.0506753
0.0211183 0.0546585
0.0223972 0.0476055
0.10665 0.0617285
0.0710836 0.0784621
0.0857795 0.0608381
0.0927397 0.0819157
0.0659615 0.0185507
0.0248169 -0.00441304
0.0997468 0.0667321
0.0268142 0.0413651
0.0321467 0.0679499
-0.0659495 -0.00411013
0.00891968 0.00755156
0.0594397 0.0749527
0.0710837 0.0734266
0.0412725 -0.00452199
0.059367 0.0537314
0.0191276 0.0487956
0.060558 0.0472898
0.0594396 0.0803877
0.0210915 -0.00405634
0.124311 0.0713945
0.0662931 0.00257068
0.143948 0.0674441
-0.10012 -0.000962838
0.0346348 -0.000934768
0.0319943 0.0769679
0.081527 0.0584284
0.0594027 0.0519555
0.0152591 0.014577
-0.0023637 0.0501978
0.121243 0.0904143
-1.92327e-05 -0.00202905
0.0495801 0.0015369
0.0424268 0.0490645
0.0318693 0.0513384
0.0251234 0.0522043
0.0968257 0.081893
0.0435741 0.0548241
parameters: [ 9.     0.434  2.     1.2    5.744]. error: 812420145735.0.
----------------------------
epoch 0, loss 1.26566
epoch 128, loss 0.865586
epoch 256, loss 0.444907
epoch 384, loss 0.648495
epoch 512, loss 0.489741
epoch 640, loss 0.812635
epoch 768, loss 0.653805
epoch 896, loss 0.645726
epoch 1024, loss 0.579891
epoch 1152, loss 0.596299
epoch 1280, loss 0.554662
epoch 1408, loss 0.598071
epoch 1536, loss 0.515074
epoch 1664, loss 0.580564
epoch 1792, loss 0.788902
epoch 1920, loss 0.592732
epoch 2048, loss 0.582375
epoch 2176, loss 0.508276
epoch 2304, loss 0.668842
epoch 2432, loss 0.805019
epoch 2560, loss 0.696808
epoch 2688, loss 0.513272
epoch 2816, loss 0.707364
epoch 2944, loss 0.579159
epoch 3072, loss 0.593267
epoch 3200, loss 0.676832
epoch 3328, loss 0.58337
epoch 3456, loss 0.729099
epoch 3584, loss 0.572747
epoch 3712, loss 0.516839
epoch 3840, loss 0.642788
epoch 3968, loss 0.709038
epoch 4096, loss 0.575671
epoch 4224, loss 0.66804
epoch 4352, loss 0.581706
epoch 4480, loss 0.710053
epoch 4608, loss 0.573362
epoch 4736, loss 0.543127
epoch 4864, loss 0.590379
epoch 4992, loss 0.646507
epoch 5120, loss 0.652501
epoch 5248, loss 0.592754
epoch 5376, loss 0.572879
epoch 5504, loss 0.599142
epoch 5632, loss 0.538885
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
5.31452e-07 -0.00144807
0.0706156 0.0702334
1.95996e-07 0.0125955
0.038792 0.0610718
0.0236481 0.0635134
0.0983621 0.0693286
0.0217933 0.0599888
-0.0385427 0.000383888
-7.68032e-07 -0.00359643
-0.129445 0.0103423
0.0706156 0.0702334
0.028139 0.0557454
0.00378707 0.0136134
0.0815268 0.0636856
0.0327315 0.0552448
0.0621308 -5.75847e-05
-3.17974e-05 -0.00222824
0.0417454 0.000941292
0.0449639 0.0256998
0.0222152 0.0662934
-0.0827278 0.00205976
4.07333e-06 -0.00266879
0.0248076 0.00664628
0.133575 0.0718041
0.0346353 -0.000686723
0.0860587 0.0354975
0.0661223 0.0701873
-0.0827278 0.00266603
-0.0707068 0.0032905
0.101814 0.0711478
0.0375382 0.0642621
0.0286789 0.0534621
0.0243594 0.0642797
0.0236481 0.0674854
0.0593665 0.0623162
0.0978866 0.0744521
-0.0204122 0.00756593
0.0106905 0.00776063
0.0131855 -0.00240326
0.022444 0.0618768
0.014471 0.00112517
0.135027 0.0660957
0.00596587 0.0671111
0.100668 0.0510843
-0.0194897 -0.00778065
0.0800819 0.0692654
0.0662801 0.00695245
-0.000805986 0.014527
0.0791268 0.0623044
0.0261394 0.0684336
0.081797 0.069364
0.0527577 0.065531
0.0477957 0.0663973
0.059407 0.0562796
0.0867199 0.0556342
0.000794682 0.0202409
-0.0346416 -0.000630773
0.0860587 0.026938
0.0594396 0.0637218
0.0594398 0.0688088
-0.0234626 0.0252069
0.0204045 0.00570344
0.0458117 0.0626079
0.0199307 0.066555
0.0397474 0.0585256
0.0978866 0.0744521
0.0621339 0.00882314
0.0329331 0.0570474
0.00250001 0.00162669
-0.0272904 0.0177232
0.0815268 0.0593711
0.00894922 0.01317
0.0867334 0.055512
-0.00142797 -0.00226544
-0.00893962 0.00110706
0.059435 0.0605095
4.07333e-06 0.023041
0.0977799 0.0659342
-0.0234626 0.0131985
-0.0385431 0.00268237
0.0271115 -6.81608e-05
0.0188597 -0.00159119
0.0594204 0.0525806
0.0851511 0.0618094
0.0532578 -0.00331241
0.0878299 0.0689805
0.0207193 0.0638127
0.0205126 0.0413456
0.104819 0.0698915
0.097887 0.0747683
0.0791268 0.0614831
0.0394802 0.0156619
0.0317697 0.0682816
-0.0385437 0.0127371
-0.0716782 -0.00327409
0.0734394 -0.00573006
0.0859467 0.0669765
0.0710835 0.0713956
0.0764118 0.0687127
-0.0152533 0.00727219
0.059367 0.0640887
0.00863092 0.0479271
0.0594116 0.0592737
0.0691986 0.0677328
0.043628 0.0563399
0.0857794 0.0634976
0.081527 0.0619118
4.57767e-08 0.000444849
0.028139 0.0707143
-0.00250418 -0.000909026
0.072741 0.0573099
0.124277 0.0717393
-0.0495799 -0.00247139
0.0878299 0.0662911
0.0261393 0.0522048
0.0776735 -0.0045081
0.0199309 0.064674
0.0037846 0.00031883
-2.15513e-06 6.72237e-05
0.0368964 0.0670035
0.0527577 0.0669475
0.0286795 0.0540573
-0.0417422 0.000371833
0.0859465 0.0667745
-0.049248 -0.000672664
0.049499 -0.00495227
0.0776855 0.0119846
-0.000835392 -0.00195054
parameters: [ 9.     0.413  2.     1.2    5.744]. error: 10469374225.2.
----------------------------
epoch 0, loss 1.14938
epoch 128, loss 0.603899
epoch 256, loss 0.690464
epoch 384, loss 0.740275
epoch 512, loss 0.716041
epoch 640, loss 0.605093
epoch 768, loss 0.602338
epoch 896, loss 0.615575
epoch 1024, loss 0.403647
epoch 1152, loss 0.635576
epoch 1280, loss 0.715363
epoch 1408, loss 0.579699
epoch 1536, loss 0.568519
epoch 1664, loss 0.638284
epoch 1792, loss 0.615412
epoch 1920, loss 0.633055
epoch 2048, loss 0.642248
epoch 2176, loss 0.696047
epoch 2304, loss 0.775032
epoch 2432, loss 0.689408
epoch 2560, loss 0.802582
epoch 2688, loss 0.572199
epoch 2816, loss 0.64866
epoch 2944, loss 0.594251
epoch 3072, loss 0.819022
epoch 3200, loss 0.581613
epoch 3328, loss 0.71961
epoch 3456, loss 0.565091
epoch 3584, loss 0.589687
epoch 3712, loss 0.51861
epoch 3840, loss 0.73862
epoch 3968, loss 0.583166
epoch 4096, loss 0.718894
epoch 4224, loss 0.650798
epoch 4352, loss 0.731967
epoch 4480, loss 0.569531
epoch 4608, loss 0.574349
epoch 4736, loss 0.637321
epoch 4864, loss 0.524113
epoch 4992, loss 0.690162
epoch 5120, loss 0.806488
epoch 5248, loss 0.471287
epoch 5376, loss 0.440174
epoch 5504, loss 0.576761
epoch 5632, loss 0.695713
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0677309 0.0670102
0.0813321 0.0555319
0.0417448 -0.00594915
0.0983621 0.0562656
0.126171 0.0680964
0.0991788 0.0632134
-0.0110929 0.0586374
0.121243 0.0512779
0.100667 0.0506676
-0.049248 -0.000494844
-0.0131854 -0.00495948
0.0710836 0.0638571
0.0144707 0.00710679
0.0594162 0.0626965
0.0385441 0.00353263
0.0131845 0.0198324
0.0594397 0.0589278
0.0122243 0.0626079
-0.00378521 -0.0049957
-0.0188593 -0.00511844
-0.0385434 -0.0045413
-0.000835063 -0.00257167
0.0131845 -0.00300371
-0.0215238 -0.00426982
0.0286789 0.0422867
0.0901948 0.0646058
0.0117782 -0.00599112
0.079197 0.0671162
0.022444 0.0537314
0.0582236 0.0595441
0.025123 0.0713783
0.0247806 0.00877029
-0.0462148 0.00777633
0.0362237 -0.00285586
0.0724734 0.0571512
-0.0362233 0.00576472
0.0815268 0.0453007
0.110244 0.0486235
0.10664 0.0763067
-0.0417425 0.000939664
0.0991789 0.0601717
0.0122295 0.0635957
0.0267419 0.00371133
0.0243586 0.032144
0.0592543 0.0565666
0.0211182 0.0691693
0.0211185 0.0708763
0.0394799 -0.00385041
-0.0716682 -0.00643362
0.0822309 0.0604971
1.74905e-07 0.00326041
-0.0416401 -0.00500584
0.0599031 0.0498454
-0.00726943 0.0503513
0.00596546 0.061268
0.0496683 0.0505898
-0.0860447 0.00498679
-0.0492483 -0.00530533
0.0224432 0.0406921
0.0321466 0.0464799
0.0594162 0.0641043
-0.0662867 -0.0053376
0.110244 0.0767477
-7.99663e-06 0.00882851
-7.99663e-06 0.012461
0.0210212 0.0718377
0.0594397 0.054417
0.0611204 0.00768783
0.00460075 -0.00528114
0.0594473 0.0624992
0.0594482 0.0810622
0.0950804 0.0406522
0.0977799 0.0805005
0.0496683 0.0462373
-0.0827278 -0.00493042
0.0327314 0.0455956
0.0122295 0.0488971
0.0405236 0.0491119
0.0131851 -0.00438603
0.0283785 0.04112
0.0189868 0.00389335
-2.86114e-05 -0.00406546
0.112932 0.0820803
0.0268195 0.0537566
0.126127 0.0752027
0.0594759 0.0622809
0.0277564 0.061267
0.0188088 -0.0041739
0.0606162 0.0599055
0.0210209 0.0711694
0.0691985 0.0571012
0.0370769 0.0616923
-0.0109489 -0.00539768
0.0594297 0.0587157
-0.0416401 -0.00464071
0.0321586 0.0592724
-0.0109774 -0.00377972
0.0730675 0.0573287
0.0865812 0.0612019
0.0248076 -0.00436846
0.0317698 0.0369412
2.5668e-06 -0.0030407
-0.000835392 -0.00345278
0.0189852 0.0216087
0.0194981 0.00742726
0.0691985 0.0493793
0.102035 0.0636555
0.0152591 0.00497491
0.0329332 0.0577695
0.0261391 0.0615014
1.96444e-05 -0.00418109
0.00131924 -0.00513805
0.0237988 0.0513861
0.0599612 0.0599155
-0.0621346 -0.00459739
-1.99995e-06 0.00237052
0.110244 0.0703086
0.056285 0.0462241
-0.0385427 -0.00518601
0.093775 0.0817258
-0.0417624 -0.00479752
0.0859466 0.0718506
-2.86114e-05 -0.00454358
0.0248076 -0.00549167
0.0593758 0.0525304
0.0594398 0.0503518
0.0267419 -0.00441831
-0.0267352 -0.00549542
parameters: [ 9.     0.427  2.     1.2    5.744]. error: 474431941.588.
----------------------------
epoch 0, loss 1.1324
epoch 128, loss 0.838073
epoch 256, loss 0.75431
epoch 384, loss 0.508432
epoch 512, loss 0.606433
epoch 640, loss 0.666151
epoch 768, loss 0.70394
epoch 896, loss 0.566062
epoch 1024, loss 0.693064
epoch 1152, loss 0.646289
epoch 1280, loss 0.749801
epoch 1408, loss 0.52268
epoch 1536, loss 0.501939
epoch 1664, loss 0.590126
epoch 1792, loss 0.519835
epoch 1920, loss 0.677855
epoch 2048, loss 0.723354
epoch 2176, loss 0.719295
epoch 2304, loss 0.529948
epoch 2432, loss 0.580378
epoch 2560, loss 0.524591
epoch 2688, loss 0.64374
epoch 2816, loss 0.560759
epoch 2944, loss 0.621814
epoch 3072, loss 0.499093
epoch 3200, loss 0.502159
epoch 3328, loss 0.676777
epoch 3456, loss 0.51384
epoch 3584, loss 0.454527
epoch 3712, loss 0.611851
epoch 3840, loss 0.585147
epoch 3968, loss 0.537441
epoch 4096, loss 0.558758
epoch 4224, loss 0.669516
epoch 4352, loss 0.460293
epoch 4480, loss 0.504832
epoch 4608, loss 0.589395
epoch 4736, loss 0.54881
epoch 4864, loss 0.430676
epoch 4992, loss 0.613862
epoch 5120, loss 0.584731
epoch 5248, loss 0.701243
epoch 5376, loss 0.600596
epoch 5504, loss 0.567929
epoch 5632, loss 0.387671
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0492512 0.0061231
0.025123 0.0657626
0.0261391 0.0527761
0.00863088 0.0480767
0.0997462 0.070718
-0.0188606 0.00250432
0.121243 0.0665833
0.0168394 0.0529275
-0.0860447 0.00302238
0.0508582 0.0572937
0.0594399 0.0590485
0.101441 0.0738065
0.0594398 0.0512387
0.0910567 0.0639374
0.0661223 0.0735738
0.0870053 0.0628667
-0.0210896 0.00728975
0.0261391 0.0511853
0.0853255 0.0651092
0.0272894 0.00747431
-0.0716782 0.005902
-0.0494953 0.00216439
0.109566 0.0633973
0.0582236 0.0569788
0.00143008 0.00060292
0.0594396 0.0591768
0.0194912 -0.00193876
0.0109586 0.00566474
0.019701 0.0502469
0.0820225 0.0737716
1.95996e-07 0.00781361
-0.0318748 0.00252241
0.0859865 0.0701569
0.0318695 0.0475586
0.0290868 0.058752
-0.0495796 0.00531286
0.060558 0.0556755
0.0140549 0.0567436
0.080392 0.0791622
0.09274 0.0636115
0.000802697 0.00360004
0.0599612 0.0621147
0.0436381 0.0705593
0.0272916 0.016625
0.0495763 0.00682286
0.0691984 0.0641809
-0.0449599 0.000583199
0.0329331 0.0550128
-1.32922e-07 0.00666628
0.0626054 0.0701418
0.0268195 0.0670878
6.7959e-07 0.00215684
0.032933 0.0471743
-1.63011e-07 0.00116598
-0.0462049 0.00206226
9.99101e-07 0.029312
0.0625951 0.0619224
0.0461384 0.0574949
0.0594399 0.0446993
0.0261394 0.0494894
0.0593105 0.0577194
0.0204028 0.022294
0.0223968 0.0455678
0.0867333 0.0730438
0.0594398 0.0584714
0.0594399 0.0633837
0.0281392 0.0503155
-1.1614e-09 0.00267682
-0.0207292 -1.83686e-05
0.0207193 0.0452478
0.0236482 0.0439629
0.0168394 0.0470096
0.0439209 0.0465566
0.0210915 -0.00219368
0.059435 0.0646794
0.0133907 0.0584828
0.0390889 0.001104
-0.0209824 -0.000165976
-0.0131858 0.00669031
-0.000835048 0.00249839
0.0211031 0.00140505
-0.0385424 0.00231272
0.0611263 0.00328616
-0.0734361 0.00409193
0.0261391 0.0565136
-0.0417491 0.00575845
0.0594398 0.0646234
0.0964305 0.0584508
-0.0467928 0.0016634
0.0205126 0.0442287
0.028139 0.0562006
0.0234681 0.000228082
0.10546 0.0689673
0.032195 0.0616887
0.0329021 0.063113
0.0907408 0.0587476
0.059181 0.0625288
0.0496683 0.0602062
0.0207193 0.0425457
0.038544 0.00730566
0.0464005 0.056468
0.0267419 0.002873
0.0261391 0.0533828
-0.0207367 0.00162632
0.0800819 0.0601871
0.0813321 0.0696762
0.0174335 0.0457355
0.0594424 0.05584
-0.0462076 0.0118582
0.0458117 0.0515518
-0.000805986 0.00770837
0.0327315 0.0497805
0.0594398 0.0569104
0.0318695 0.0603885
8.40829e-06 0.000538555
0.027823 0.0480534
0.0387924 0.0569237
0.0113418 -0.000150318
-0.000835392 0.0294125
0.0710837 0.0640757
-0.00700262 0.0597577
0.0827282 0.00724265
0.0322983 0.0619939
0.0594395 0.059997
0.0964305 0.0599488
0.0458117 0.0632792
0.0865812 0.0676887
0.0174339 0.0421991
parameters: [ 9.     0.421  2.     1.2    5.744]. error: 3173737098.44.
----------------------------
epoch 0, loss 1.05661
epoch 128, loss 1.05683
epoch 256, loss 0.896269
epoch 384, loss 0.790807
epoch 512, loss 0.893886
epoch 640, loss 0.759292
epoch 768, loss 0.772055
epoch 896, loss 0.688399
epoch 1024, loss 0.779959
epoch 1152, loss 0.798667
epoch 1280, loss 0.866666
epoch 1408, loss 0.63613
epoch 1536, loss 0.699251
epoch 1664, loss 0.89169
epoch 1792, loss 0.683329
epoch 1920, loss 0.883666
epoch 2048, loss 0.757862
epoch 2176, loss 0.658206
epoch 2304, loss 0.614942
epoch 2432, loss 0.90781
epoch 2560, loss 0.587376
epoch 2688, loss 0.772563
epoch 2816, loss 0.668954
epoch 2944, loss 0.810864
epoch 3072, loss 0.620211
epoch 3200, loss 0.66507
epoch 3328, loss 0.658953
epoch 3456, loss 0.59574
epoch 3584, loss 0.550981
epoch 3712, loss 0.453482
epoch 3840, loss 0.666374
epoch 3968, loss 0.715993
epoch 4096, loss 0.717402
epoch 4224, loss 0.533165
epoch 4352, loss 0.766646
epoch 4480, loss 0.744379
epoch 4608, loss 0.573628
epoch 4736, loss 0.597539
epoch 4864, loss 0.539279
epoch 4992, loss 0.698876
epoch 5120, loss 0.554558
epoch 5248, loss 0.630205
epoch 5376, loss 0.621655
epoch 5504, loss 0.607166
epoch 5632, loss 0.723497
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0734384 -0.00485931
0.0188086 -0.00524453
0.0396776 0.0599487
1.96444e-05 0.00799324
0.00856342 0.0219772
0.0477957 0.0604812
0.0189852 0.00459642
0.100097 0.062045
-0.000835048 0.0116446
0.0375434 0.0592868
0.022215 0.0611038
0.0871095 0.0593795
-0.0272977 0.00269073
0.00080867 0.0060614
0.0396777 0.0589769
-0.00250418 0.000778296
0.097887 0.0625431
0.0373475 0.0587655
0.0458116 0.0617229
0.126127 0.0577945
3.5013e-07 0.0060296
0.0223968 0.0593102
0.0803921 0.0583976
0.0321467 0.0594015
0.0964307 0.0594267
0.0650734 0.055274
-0.0295823 0.00140614
0.0621339 0.00383425
0.0813321 0.0611169
0.0589127 0.0563088
-0.0716686 -0.00359075
0.0277564 0.0578084
-0.0194897 0.00179644
0.0920969 0.0578615
0.059435 0.0672064
0.0813268 0.0599122
0.0390895 -0.00821922
0.109549 0.0614944
0.0966595 0.0582648
0.0329332 0.0611656
0.0458117 0.0594676
-0.0362226 0.0086462
-6.0681e-07 0.00867052
-0.0734361 0.00058543
0.0283783 0.0586633
-3.66166e-06 0.00666196
-0.0267425 0.0218197
0.059435 0.0627534
0.0691072 0.0561219
0.0752321 0.0568076
0.059435 0.0627534
0.0234704 -0.00192953
0.0223968 0.0590684
-0.0234744 0.00118407
0.0662803 0.00778658
-0.00701261 0.0614319
-0.0494953 0.00765038
0.0109781 0.00454171
-0.0417491 0.000448418
-0.0611272 -0.00172145
0.0243594 0.0572527
0.0318695 0.0558744
0.0211183 0.0625187
0.0691986 0.0629268
0.101441 0.0597206
0.0589763 0.0585954
0.042448 0.060175
0.032195 0.0560558
-0.0056768 0.0565164
0.0346353 0.00738116
0.0594759 0.0574552
-0.0118764 0.0579523
0.0861488 0.0614362
0.0191272 0.0586746
0.0177216 0.00265933
-0.053257 0.00967895
0.0321468 0.0590456
-0.0417491 -0.00210987
0.0594398 0.0596376
0.0631207 0.00361063
0.0477959 0.0623504
0.0117782 -0.00294107
0.100667 0.0595149
0.0592543 0.0546252
0.0387924 0.0597202
0.0562849 0.0594997
0.0385437 0.00777626
0.0333262 0.0594592
0.0977799 0.0618237
-0.0807974 0.0109278
0.0384776 0.0623295
-0.0113473 0.00729365
0.032902 0.0636734
0.0857789 0.0606002
4.07333e-06 0.0197225
0.0188018 0.0617169
0.0497673 0.0572607
-0.0131845 -0.000671691
0.00894166 0.0175501
0.102035 0.0568996
0.0407515 0.0554928
0.059181 0.0560486
-0.0209708 0.0125114
0.0791961 0.0588228
0.0283785 0.0564412
0.00931426 0.05828
0.0281392 0.062273
0.0261391 0.0584244
0.0991788 0.0617804
0.018802 0.00358203
0.081797 0.05916
-0.0117783 0.00191049
0.0661224 0.0630956
0.0321632 0.0578713
0.0594397 0.0631584
0.0397476 0.0568868
0.0267393 0.0019048
-0.0860447 0.00665063
0.0301505 0.0615388
0.0236481 0.0585243
0.0168394 0.0558611
-0.0417624 -0.00317441
0.0527575 0.0625894
0.0871096 0.0609173
0.0223972 0.0589017
0.104819 0.0610735
0.0626054 0.0569622
0.0194912 -0.00301945
parameters: [ 9.     0.421  3.     1.2    5.744]. error: 389176948.914.
----------------------------
epoch 0, loss 1.25608
epoch 128, loss 1.12384
epoch 256, loss 0.984276
epoch 384, loss 1.19123
epoch 512, loss 1.13622
epoch 640, loss 1.21201
epoch 768, loss 1.28638
epoch 896, loss 0.9043
epoch 1024, loss 0.897435
epoch 1152, loss 1.09651
epoch 1280, loss 0.990968
epoch 1408, loss 1.0998
epoch 1536, loss 0.879659
epoch 1664, loss 1.1673
epoch 1792, loss 1.01122
epoch 1920, loss 1.05914
epoch 2048, loss 1.01292
epoch 2176, loss 1.21272
epoch 2304, loss 1.07288
epoch 2432, loss 1.08994
epoch 2560, loss 1.01687
epoch 2688, loss 1.00663
epoch 2816, loss 0.768298
epoch 2944, loss 1.15168
epoch 3072, loss 1.04474
epoch 3200, loss 1.04213
epoch 3328, loss 1.18439
epoch 3456, loss 1.36609
epoch 3584, loss 1.09121
epoch 3712, loss 1.06725
epoch 3840, loss 1.12544
epoch 3968, loss 0.92842
epoch 4096, loss 1.206
epoch 4224, loss 0.962232
epoch 4352, loss 1.08288
epoch 4480, loss 1.1082
epoch 4608, loss 0.947786
epoch 4736, loss 1.17053
epoch 4864, loss 0.946785
epoch 4992, loss 0.910267
epoch 5120, loss 0.940932
epoch 5248, loss 0.92732
epoch 5376, loss 1.00308
epoch 5504, loss 1.10076
epoch 5632, loss 1.098
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0237988 0.0248445
0.109566 0.0444941
0.0851613 0.0217979
-0.0367964 0.0146436
0.0278231 0.0297195
0.132184 0.0297828
0.0594398 0.0295143
0.0920961 0.0222923
0.0144704 0.0144986
0.000807032 0.0366398
0.0971043 0.0320566
-0.00549668 0.0327438
-0.0132848 0.0259172
0.0691987 0.019764
0.0724734 0.0508705
4.07609e-05 0.0266114
0.0300726 0.0261446
0.046218 0.013221
0.0191276 0.0408936
0.0739227 0.0381596
0.0210209 0.0424225
0.0631173 0.0243968
0.0407513 0.0303451
0.0211182 0.0399105
-0.00236373 0.0407257
0.0945153 0.0240813
0.0286789 0.0316632
0.00863029 0.0341235
-0.0211014 0.0233401
0.0691985 0.0179878
-2.95455e-07 0.02592
-0.0857226 0.0297692
-0.053257 0.0378058
0.0605996 0.0397004
0.0131848 0.024102
0.0878299 0.0265526
-0.0247793 0.0356247
0.0321468 0.027018
0.0800821 0.0186651
0.0857284 0.0296558
-2.23337e-05 0.0310703
0.0867333 0.0421068
-0.0416401 0.0435121
0.00080867 0.035818
-0.0318771 0.0251897
0.0333261 0.0359994
0.01134 0.0320028
0.0267393 0.020493
-0.0295746 0.0185951
0.0594452 0.0488117
0.0267779 0.0256879
-0.0271121 0.0268579
-0.0462169 0.0291613
-0.0413988 0.031064
0.0278231 0.02294
0.041765 0.0211816
0.0594398 0.0137164
0.0594116 0.0316247
0.102034 0.0367171
0.0461384 0.0211053
0.0290868 0.0309518
0.0267781 0.0273919
-0.0111821 0.0196252
-0.0295559 0.0232818
0.102034 0.0328875
-0.00700272 0.0341715
0.0594473 0.0351158
0.00931417 0.039362
0.0281391 0.0534023
0.00932522 0.0374011
0.00931417 0.027065
-0.0144836 0.0384811
0.0194912 0.0197121
0.118684 0.0466001
-0.0204001 0.0288668
0.0217933 0.029996
-0.0414059 0.0214151
0.0267393 0.0223544
-3.11621e-07 0.0261211
0.121243 0.0442121
0.0867333 0.0375715
0.10665 0.0370884
0.0321466 0.0254457
0.0977802 0.0398035
0.0412752 0.0295216
1.95996e-07 0.0180134
0.027745 0.0453161
0.0321586 0.0240155
0.0621339 0.0250806
0.0194912 0.0231619
0.0626002 0.0281213
-0.0716787 0.0275923
0.0882823 0.024116
-1.92327e-05 0.0241325
0.132178 0.0334382
0.0867333 0.0326582
0.0435639 0.0152191
0.0188088 0.0271062
-0.000801651 0.0300842
0.0701141 0.0398136
0.0267419 0.0179754
0.0594325 0.0295672
-0.0113495 0.0260373
0.0950804 0.040472
0.0317699 0.0136167
0.0594452 0.0412121
0.0594399 0.0256745
-0.0529722 0.0212778
-6.15975e-07 0.0284524
-0.00236343 0.0375551
-0.0611267 0.023271
-0.0716686 0.0211518
-1.92327e-05 0.0273488
0.0710837 0.0450445
3.82209e-05 0.0219735
0.0224432 0.0269556
0.0594481 0.0488092
-0.0346416 0.0132061
0.081797 0.0239183
0.104819 0.0262669
0.000807032 0.0326798
-0.0117788 0.0224516
0.0996717 0.0249033
0.0520969 0.0231423
0.0867067 0.0265818
-0.0300728 0.0246145
-0.0611177 0.0231438
0.0281391 0.0300357
parameters: [ 9.     0.421  4.618  1.2    5.744]. error: 25607725991.8.
----------------------------
epoch 0, loss 1.07202
epoch 128, loss 0.950972
epoch 256, loss 0.902302
epoch 384, loss 0.691019
epoch 512, loss 0.697653
epoch 640, loss 0.634838
epoch 768, loss 0.569804
epoch 896, loss 0.836856
epoch 1024, loss 0.672179
epoch 1152, loss 0.670632
epoch 1280, loss 0.667921
epoch 1408, loss 0.666656
epoch 1536, loss 0.700756
epoch 1664, loss 0.785787
epoch 1792, loss 0.685809
epoch 1920, loss 0.454935
epoch 2048, loss 0.577646
epoch 2176, loss 0.788512
epoch 2304, loss 0.627464
epoch 2432, loss 0.578448
epoch 2560, loss 0.56104
epoch 2688, loss 0.79273
epoch 2816, loss 0.644208
epoch 2944, loss 0.528024
epoch 3072, loss 0.689677
epoch 3200, loss 0.588962
epoch 3328, loss 0.59529
epoch 3456, loss 0.760496
epoch 3584, loss 0.575162
epoch 3712, loss 0.475586
epoch 3840, loss 0.729427
epoch 3968, loss 0.620194
epoch 4096, loss 0.591053
epoch 4224, loss 0.552397
epoch 4352, loss 0.788245
epoch 4480, loss 0.795701
epoch 4608, loss 0.57409
epoch 4736, loss 0.700341
epoch 4864, loss 0.692128
epoch 4992, loss 0.464705
epoch 5120, loss 0.614407
epoch 5248, loss 0.718689
epoch 5376, loss 0.448222
epoch 5504, loss 0.669857
epoch 5632, loss 0.606202
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.088282 0.0531563
0.0424215 0.0605432
0.020735 -0.00499083
0.0594399 0.0557826
0.0318693 0.0502392
-0.0117817 -0.00660716
0.0764272 0.0558205
0.0188086 -0.00821843
-0.0449623 -0.000278528
-0.0271278 -0.00482159
0.0859962 0.0639678
0.0412702 0.00209917
0.0626054 0.0608498
0.00378707 0.00593595
0.0477959 0.0574958
0.044975 0.059589
0.0326469 0.00616897
0.102034 0.0522807
0.0208321 0.0151853
0.0224432 0.0537221
0.134995 0.059569
0.0170834 0.0613137
0.028139 0.0550498
0.0594427 0.0540823
0.0243585 0.0521619
0.0791961 0.0513846
0.0920969 0.0528526
0.0594397 0.0592709
0.00932421 0.0485847
0.0901949 0.0520224
0.0390986 -0.00615478
0.038792 0.0542438
0.0482639 0.0599805
0.0859465 0.0611195
6.7959e-07 0.0131732
0.034034 -0.00353769
0.0329021 0.0599984
-0.0117783 0.00352288
0.0910567 0.0558391
0.0594397 0.0585428
0.0278229 0.0548727
0.0188023 0.00294808
-0.000801651 -0.00248831
0.0144859 0.00885063
-0.0611177 0.00478793
0.0208321 -0.00204192
0.121243 0.060765
-0.049248 -0.00540548
0.0730675 0.055059
0.0267393 0.00948897
-0.0631103 -0.00686125
0.0497674 0.0512462
-0.0449599 -0.00346988
0.0945038 0.0517083
0.034034 0.00534639
-0.0144836 -0.000520191
0.0868851 0.0570973
0.0305923 0.0565848
0.0857789 0.0539111
0.0950805 0.0588164
0.0673482 0.0484807
0.0152545 0.00492065
0.0396776 0.0536303
0.0659615 0.00180681
-0.00700272 0.0555611
0.10664 0.0535505
0.100097 0.0643826
0.0271289 -0.00161398
0.0861487 0.0578366
-0.0412757 0.00916784
0.0509553 0.0557163
0.0330542 0.0486531
0.0606162 0.0468787
0.0724734 0.0506368
-0.0271126 -0.00618875
-0.0144713 0.0108368
0.0364958 0.0474497
-0.0417491 -0.0045253
0.0317699 0.0548998
0.0362233 0.0152667
0.0594399 0.0540424
-0.0106908 -0.00815688
0.0134225 0.0713089
0.129445 -0.00308015
-3.91322e-05 0.00654172
-0.0144843 -0.00131476
-0.0209708 -0.00240937
0.0234638 -0.00381623
0.0871098 0.0529212
0.0860587 0.00928389
0.0462156 -0.00262302
0.129453 0.00881986
-0.0417422 0.00462987
0.0878299 0.0580439
-0.129453 0.000688796
0.0626054 0.0607983
0.0594397 0.0625229
-0.0326435 -0.00525347
-0.0056768 0.0506818
0.0594379 0.072541
0.0495796 0.000446555
0.0562648 0.054129
-0.0111821 0.0442151
0.046209 0.00934356
-0.013282 0.0723258
-0.0271153 0.0115526
0.0871097 0.0564527
-0.00728242 0.0454563
-0.0734426 0.00982748
-0.0621343 -0.00411608
0.0318766 0.00738645
0.0861486 0.0595651
-0.0532643 -0.00148352
-0.0417573 0.012168
0.0144846 -0.000666983
0.00891968 -0.00518012
0.0594116 0.0495309
0.0673482 0.0479449
0.0385427 0.0130868
-0.00894944 0.00560022
-0.0117783 -0.00158
0.0168394 0.05139
-0.00737805 0.0443051
0.0764481 0.056655
0.0710835 0.0563131
0.0594398 0.0552103
0.0859466 0.0574917
0.0268143 0.0585512
parameters: [ 9.     0.421  3.     1.2    5.744]. error: 523139996.018.
----------------------------
epoch 0, loss 1.25371
epoch 128, loss 1.00453
epoch 256, loss 1.34754
epoch 384, loss 1.17647
epoch 512, loss 1.14466
epoch 640, loss 0.922627
epoch 768, loss 0.903854
epoch 896, loss 0.825076
epoch 1024, loss 0.975117
epoch 1152, loss 0.988603
epoch 1280, loss 0.836912
epoch 1408, loss 1.03246
epoch 1536, loss 0.773112
epoch 1664, loss 0.979489
epoch 1792, loss 0.845791
epoch 1920, loss 0.74965
epoch 2048, loss 0.576948
epoch 2176, loss 0.765481
epoch 2304, loss 0.880735
epoch 2432, loss 0.809158
epoch 2560, loss 1.15321
epoch 2688, loss 0.771876
epoch 2816, loss 0.699841
epoch 2944, loss 0.879192
epoch 3072, loss 0.821131
epoch 3200, loss 0.818403
epoch 3328, loss 0.983614
epoch 3456, loss 0.702778
epoch 3584, loss 0.907293
epoch 3712, loss 0.705316
epoch 3840, loss 0.674727
epoch 3968, loss 0.733942
epoch 4096, loss 0.717163
epoch 4224, loss 0.660347
epoch 4352, loss 0.55744
epoch 4480, loss 0.682192
epoch 4608, loss 0.587415
epoch 4736, loss 0.724059
epoch 4864, loss 0.706473
epoch 4992, loss 0.584458
epoch 5120, loss 0.642473
epoch 5248, loss 0.680978
epoch 5376, loss 0.647251
epoch 5504, loss 0.673006
epoch 5632, loss 0.794395
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.00932421 0.0538488
0.0189868 0.00252364
0.0417484 -0.00818715
0.0859865 0.063307
0.0496681 0.0557453
0.0192074 0.0616674
0.0467944 0.0171323
0.130757 0.0688494
0.00932421 0.049795
0.0964305 0.0456494
0.0594399 0.0612424
-0.0109566 -0.00176049
0.0920969 0.0535299
0.0321467 0.0502563
0.0594398 0.0546287
-0.0394792 0.0205286
0.0317696 0.0579132
0.0188603 0.00424098
0.0220539 0.0484022
0.0364958 0.0452829
0.0197009 0.0514025
-3.17974e-05 0.0113851
0.0318693 0.0507942
0.0283785 0.0510709
0.0562648 0.0536502
0.0562597 0.0576365
0.0594396 0.053894
0.0449639 0.014308
9.99101e-07 0.00110421
0.0305919 0.0503308
0.0435639 0.0495448
-0.0532643 0.0180387
0.0327312 0.0592094
-0.0113473 0.0164695
0.0594396 0.050942
-0.00378521 0.000450251
0.0189868 -6.31093e-05
0.0222151 0.0527577
1.13647e-06 0.0209721
0.0865811 0.0501369
0.0897923 0.0502471
-0.0248069 0.0181285
0.0734394 0.00409898
-0.000786289 0.0121391
0.0362234 0.0159062
0.0897923 0.0563288
0.0865811 0.0491988
0.0752955 0.0499025
0.0144846 0.0114488
0.0867067 0.0466507
0.0281393 0.0557071
-0.041769 0.00351711
0.0870049 0.0506178
0.0662801 -0.00874402
0.0594162 0.0475304
0.0271109 0.00956468
0.10664 0.05448
0.00891968 0.00390359
-0.0611177 -0.00369844
0.0952316 0.055168
0.0594919 0.0502684
0.0859466 0.049591
0.0861486 0.0569061
-0.0152533 0.00903259
0.0621308 0.00519952
0.0272894 -0.00417258
0.0317699 0.0487279
0.0199307 0.0562885
0.0248097 0.0174655
0.0267779 0.05179
-0.0385434 0.011556
-0.0385437 -0.00409451
0.0594379 0.0658466
0.0496681 0.0549425
0.130756 0.0605694
0.0318695 0.0497219
0.0582236 0.0485746
0.0329332 0.0483546
0.0860364 0.00389628
-0.0234626 0.0137131
8.40829e-06 0.00488514
0.125883 0.0584967
0.0468012 0.01628
0.0106903 -0.00442346
0.097887 0.0577878
0.0706155 0.0568705
0.0223972 0.0473011
-0.0716682 0.00954081
0.0594399 0.0566482
0.0144853 -0.00483647
0.0589764 0.0573655
-0.0390966 -0.00849542
-0.0416423 0.00929436
0.0496682 0.0513511
0.0626054 0.057681
-0.0860447 0.00553315
-0.0110929 0.044785
0.0301504 0.0510022
0.0417489 -0.00135607
0.0133878 0.0660075
0.0177216 0.000328866
0.081527 0.0465947
0.100115 -0.0012527
0.0261395 0.0452101
0.0752321 0.0514197
-0.0188028 -0.00588673
0.0492512 -0.00174412
0.0365698 0.00658292
0.0868849 0.0655348
0.0783558 0.0510387
0.0329331 0.0537125
-0.034636 0.00102037
0.0439209 0.0559642
0.038792 0.0471511
2.96104e-08 0.000216429
-0.0346416 0.00192138
0.0781227 0.053455
0.0813321 0.0527125
0.0594376 0.0521012
0.0188088 0.000382771
0.0937752 0.0558782
0.0594396 0.0559454
-0.129449 0.00555324
0.05627 0.0562712
-0.129449 0.00150873
0.0329331 0.0556807
0.126171 0.0551769
0.094504 0.0483356
parameters: [ 9.     0.421  3.618  1.2    5.744]. error: 462601413.732.
----------------------------
epoch 0, loss 1.25888
epoch 128, loss 1.13957
epoch 256, loss 0.952878
epoch 384, loss 1.03738
epoch 512, loss 0.838407
epoch 640, loss 1.08423
epoch 768, loss 0.80745
epoch 896, loss 0.7785
epoch 1024, loss 1.19006
epoch 1152, loss 1.10835
epoch 1280, loss 1.16338
epoch 1408, loss 0.779247
epoch 1536, loss 0.911426
epoch 1664, loss 0.976022
epoch 1792, loss 0.843914
epoch 1920, loss 0.873653
epoch 2048, loss 0.852646
epoch 2176, loss 0.849093
epoch 2304, loss 0.749327
epoch 2432, loss 0.700489
epoch 2560, loss 0.889592
epoch 2688, loss 0.641942
epoch 2816, loss 0.760404
epoch 2944, loss 0.796325
epoch 3072, loss 0.712784
epoch 3200, loss 0.930501
epoch 3328, loss 0.731367
epoch 3456, loss 0.765623
epoch 3584, loss 0.697074
epoch 3712, loss 0.786786
epoch 3840, loss 0.833979
epoch 3968, loss 0.728249
epoch 4096, loss 0.736993
epoch 4224, loss 0.732925
epoch 4352, loss 0.697313
epoch 4480, loss 0.803541
epoch 4608, loss 0.7167
epoch 4736, loss 0.663756
epoch 4864, loss 0.684883
epoch 4992, loss 0.757603
epoch 5120, loss 0.707577
epoch 5248, loss 0.625115
epoch 5376, loss 0.872298
epoch 5504, loss 0.684268
epoch 5632, loss 0.829261
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0281393 0.0523128
0.0133878 0.0656805
0.106629 0.0494685
0.0317696 0.0568021
0.0375381 0.0458035
-3.6833e-05 0.0254456
0.0594162 0.0401675
0.0859962 0.0536033
0.057604 0.0691074
0.0813321 0.0432134
0.0417583 0.00315386
0.0327315 0.0535945
-0.0234626 0.0144398
0.0867202 0.0499759
-0.0340213 0.0170247
0.0508582 0.0535662
0.0390889 0.0186265
0.0867201 0.0529496
0.0675205 0.0499446
-0.00700272 0.0568159
0.0317698 0.0580462
9.99101e-07 0.00318295
0.0594398 0.0473137
0.0937753 0.0584541
0.0384776 0.0559775
0.0861486 0.0575258
0.0396778 0.0423591
-0.027288 0.0159801
0.00596587 0.054319
0.0730675 0.0517165
-0.0207316 0.0023319
-0.0177216 0.0124971
0.121243 0.0527141
0.0267393 -0.00139892
0.0477956 0.0464866
0.0318693 0.0482041
0.106629 0.0470693
-0.00733491 0.0446707
-0.0111692 0.0459879
0.0966595 0.0521559
-0.0109842 0.0319445
-0.0412731 0.0194214
0.0495796 -0.00468084
0.0117845 0.0127259
-0.0188023 0.0104815
0.0996717 0.0427717
0.0140549 0.0449709
0.0968255 0.0508215
-6.66928e-06 0.00723145
-0.0529659 0.00236775
0.0267443 0.0140017
0.0739227 0.0550947
0.0857183 0.0190672
0.0385437 0.00682007
0.102035 0.0481354
0.0117782 0.00585833
0.0300726 0.0033139
0.0691985 0.0466815
0.000796249 0.0132772
0.0562849 0.0534056
-0.0110929 0.049379
-0.00893962 0.0166568
0.0710838 0.0437985
0.059435 0.0675764
-0.0385433 0.00645929
0.0950805 0.0524544
0.0661225 0.0549474
0.0764272 0.0598304
0.0594397 0.0558807
5.31452e-07 0.00774606
0.0109517 0.0159486
0.0412776 0.00305006
0.0323166 0.000497469
0.0468064 0.0261619
0.0675205 0.0543288
0.0815268 0.0471798
0.0327313 0.0489893
0.0317699 0.0619177
2.41162e-06 0.000906859
0.0152616 0.0160704
0.080402 0.0483719
0.0416484 0.0223715
-0.00378521 0.012165
0.0301505 0.0530532
0.0037846 0.000513908
0.132178 0.0695055
0.0368964 0.0425238
0.00932522 0.0442119
-0.0300731 0.00757762
0.0412776 0.00883782
0.0272988 0.00737156
-0.0611267 -0.00342562
0.088282 0.0446017
4.17498e-06 0.016681
-0.00236343 0.052761
0.0594397 0.0547338
0.0907409 0.0466631
0.0691986 0.0429104
-0.02083 0.0203537
-0.00735889 0.0496966
0.0964766 0.047937
0.0271115 0.00754326
0.0594398 0.0552707
0.0594397 0.0515374
0.0268195 0.0452308
0.0424369 0.0555284
0.0122243 0.0480455
-0.10012 0.00195188
0.0527575 0.0554162
-0.129457 0.022348
0.100097 0.0599908
-0.0207292 0.00494409
0.0966595 0.050309
-0.034636 0.00951887
0.025123 0.0585405
0.043628 0.055999
0.029557 0.0109737
0.00863092 0.0464731
0.0594395 0.0413032
0.106634 0.0575152
0.0458115 0.052849
-0.0117783 0.00928375
0.0594427 0.0528127
0.05937 0.048182
0.0605996 0.044543
0.0971047 0.0694617
-6.15975e-07 0.0180409
0.129445 -0.0014006
parameters: [ 9.     0.421  4.     1.2    5.744]. error: 256473457.353.
----------------------------
epoch 0, loss 1.35319
epoch 128, loss 1.43474
epoch 256, loss 1.2532
epoch 384, loss 1.22421
epoch 512, loss 1.17838
epoch 640, loss 1.12016
epoch 768, loss 1.2103
epoch 896, loss 1.11354
epoch 1024, loss 1.33406
epoch 1152, loss 1.15345
epoch 1280, loss 0.954294
epoch 1408, loss 1.08075
epoch 1536, loss 1.0966
epoch 1664, loss 1.24568
epoch 1792, loss 0.935859
epoch 1920, loss 0.783659
epoch 2048, loss 1.10985
epoch 2176, loss 0.870836
epoch 2304, loss 0.870933
epoch 2432, loss 0.979538
epoch 2560, loss 1.08402
epoch 2688, loss 1.10923
epoch 2816, loss 1.15414
epoch 2944, loss 0.86135
epoch 3072, loss 0.954948
epoch 3200, loss 1.04795
epoch 3328, loss 0.95436
epoch 3456, loss 0.956537
epoch 3584, loss 1.0405
epoch 3712, loss 0.901567
epoch 3840, loss 1.05251
epoch 3968, loss 0.929739
epoch 4096, loss 0.99657
epoch 4224, loss 1.24054
epoch 4352, loss 1.03148
epoch 4480, loss 1.06263
epoch 4608, loss 0.800016
epoch 4736, loss 0.984892
epoch 4864, loss 1.05006
epoch 4992, loss 1.02298
epoch 5120, loss 1.02569
epoch 5248, loss 0.956487
epoch 5376, loss 0.945334
epoch 5504, loss 0.984893
epoch 5632, loss 0.747637
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.00894922 0.0188515
0.0208321 0.0349013
0.0813269 0.0536947
0.0248097 0.0250985
-0.0529682 0.0205705
0.121243 0.0411876
0.0317699 0.0562744
0.088282 0.0337978
0.0210209 0.0368383
-0.0413988 0.0225191
0.060558 0.0373589
0.0594466 0.0307756
0.0904958 0.036435
0.0827282 0.0280293
0.0860587 0.02869
0.0589765 0.0393529
0.0589765 0.0527878
0.0691072 0.0352231
0.0927397 0.0376605
0.0236479 0.0598508
0.0368964 0.0443725
0.0217931 0.0468524
0.0189892 0.030176
0.0224432 0.0422816
-0.0394792 0.0266233
0.0237989 0.0427947
-0.0414033 0.0395063
-0.0109774 0.0308291
0.0321586 0.0425899
0.0927396 0.0517503
0.0532557 0.0300548
0.0594251 0.0328497
0.0897923 0.0531529
0.081797 0.0417971
-0.0662843 0.0302225
0.0222152 0.0413036
-0.0346416 0.0330473
0.0964307 0.057531
0.0261393 0.0479106
0.0207193 0.0470337
0.0594397 0.05307
0.0643397 0.0316203
-0.000801651 0.0325786
0.0897923 0.0531529
0.132184 0.0414364
0.0424369 0.0455824
0.0327315 0.0519397
0.109566 0.044231
0.0562851 0.0475121
0.0482638 0.0496842
0.0950805 0.0597328
-0.129449 0.0220211
0.0594452 0.0342082
0.0319943 0.0365093
0.0971043 0.0376606
0.0901949 0.0441597
0.0859466 0.0560455
0.0710835 0.0364027
0.0329332 0.0495534
0.0435639 0.0320908
-0.0707062 0.0309267
-0.0394796 0.0152979
-0.0248069 0.0336532
0.09274 0.0322203
0.062595 0.0474562
-0.0414014 0.0237402
-0.0023637 0.0366904
-0.0189864 0.0282915
-0.0144713 0.0399276
0.0496681 0.0491241
0.0189868 0.0334518
0.124354 0.0426585
-0.000835377 0.0337447
0.0968257 0.0369037
0.0317699 0.0465083
0.0594398 0.0358564
0.134995 0.0417142
0.0321632 0.0336128
0.0407513 0.0378196
0.0867334 0.0364266
0.01134 0.0466975
0.0261391 0.0357586
0.0594397 0.0474594
-0.00378297 0.0267956
0.0691986 0.051043
0.134985 0.0369091
0.109566 0.0375961
0.0594251 0.0352659
-0.0417792 0.0293703
0.0764272 0.0353085
-0.0144856 0.0338612
0.0329332 0.0611574
0.0210209 0.0456649
-0.0631232 0.0254797
0.0268143 0.0444994
0.0691072 0.0433413
0.0594251 0.0363609
0.0739226 0.0407404
0.0412702 0.0452306
0.0152616 0.0377524
-0.00700272 0.0262459
-0.0131848 0.0392547
0.0937752 0.0498149
0.0370769 0.0343885
0.0362237 0.0325727
-0.00142113 0.0395707
0.0290868 0.0398767
0.0464004 0.0396296
0.0945151 0.0366634
0.0199307 0.0376393
0.0968257 0.0371111
-0.0271193 0.0236216
0.0268195 0.0274298
0.0134334 0.0318478
0.0390991 0.0291082
0.0937752 0.0467554
0.0866799 0.0347376
0.027745 0.0382716
0.129925 0.033205
0.0667826 0.0471917
0.0776855 0.0231202
-0.0188023 0.0298949
0.0606162 0.0418962
0.0594396 0.0488512
0.0370775 0.0443434
0.000834782 0.0257448
0.0532578 0.0208527
0.0594397 0.0368706
parameters: [ 9.     0.421  4.236  1.2    5.744]. error: 53.1221710778.
----------------------------
epoch 0, loss 0.881326
epoch 128, loss 0.966036
epoch 256, loss 1.07417
epoch 384, loss 1.05629
epoch 512, loss 1.22113
epoch 640, loss 0.971257
epoch 768, loss 1.02105
epoch 896, loss 1.14876
epoch 1024, loss 0.97807
epoch 1152, loss 1.13279
epoch 1280, loss 0.761363
epoch 1408, loss 0.914481
epoch 1536, loss 1.00017
epoch 1664, loss 0.820032
epoch 1792, loss 1.18434
epoch 1920, loss 0.885632
epoch 2048, loss 1.05435
epoch 2176, loss 0.877086
epoch 2304, loss 0.982708
epoch 2432, loss 0.814302
epoch 2560, loss 1.1214
epoch 2688, loss 0.958163
epoch 2816, loss 1.14684
epoch 2944, loss 0.963843
epoch 3072, loss 0.989926
epoch 3200, loss 0.954843
epoch 3328, loss 1.03696
epoch 3456, loss 0.924701
epoch 3584, loss 1.12707
epoch 3712, loss 1.12632
epoch 3840, loss 1.16829
epoch 3968, loss 0.948304
epoch 4096, loss 0.764713
epoch 4224, loss 0.724011
epoch 4352, loss 0.911107
epoch 4480, loss 0.885771
epoch 4608, loss 0.848765
epoch 4736, loss 1.10605
epoch 4864, loss 0.840948
epoch 4992, loss 1.08
epoch 5120, loss 0.956186
epoch 5248, loss 0.964044
epoch 5376, loss 0.862567
epoch 5504, loss 1.02377
epoch 5632, loss 1.13007
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0394799 0.0299146
0.0520969 0.0340211
-0.0414059 0.0305574
0.088729 0.0321345
0.0964772 0.0326077
0.0140549 0.0332987
0.0952316 0.0345538
0.0236482 0.0346118
0.0317698 0.0342607
0.0594398 0.0327776
0.10665 0.0319561
0.0855722 0.0375804
-0.0247813 0.033828
-0.0188605 0.0316835
-0.0394796 0.0298332
0.0871097 0.0344241
0.0197009 0.0345103
-0.0188606 0.0334552
0.0800819 0.0333206
0.0237988 0.0331153
0.0594482 0.0311823
0.0305919 0.0326988
0.0776735 0.0324101
-0.0118765 0.0370077
-0.036223 0.0365791
0.121243 0.0310965
0.0322982 0.0322412
0.0734366 0.0326174
0.0594396 0.034089
0.101441 0.0322168
0.0861486 0.0351957
0.0317696 0.035363
0.0390991 0.033662
0.0594396 0.0347516
0.0247829 0.0341896
0.022215 0.0309428
-0.0204028 0.033649
-0.0234699 0.0308377
-0.0414014 0.0318044
0.0730675 0.0394063
-0.0417422 0.0295523
-0.00700262 0.0347268
0.0594397 0.034015
0.132184 0.0375588
0.0319945 0.0329848
0.0752955 0.0360111
0.0272894 0.0313132
0.0532557 0.032701
0.0508582 0.0308913
0.0436381 0.0346963
-0.0113473 0.0318548
0.0920969 0.0334092
0.101441 0.030318
0.0497674 0.037117
0.0594396 0.0338018
0.0267781 0.0358262
0.022215 0.0336311
-3.6833e-05 0.0315882
-0.00250418 0.031143
-0.0131851 0.0296469
0.0461385 0.0352928
-0.0113401 0.0313435
0.0945038 0.0311475
0.0210212 0.034947
0.0261394 0.0333911
0.0424529 0.0349512
0.0271109 0.0315272
0.0594297 0.0327851
0.0144707 0.0298518
-0.0707068 0.0339505
-0.0106908 0.0336819
-0.0271121 0.0334444
-7.99663e-06 0.031548
0.0251231 0.0343544
-0.0113473 0.0289854
0.0247829 0.0330379
0.0321467 0.031319
0.0261393 0.0379007
0.062595 0.0327216
0.125146 0.0379868
0.0477957 0.0297057
-0.0117783 0.031862
0.0177216 0.0317703
0.0701141 0.0376935
0.028139 0.0311783
0.0582651 0.0376929
0.0594349 0.0322147
0.100667 0.0335794
0.0861486 0.0331534
-0.00236343 0.031073
0.032933 0.0304783
0.0529686 0.0302524
0.0243594 0.0338303
0.081797 0.0357048
0.05937 0.0376933
-0.00968441 0.0366611
0.0117785 0.0296914
0.0764272 0.034298
0.0791268 0.0340846
0.090741 0.0308281
0.109566 0.0323242
0.074953 0.0308321
0.100097 0.0334449
0.0300726 0.0303492
0.0707066 0.0293125
0.0594398 0.0358067
0.0122142 0.0318049
0.0462156 0.0293251
0.0271187 0.0305247
-0.0462049 0.0311606
0.00596546 0.0380955
0.0813269 0.0343543
0.0589133 0.0346505
-0.041635 0.0302219
0.0989678 0.0376821
0.0594449 0.0327872
0.0387924 0.0329981
0.0594481 0.0322829
0.0730674 0.0370211
0.125146 0.0358601
0.0920613 0.0337697
0.0861488 0.0345433
0.0494968 0.0295759
-0.0109842 0.0337266
0.0330948 0.0305733
0.0776735 0.0327239
0.0211183 0.0385786
0.0739226 0.035234
parameters: [ 9.     0.421  4.382  1.2    5.744]. error: 1.33072966841.
----------------------------
epoch 0, loss 1.55889
epoch 128, loss 1.43436
epoch 256, loss 1.5321
epoch 384, loss 1.29774
epoch 512, loss 1.23097
epoch 640, loss 1.2435
epoch 768, loss 1.12757
epoch 896, loss 1.24077
epoch 1024, loss 1.05482
epoch 1152, loss 1.28781
epoch 1280, loss 1.17408
epoch 1408, loss 1.36343
epoch 1536, loss 1.21369
epoch 1664, loss 1.1489
epoch 1792, loss 1.37121
epoch 1920, loss 1.32979
epoch 2048, loss 1.15741
epoch 2176, loss 1.39835
epoch 2304, loss 1.21475
epoch 2432, loss 0.937018
epoch 2560, loss 1.36226
epoch 2688, loss 0.910361
epoch 2816, loss 0.955028
epoch 2944, loss 0.898962
epoch 3072, loss 1.26861
epoch 3200, loss 1.10448
epoch 3328, loss 1.10328
epoch 3456, loss 1.18735
epoch 3584, loss 0.834841
epoch 3712, loss 1.00378
epoch 3840, loss 1.0433
epoch 3968, loss 0.957474
epoch 4096, loss 0.941704
epoch 4224, loss 1.33448
epoch 4352, loss 1.0326
epoch 4480, loss 0.888271
epoch 4608, loss 0.990427
epoch 4736, loss 0.730022
epoch 4864, loss 1.00322
epoch 4992, loss 0.904661
epoch 5120, loss 0.959709
epoch 5248, loss 0.969446
epoch 5376, loss 1.08274
epoch 5504, loss 0.952196
epoch 5632, loss 0.975329
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0716782 0.0336952
-0.0716682 0.00192699
-0.0144856 0.0144626
0.0268143 0.038158
2.05834e-07 0.0205428
3.62028e-05 0.0317278
-0.0118765 0.0190495
0.00863092 0.0208466
0.0594399 0.0301541
0.000834782 0.00839508
-0.0300731 0.0164034
-3.91322e-05 0.0160789
0.0989675 0.0421561
-0.0271121 0.00660211
0.05627 0.0421957
-0.0611171 0.00862359
0.0920508 0.0261634
0.0813268 0.0282458
-0.0188606 0.0133233
0.0594396 0.0241139
-0.00894125 0.00860878
0.0117785 0.00688483
0.00894003 0.00913279
0.0134334 0.038242
0.0271185 0.0198418
0.061289 0.0304129
-0.0144713 0.0164432
0.0624794 0.0342567
0.0851609 0.0404835
0.0532535 0.0383999
0.0730676 0.0291675
0.126222 0.0211001
-0.0394792 0.0302648
0.0387924 0.0402738
-0.0494977 0.0338389
0.0210981 0.0231517
0.0977799 0.0389241
0.118684 0.0430197
-0.053257 0.0225546
0.0318693 0.0427657
0.0261392 0.032228
0.0626054 0.0421927
-0.0295746 0.0229641
0.0897923 0.0445146
0.0318695 0.0317825
0.0267393 0.0139187
-0.0621343 0.0393293
-0.0394796 0.0138173
0.0861488 0.0480026
0.0131855 0.0311474
0.0868851 0.0397463
0.0593312 0.0452367
-0.0860382 0.0109468
0.00459996 0.043298
0.000793148 0.0281355
0.0417583 0.0114787
0.0109586 0.0233249
0.0330948 0.0261089
0.0281392 0.0239545
0.0594399 0.044421
0.0752955 0.0271727
0.00931417 0.0165032
0.0594398 0.034159
0.0234638 0.0140964
-0.0194973 0.0165425
-0.0532643 0.00994438
0.0174339 0.0285171
0.022215 0.0226706
0.0122142 0.0380419
0.0781223 0.0220803
0.049499 0.0243303
0.0267781 0.0373916
0.0867199 0.0238639
0.102035 0.037369
0.043921 0.0347395
0.129453 0.00737741
0.0920961 0.0202459
0.0701141 0.0247006
0.0661224 0.0204486
0.0707072 0.0370839
0.135027 0.044714
0.0467944 0.0236634
-0.129445 0.040136
-0.00891927 0.00567908
0.0509553 0.0163357
0.10665 0.029311
0.0261392 0.0207128
0.0286795 0.0178128
-1.92327e-05 0.00750644
0.0222151 0.0393812
0.109566 0.0179241
0.0144707 0.0100598
0.0594301 0.0429358
0.09274 0.0170979
0.0870049 0.0331343
0.0281392 0.0245776
-0.0346352 0.0201572
0.000793148 0.0311304
0.0209836 0.00363669
0.0594397 0.0355532
0.0477958 0.0282644
0.126086 0.0168949
-3.6833e-05 0.032885
-0.0152533 0.0267174
-0.0131858 0.0413738
-0.0188023 0.00920176
-0.00726943 0.0301268
0.0267393 0.0240994
0.000793148 0.019169
-0.0267425 0.0238821
-2.15513e-06 0.00949334
0.0764118 0.0190128
0.0594398 0.0303435
0.0210958 0.0431001
0.0495768 0.0186248
0.0716711 0.0257142
0.110244 0.021381
0.0621308 0.014121
0.135027 0.0222757
-0.00700272 0.037953
0.0594396 0.0505899
0.0815268 0.0224892
0.0594399 0.0301136
0.0813269 0.0409317
0.0594162 0.0290571
-0.0807986 0.030766
-0.0385437 0.0223597
0.0416434 0.0170337
parameters: [ 9.     0.421  4.309  1.2    5.744]. error: 31467252157.1.
----------------------------
epoch 0, loss 1.37389
epoch 128, loss 1.401
epoch 256, loss 1.10425
epoch 384, loss 0.966218
epoch 512, loss 1.06205
epoch 640, loss 1.13102
epoch 768, loss 0.992598
epoch 896, loss 0.787685
epoch 1024, loss 1.07795
epoch 1152, loss 1.00322
epoch 1280, loss 1.14378
epoch 1408, loss 1.09621
epoch 1536, loss 1.10345
epoch 1664, loss 0.947178
epoch 1792, loss 1.12377
epoch 1920, loss 1.31687
epoch 2048, loss 0.943973
epoch 2176, loss 1.11957
epoch 2304, loss 1.1229
epoch 2432, loss 1.09637
epoch 2560, loss 0.947383
epoch 2688, loss 0.848417
epoch 2816, loss 0.952547
epoch 2944, loss 1.11793
epoch 3072, loss 0.98873
epoch 3200, loss 0.944491
epoch 3328, loss 1.1417
epoch 3456, loss 0.910749
epoch 3584, loss 1.02401
epoch 3712, loss 1.16214
epoch 3840, loss 0.953195
epoch 3968, loss 0.943053
epoch 4096, loss 1.05663
epoch 4224, loss 0.803021
epoch 4352, loss 0.919686
epoch 4480, loss 0.903118
epoch 4608, loss 1.16712
epoch 4736, loss 0.87938
epoch 4864, loss 0.882556
epoch 4992, loss 1.01044
epoch 5120, loss 0.952605
epoch 5248, loss 0.867714
epoch 5376, loss 0.99814
epoch 5504, loss 1.05112
epoch 5632, loss 0.999192
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.12945 0.0399371
0.0594297 0.0385882
0.126171 0.0335708
0.0857795 0.0377892
0.0997468 0.0326918
0.0871098 0.0417944
-0.0110929 0.0507138
-0.0621274 0.0256942
0.00131949 0.0214817
0.0739226 0.0517016
-1.99995e-06 0.0207433
0.0144704 0.0305247
0.042448 0.057397
0.0417484 0.0383051
0.0860364 0.0365517
0.0538015 0.0409
0.0122352 0.0353164
0.0950803 0.051825
-0.0412685 0.0249214
0.0593665 0.0489139
-0.000807624 0.0305108
0.0650733 0.0422512
0.0317697 0.0432719
0.0277564 0.038113
-0.0189864 0.0250469
-0.0204122 0.0309055
0.121243 0.034757
0.0496682 0.0442505
-0.0209824 0.0580287
-0.0132848 0.0504519
-0.0117817 0.0281123
-0.0248182 0.0215383
0.0237986 0.0451069
0.0321467 0.0468911
-0.0860447 0.0196026
0.0589763 0.039766
0.0527576 0.0440218
0.0857789 0.0363233
0.0911183 0.0244594
0.0977799 0.0358995
0.0286789 0.0476152
0.0144859 0.0269547
-0.0117817 0.0277004
1.01848e-06 0.027528
0.130757 0.0369985
0.0594395 0.0298125
-3.76332e-06 0.0246435
0.0261392 0.0341124
0.00856342 0.0288635
-0.0412685 0.0376966
0.0594397 0.0527081
0.0529729 0.0461443
-0.000801651 0.0301417
0.0144707 0.0280648
0.093775 0.0549002
0.0920961 0.0378444
0.0482639 0.0561606
0.0286795 0.0312086
-0.0462169 0.0399692
-0.0234744 0.0425495
-0.0131858 0.0355001
0.105471 0.0472265
-0.0416423 0.03072
0.0920508 0.0332018
0.0582589 0.0465171
0.125873 0.0648448
0.0496683 0.0420537
0.0968257 0.0526393
0.00894003 0.0262072
-0.129457 0.0419329
0.0305919 0.0509972
0.0217933 0.0384164
0.027823 0.055436
0.0394802 0.038759
0.0117785 0.0361284
0.0271115 0.0176825
-0.000793636 0.0417041
0.0813269 0.0530017
0.0508582 0.0534809
0.0290868 0.0621847
0.0251233 0.055701
0.0464005 0.037057
-0.0210896 0.0317944
0.0599029 0.0463487
0.0866799 0.0409493
0.0594162 0.0373476
0.0109517 0.0376247
0.0859962 0.057478
0.0394802 0.0482907
0.0414051 0.0373559
0.0477959 0.0623613
-0.041769 0.0287535
0.0273016 0.0522823
0.0582389 0.0625351
0.0910567 0.032756
0.0330948 0.0359517
6.39156e-06 0.034803
0.0321468 0.0348396
-0.02083 0.0292316
0.0868848 0.0383954
0.0199307 0.0418929
0.0168394 0.0474876
0.021525 0.0307976
-0.0495799 0.0323003
0.0599029 0.0478175
0.0188023 0.0314222
0.059181 0.0260496
-0.00236343 0.0560003
3.62028e-05 0.0290522
0.014471 0.0258875
-2.15513e-06 0.0299456
0.0327313 0.0334905
0.032158 0.0418913
0.0897923 0.0432796
0.101441 0.0459754
0.0424316 0.043867
0.134995 0.055896
0.0594398 0.0396293
0.0477957 0.0533826
0.0594397 0.0386064
0.079197 0.0327641
0.0927397 0.069302
0.0144846 0.043325
0.0329331 0.0455784
0.0210958 0.0482267
-0.0152556 0.0282353
0.0234638 0.0445484
0.0319944 0.0533972
parameters: [ 9.     0.421  4.472  1.2    5.744]. error: 1201044279.81.
----------------------------
epoch 0, loss 1.20787
epoch 128, loss 1.02687
epoch 256, loss 1.02121
epoch 384, loss 1.16206
epoch 512, loss 0.820584
epoch 640, loss 1.028
epoch 768, loss 1.03304
epoch 896, loss 1.06269
epoch 1024, loss 0.898475
epoch 1152, loss 0.896411
epoch 1280, loss 1.12528
epoch 1408, loss 1.03233
epoch 1536, loss 0.914163
epoch 1664, loss 1.01298
epoch 1792, loss 0.85971
epoch 1920, loss 0.984181
epoch 2048, loss 1.1414
epoch 2176, loss 0.902195
epoch 2304, loss 0.87354
epoch 2432, loss 0.929644
epoch 2560, loss 0.888238
epoch 2688, loss 1.06509
epoch 2816, loss 1.05023
epoch 2944, loss 0.872388
epoch 3072, loss 1.15637
epoch 3200, loss 0.935095
epoch 3328, loss 0.785558
epoch 3456, loss 1.04518
epoch 3584, loss 0.86768
epoch 3712, loss 1.00393
epoch 3840, loss 1.14817
epoch 3968, loss 0.834517
epoch 4096, loss 0.874509
epoch 4224, loss 0.795
epoch 4352, loss 1.04516
epoch 4480, loss 0.87404
epoch 4608, loss 0.752987
epoch 4736, loss 0.899516
epoch 4864, loss 0.995768
epoch 4992, loss 0.843187
epoch 5120, loss 0.923762
epoch 5248, loss 0.856352
epoch 5376, loss 1.14634
epoch 5504, loss 0.873432
epoch 5632, loss 0.87706
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0327314 0.0377975
-0.032317 0.0232725
0.0920961 0.038378
0.081797 0.0433499
0.0592406 0.0361072
-0.00236343 0.0318126
0.0416434 0.0337277
0.0117782 0.0260491
-0.0207316 0.0210897
0.0464005 0.0346074
0.0329332 0.0325542
0.0477957 0.02891
0.061289 0.0408448
3.5013e-07 0.0342423
0.0337182 0.037501
0.093775 0.0320677
0.0870053 0.034569
0.0191272 0.0228514
0.022444 0.0338879
0.0237988 0.0529753
-0.0495799 0.0207024
0.0261394 0.0440915
-0.0194973 0.0230221
-0.0532643 0.0261816
0.0727411 0.0548824
0.0865811 0.0387662
0.0037846 0.0136732
-0.0209708 0.0321934
5.31452e-07 0.0348291
0.0971047 0.0430532
0.0791961 0.0353131
0.0424529 0.0178351
0.104819 0.0394789
0.060558 0.0352635
0.0986864 0.0316266
0.0424529 0.0272091
-0.00731719 0.0369775
-0.0621343 0.0233401
0.0122452 0.0329032
0.0867334 0.0334901
0.0861486 0.0293972
-0.0860366 0.0177178
0.0582651 0.0357121
0.0599612 0.0336687
0.085161 0.0378647
-0.0271153 0.025187
0.0329332 0.049832
-0.014485 0.0149183
0.0527578 0.0423589
0.0594396 0.0349352
0.126169 0.0389352
0.13219 0.0397028
0.0941706 0.0344695
0.0594397 0.0325708
0.0317697 0.0329409
0.0417629 0.0202349
0.0920969 0.0376412
0.0211031 0.0187515
0.0261395 0.0440252
0.0462066 0.0284728
0.0871096 0.0345632
-0.000805986 0.0292878
0.100097 0.0411267
0.0271289 0.0216668
0.062595 0.0360064
0.0174339 0.0353074
0.0594397 0.0296574
0.043628 0.0333548
0.0131858 0.0307001
-0.000781954 0.0183414
-0.0413988 0.0356085
0.032933 0.0396769
0.088729 0.0350949
-0.0295823 0.0201867
-3.11621e-07 0.0251545
0.0319943 0.0484773
2.9023e-05 0.0175827
0.0174335 0.0309424
0.0194912 0.0180704
-0.0412685 0.0414409
0.0435639 0.0308294
0.05627 0.040851
0.0290868 0.052268
-0.0414033 0.0190206
0.0461382 0.054726
-0.0734384 0.0261442
-0.014471 0.0280373
0.027823 0.0369714
0.0734394 0.0270505
0.0373475 0.0467816
0.0971043 0.0374057
0.0199307 0.0397035
-0.0267352 0.0227747
0.0205116 0.0433981
0.0857284 0.0294084
0.059435 0.0405979
0.0997462 0.0490542
0.0867202 0.0327079
0.0927398 0.0511949
2.27454e-05 0.0243849
0.0538009 0.0426274
0.0860428 0.0334904
0.130005 0.04641
-0.036223 0.0226845
0.0261392 0.0463356
-0.0144843 0.026122
-0.0494977 0.0191138
0.0417489 0.0335826
0.088729 0.0335747
0.0335492 0.0300617
0.0882823 0.0286907
0.0261392 0.0502685
0.0870053 0.035114
0.0322983 0.0379138
0.0764481 0.0276025
-0.0250779 0.0489998
0.0594379 0.0433944
0.014471 0.0193151
0.0134225 0.0402116
-0.0144706 0.0409363
-0.00894125 0.025279
0.062595 0.0346245
0.0492474 0.0235508
0.0295774 0.024266
0.0859465 0.0439165
0.0945038 0.0289108
0.0594481 0.0386628
0.0281391 0.0477604
parameters: [ 9.     0.421  4.416  1.2    5.744]. error: 29911076641.7.
----------------------------
epoch 0, loss 1.05076
epoch 128, loss 1.12396
epoch 256, loss 1.27595
epoch 384, loss 1.19688
epoch 512, loss 1.2491
epoch 640, loss 1.07489
epoch 768, loss 1.18229
epoch 896, loss 1.5366
epoch 1024, loss 1.41504
epoch 1152, loss 1.13559
epoch 1280, loss 0.963346
epoch 1408, loss 1.12367
epoch 1536, loss 1.2111
epoch 1664, loss 1.19767
epoch 1792, loss 1.1173
epoch 1920, loss 1.04139
epoch 2048, loss 1.0409
epoch 2176, loss 1.16787
epoch 2304, loss 1.17459
epoch 2432, loss 1.07663
epoch 2560, loss 0.93203
epoch 2688, loss 0.785297
epoch 2816, loss 1.15765
epoch 2944, loss 1.2587
epoch 3072, loss 1.12781
epoch 3200, loss 0.895899
epoch 3328, loss 1.06959
epoch 3456, loss 0.869852
epoch 3584, loss 0.816642
epoch 3712, loss 1.04873
epoch 3840, loss 1.08597
epoch 3968, loss 1.11595
epoch 4096, loss 0.990406
epoch 4224, loss 1.18725
epoch 4352, loss 1.04438
epoch 4480, loss 0.950124
epoch 4608, loss 1.01188
epoch 4736, loss 0.992395
epoch 4864, loss 1.10036
epoch 4992, loss 0.897227
epoch 5120, loss 1.00154
epoch 5248, loss 1.03029
epoch 5376, loss 1.14595
epoch 5504, loss 1.07222
epoch 5632, loss 0.929505
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0364958 0.0340831
0.123121 0.0251283
0.0527578 0.0348353
0.0764328 0.0262278
4.07333e-06 0.00883659
0.0594426 0.0338584
0.0599612 0.024009
0.0477959 0.0304846
0.0397476 0.0266188
0.0594396 0.0296945
0.0650734 0.016209
0.0626002 0.0162542
-0.0860431 0.0184478
0.01684 0.0320959
0.0236481 0.0355638
0.032158 0.0307609
0.0495801 0.0207186
-5.97989e-06 0.0193974
0.130756 0.0238391
0.0318766 0.0275442
0.0144853 0.0186639
0.0907408 0.0310781
-0.0611272 0.0184912
0.110244 0.0365583
-0.0132965 0.0261801
-7.99663e-06 0.0107986
0.00250001 0.017415
0.0867332 0.0287721
0.032195 0.0275758
0.0594374 0.0347967
0.0991788 0.0289955
0.0272988 0.0310365
0.0861486 0.0294618
-4.4075e-07 0.0144855
0.129458 0.0201046
0.0268143 0.0309596
-1.32922e-07 0.0200048
-0.0188062 0.0144225
0.0594375 0.0267855
0.0109586 0.0242956
8.39808e-07 0.013114
0.0650734 0.0209717
-0.0860447 0.0174098
0.0482639 0.0283274
0.0373484 0.0320572
-0.0131851 0.0150576
-0.0144856 0.0265219
0.0327313 0.0242185
-0.0248182 0.0317989
0.0223972 0.0325168
0.0373484 0.0309232
0.126171 0.0273701
0.025123 0.029491
0.101441 0.0354489
0.0594399 0.0236884
0.090741 0.0323548
0.0117847 0.0201344
0.059435 0.0337465
0.0920961 0.0259196
0.0390991 0.0177714
0.088729 0.0308085
0.049499 0.0285631
0.0220538 0.0310998
-0.0204028 0.0237899
0.0468012 0.0217246
-0.000835063 0.00848617
9.99101e-07 0.0240239
0.0217933 0.0205215
0.0375382 0.0330986
0.0859865 0.031692
-0.0362226 0.0160403
0.0122295 0.0256943
-0.0776845 0.0252385
-0.0117788 0.0190115
0.0477957 0.0320748
-0.0248165 0.0260967
0.0813268 0.0324621
-0.0023637 0.029667
-0.0807986 0.0323484
0.106645 0.0250741
0.0209836 0.0290462
0.0435639 0.0250596
0.121243 0.034877
0.032933 0.0339415
0.0317699 0.0192069
0.12614 0.0304754
0.0706156 0.028374
-0.0161473 0.0231583
0.100668 0.0262542
0.0871095 0.0246522
-0.00628321 0.0345189
0.027823 0.0159989
-0.0207292 0.0190027
0.0675205 0.0211659
-2.95455e-07 0.0216817
0.0301505 0.030529
0.0764428 0.0283194
0.0267369 0.0210683
0.0861486 0.0291556
0.0710836 0.0274242
-4.4075e-07 0.015856
-0.065961 0.0349075
0.0867119 0.0327489
-0.0023636 0.0311984
-0.00131975 0.0171628
-0.00700272 0.0303658
-0.00701261 0.0223164
0.101441 0.0336079
-0.0118768 0.0226706
0.076417 0.0306146
-0.0385437 0.0252929
0.0511643 0.0261433
0.092056 0.0262905
0.0327315 0.0294414
0.09274 0.0329072
0.126171 0.0182969
-1.92327e-05 0.0209982
0.029557 0.0282108
-0.10011 0.0125858
0.101441 0.0329544
0.0527576 0.0250859
0.044975 0.0247
-0.0131858 0.00716644
-0.0385424 0.0220603
-0.0529698 0.0144412
0.0594301 0.0346423
0.036223 0.0105173
0.0977799 0.0230035
parameters: [ 9.     0.421  4.354  1.2    5.744]. error: 896567764.463.
----------------------------
epoch 0, loss 1.0938
epoch 128, loss 0.952709
epoch 256, loss 1.08284
epoch 384, loss 0.801654
epoch 512, loss 0.901802
epoch 640, loss 0.965693
epoch 768, loss 0.984088
epoch 896, loss 0.973287
epoch 1024, loss 0.998219
epoch 1152, loss 1.12216
epoch 1280, loss 1.08968
epoch 1408, loss 1.07957
epoch 1536, loss 0.914885
epoch 1664, loss 1.00816
epoch 1792, loss 1.08588
epoch 1920, loss 0.945226
epoch 2048, loss 0.825157
epoch 2176, loss 1.16015
epoch 2304, loss 1.06228
epoch 2432, loss 0.952382
epoch 2560, loss 0.778861
epoch 2688, loss 1.18828
epoch 2816, loss 0.91833
epoch 2944, loss 0.95323
epoch 3072, loss 0.855855
epoch 3200, loss 1.14801
epoch 3328, loss 1.01741
epoch 3456, loss 0.849595
epoch 3584, loss 0.775553
epoch 3712, loss 1.02093
epoch 3840, loss 1.11547
epoch 3968, loss 0.966439
epoch 4096, loss 0.729227
epoch 4224, loss 0.920207
epoch 4352, loss 0.883894
epoch 4480, loss 1.04214
epoch 4608, loss 0.745613
epoch 4736, loss 1.02444
epoch 4864, loss 0.797002
epoch 4992, loss 1.03329
epoch 5120, loss 1.13768
epoch 5248, loss 0.965868
epoch 5376, loss 0.977702
epoch 5504, loss 1.15681
epoch 5632, loss 0.962843
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0317699 0.0342537
-0.0621272 0.0178324
0.0707072 0.0135069
0.0594397 0.0454332
0.0248076 0.0218277
0.0529729 0.0252705
0.0871098 0.0459791
-0.0414033 0.0251006
0.0776735 0.0281494
0.0941706 0.0446943
0.0964307 0.0359575
0.0373484 0.0269782
0.0222152 0.0542139
0.0261391 0.0351658
0.0800821 0.0343371
0.018802 0.0317827
-0.039089 0.0202039
0.0859467 0.0387873
0.015257 0.0323158
3.62028e-05 0.0323062
-0.0300731 0.0191011
0.00894166 0.0152918
0.0109586 0.0196368
-0.00459356 0.0160949
0.0691984 0.0405162
0.0820225 0.0218769
0.0599029 0.0330529
0.0424428 0.0262022
0.0582589 0.0302221
0.0871095 0.0351196
0.0594397 0.0303377
0.0464005 0.0363773
0.0594426 0.0430137
-0.027288 0.0240059
0.0278231 0.0393703
0.0867199 0.034956
0.0865811 0.0368973
0.046218 0.0290142
4.57767e-08 0.0267901
0.0897923 0.0371662
0.126171 0.0369703
-0.0385437 0.0246909
0.0222151 0.0257891
0.0405236 0.0434612
0.0461382 0.0441462
0.0189828 0.0253543
0.0901948 0.0463825
0.0621308 0.0245158
0.0271115 0.0312107
0.0868848 0.0529594
0.0140549 0.0378902
-0.0621274 0.0146246
-0.10012 0.0224407
0.0387924 0.0344265
0.0495763 0.0142613
0.046218 0.0162149
0.0144707 0.0170684
0.042448 0.0380323
0.0710837 0.0375118
0.0594379 0.035182
0.112932 0.0512371
0.0904958 0.0449351
-0.0412711 0.0203445
0.0541426 0.0298764
0.0224432 0.0396156
0.0625951 0.0463842
0.032902 0.0368361
0.0109586 0.0326753
0.0333262 0.0337042
0.0594396 0.0434653
-0.0271111 0.0083503
0.0365698 0.0100709
0.0764328 0.0362695
0.0707072 0.0168651
0.0621302 0.0303569
-0.0662843 0.0159479
0.0188603 0.0301209
0.0317699 0.0422111
0.0867332 0.0397201
0.0286795 0.0443309
0.014484 0.0232735
0.0337178 0.034267
0.0394802 0.0314002
-0.0860447 0.0110302
0.0322983 0.0473026
0.036794 0.0215762
0.0397476 0.0352313
-0.0385434 0.0250416
0.0594396 0.0459706
0.0327314 0.0374423
0.0207377 0.0253273
0.0562648 0.0399712
0.0691985 0.0408913
0.0318693 0.0478488
-0.0529659 0.0242655
0.0860364 0.0195657
-0.0417422 0.0178735
0.0251231 0.0249832
0.0594301 0.039907
-0.0662833 0.0252298
0.0920561 0.0396272
0.0594397 0.0433742
0.072741 0.0392366
0.12945 0.0447048
-0.0272904 0.0219629
-0.0716787 0.0254586
-0.0109842 0.0388945
0.0594399 0.0447688
0.0520969 0.0387552
0.0813321 0.0354212
-0.00549668 0.0419587
0.0268142 0.0365573
-0.0340329 0.0288892
0.10664 0.0363296
-3.66166e-06 0.0253542
0.0152616 0.0348266
0.0416418 0.0373043
0.0122142 0.0212086
0.0871098 0.0325529
-0.0248165 0.0251123
-0.0152533 0.0317232
0.0317699 0.0324698
0.0385427 0.035253
0.13219 0.0314526
0.00596587 0.0283085
0.029557 0.0204196
0.0407515 0.0284287
-0.0188593 0.0233144
parameters: [ 9.     0.421  4.382  1.2    5.744]. error: 2.62013553325e+12.
----------------------------
epoch 0, loss 1.32623
epoch 128, loss 1.26947
epoch 256, loss 1.27478
epoch 384, loss 1.46797
epoch 512, loss 1.3251
epoch 640, loss 1.29284
epoch 768, loss 1.30118
epoch 896, loss 1.11106
epoch 1024, loss 1.22822
epoch 1152, loss 1.17312
epoch 1280, loss 1.39751
epoch 1408, loss 0.845529
epoch 1536, loss 1.19764
epoch 1664, loss 1.39882
epoch 1792, loss 1.10429
epoch 1920, loss 1.15651
epoch 2048, loss 1.0084
epoch 2176, loss 1.14682
epoch 2304, loss 1.0815
epoch 2432, loss 0.87892
epoch 2560, loss 1.05631
epoch 2688, loss 1.21402
epoch 2816, loss 1.04765
epoch 2944, loss 1.19495
epoch 3072, loss 1.16807
epoch 3200, loss 1.00743
epoch 3328, loss 1.13002
epoch 3456, loss 1.2404
epoch 3584, loss 1.0121
epoch 3712, loss 1.32737
epoch 3840, loss 1.10952
epoch 3968, loss 1.12394
epoch 4096, loss 1.16096
epoch 4224, loss 1.12798
epoch 4352, loss 1.28348
epoch 4480, loss 1.22505
epoch 4608, loss 0.945335
epoch 4736, loss 1.08282
epoch 4864, loss 0.90647
epoch 4992, loss 0.813344
epoch 5120, loss 1.12177
epoch 5248, loss 0.994272
epoch 5376, loss 1.03632
epoch 5504, loss 1.17368
epoch 5632, loss 0.978224
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594027 0.0508162
0.0375434 0.0436445
0.088282 0.0492324
0.0424428 0.0471151
-0.00733491 0.0468856
0.0710836 0.0440298
-0.0417425 0.0446941
0.0626054 0.0476801
0.0945151 0.0528178
0.0300726 0.0493499
0.0211031 0.0470279
0.0435639 0.0417416
0.0327314 0.0484183
0.076422 0.0478412
0.026809 0.0432602
0.032933 0.0467394
0.0867202 0.0490276
0.123121 0.0411128
0.0532557 0.0526399
0.130757 0.0501103
0.132178 0.0488733
0.132184 0.0487831
-0.0267379 0.0450998
0.0152616 0.0464078
-0.0734312 0.048351
0.0964772 0.0502284
0.0281392 0.0503913
0.0631173 0.0489555
0.0867334 0.0468392
0.0594397 0.0464844
-0.00142797 0.0445529
0.0199307 0.0458992
0.0716781 0.0457594
0.0424369 0.0456794
0.0764118 0.0434065
0.0595095 0.0413794
0.109566 0.0459064
8.40829e-06 0.0410017
0.0594398 0.0488317
0.0907408 0.0504842
2.5668e-06 0.0453171
0.0191276 0.0470877
0.0497671 0.0441265
0.0385431 0.048771
0.0594399 0.0508761
0.0594449 0.0481873
-0.00701261 0.0427096
0.00863032 0.0456638
0.0968257 0.0467381
0.0764428 0.0434394
0.126086 0.0432296
-0.00484737 0.045794
0.0991789 0.0519313
-2.99285e-06 0.0418967
0.0327313 0.0434707
0.05627 0.0452682
-3.17974e-05 0.0468812
-0.016115 0.0423697
0.0211183 0.0438534
-0.0188057 0.0440181
0.0871096 0.0498878
0.0113491 0.0486648
0.0983621 0.0478222
0.0290868 0.0537741
-0.0414014 0.0487429
-0.041769 0.0441901
0.05627 0.0452682
0.0464005 0.0482624
0.0971047 0.0501081
-0.0118768 0.0500923
0.0327315 0.0441634
0.0594426 0.0454942
0.074953 0.0393029
0.0477959 0.0534519
0.0134334 0.0387083
0.0724741 0.0465513
-0.0210896 0.0495595
0.0865811 0.0459528
0.0659505 0.0535359
0.0937752 0.0408879
-0.0248182 0.0480944
0.0220537 0.0429831
0.0861487 0.0438058
0.0362234 0.0457706
-0.00236343 0.0457698
0.0594397 0.0441565
0.0901952 0.0467301
0.0177216 0.0461468
0.0188018 0.0462099
0.0267369 0.0522369
1.96444e-05 0.0458301
-0.0267425 0.0516348
0.0859467 0.0536577
0.0301505 0.0463393
-0.00968441 0.0404532
0.0117785 0.0463404
0.0327313 0.048412
-0.00236368 0.0493475
0.0247829 0.046482
0.0477956 0.0480393
0.0861487 0.0463878
0.0384776 0.0449856
-0.0631232 0.0512552
4.57767e-08 0.0470482
0.0538177 0.0493538
0.015257 0.0486891
-0.0467978 0.0458301
-1.92327e-05 0.0488804
0.0945151 0.045364
0.0532677 0.0541253
0.0631196 0.0531669
-0.0250779 0.0575215
0.0730675 0.0506468
0.0477959 0.0534195
0.0117785 0.04443
0.0594301 0.0539245
-0.0860431 0.0454761
-0.034636 0.0454882
0.0927398 0.0542134
0.046799 0.0460513
0.0458117 0.0517025
-0.0271126 0.0474486
0.0752321 0.0439879
0.0594452 0.0412092
0.000793148 0.0452243
0.0261395 0.0507536
-0.00701261 0.0510201
0.0327313 0.0475461
parameters: [ 9.     0.421  4.382  2.2    5.744]. error: 1.0708899916e+13.
----------------------------
epoch 0, loss 1.00844
epoch 128, loss 0.876303
epoch 256, loss 1.17575
epoch 384, loss 1.27705
epoch 512, loss 1.34932
epoch 640, loss 0.797715
epoch 768, loss 1.03264
epoch 896, loss 0.681371
epoch 1024, loss 0.92783
epoch 1152, loss 0.781611
epoch 1280, loss 0.823064
epoch 1408, loss 0.947454
epoch 1536, loss 0.834443
epoch 1664, loss 1.10741
epoch 1792, loss 0.950648
epoch 1920, loss 0.982042
epoch 2048, loss 1.26537
epoch 2176, loss 0.893215
epoch 2304, loss 0.781329
epoch 2432, loss 1.14739
epoch 2560, loss 1.26654
epoch 2688, loss 1.08659
epoch 2816, loss 0.681915
epoch 2944, loss 1.13931
epoch 3072, loss 0.886016
epoch 3200, loss 0.805952
epoch 3328, loss 1.57374
epoch 3456, loss 1.15828
epoch 3584, loss 1.29914
epoch 3712, loss 0.841576
epoch 3840, loss 1.03573
epoch 3968, loss 0.735737
epoch 4096, loss 1.25947
epoch 4224, loss 1.30938
epoch 4352, loss 0.904549
epoch 4480, loss 1.53793
epoch 4608, loss 0.820794
epoch 4736, loss 0.817554
epoch 4864, loss 0.437517
epoch 4992, loss 1.00373
epoch 5120, loss 0.684943
epoch 5248, loss 0.953259
epoch 5376, loss 0.878427
epoch 5504, loss 1.259
epoch 5632, loss 0.938802
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0337182 0.059865
0.0234704 0.0410963
-2.23337e-05 0.0368649
0.121243 0.053216
0.0496682 0.0559745
3.5013e-07 0.0352217
0.101814 0.04523
0.032933 0.0482385
0.0594375 0.0497118
0.0968255 0.0550226
6.39156e-06 0.036311
0.0710838 0.0491749
0.0868849 0.0480991
0.100097 0.0509377
0.0177216 0.0474208
0.0764481 0.051153
0.0920508 0.0521792
-0.0204101 0.0485105
-0.129445 0.039759
0.0991789 0.0563548
0.0170834 0.0390745
0.0223972 0.0478266
-0.0707068 0.0466986
0.0989675 0.0502782
0.0997468 0.0522477
-3.40966e-05 0.0419128
0.0594398 0.0552643
2.9023e-05 0.0499074
0.0373484 0.0495771
0.0594397 0.0503863
0.00250079 0.0246292
0.0997462 0.0462127
-0.0188593 0.0288824
0.0134334 0.0480539
0.0977799 0.0539609
0.0594396 0.0549281
0.0991788 0.0402964
0.0599029 0.0470966
-0.00142797 0.0323025
0.05943 0.0396585
0.0964766 0.0437413
0.046218 0.0298779
0.0927397 0.0486303
-0.0204001 0.034836
-0.0529722 0.0390317
-0.036223 0.0474362
0.0346452 0.0341326
-0.0716682 0.0440571
0.100097 0.0465014
0.0562649 0.0380003
0.0188019 0.0507159
0.0582651 0.0475598
-0.0857218 0.0302755
-0.0494977 0.0456575
0.104819 0.0530023
0.0321586 0.0439081
0.0781223 0.0526264
-3.17974e-05 0.04032
-3.66166e-06 0.047005
-0.0272904 0.0443852
0.0317699 0.0535677
0.0945151 0.0493832
0.0318787 0.0350034
-0.0776729 0.0518564
0.021525 0.0517168
0.0661225 0.0486703
0.0910567 0.0525341
0.0859865 0.0439618
0.0330953 0.0497063
0.0207331 0.0484663
0.0247829 0.0464575
-0.0385433 0.0427514
0.0800819 0.0379937
0.0317699 0.0404051
0.0439209 0.0433355
0.0271163 0.0446243
0.0594397 0.0518222
-0.0267425 0.0453623
0.0318693 0.0427894
-0.00460347 0.0377985
-0.0417792 0.047246
-0.080798 0.0430199
0.0966596 0.0306007
0.0390889 0.0421618
0.13219 0.0533914
0.0468064 0.0337069
-0.0385433 0.0206126
0.027823 0.0474931
0.0781227 0.0335448
0.0594398 0.0457389
0.0920613 0.0330046
0.059435 0.0411131
0.0589765 0.0545048
0.0152545 0.0502855
0.022215 0.0544604
0.0364958 0.0338549
0.0594301 0.0372296
0.0368964 0.0384409
-0.0117788 0.040917
0.0791268 0.0518257
0.080402 0.0319545
0.0739227 0.0362906
0.000807032 0.0380439
-0.0188062 0.0319422
0.01684 0.0521013
0.0412725 0.0246708
0.0867335 0.0583649
0.0650733 0.0490225
-0.0295746 0.0354487
-0.0247813 0.0269372
0.0435639 0.0480231
0.0290868 0.0388172
0.0261392 0.0584897
0.025123 0.039497
-0.0113401 0.0485092
-0.0611177 0.0226924
-0.0417425 0.0348843
-0.0734361 0.0350016
0.080402 0.0293001
-0.0194973 0.0385198
0.0396778 0.0350375
0.0661224 0.0547372
0.0281393 0.0448911
-0.00236368 0.0340787
0.076422 0.0516272
0.0527577 0.0530948
0.135017 0.0416743
-0.0131841 0.0401612
parameters: [ 9.     0.421  4.382  0.418  5.744]. error: 32096014382.6.
----------------------------
epoch 0, loss 1.23652
epoch 128, loss 0.48931
epoch 256, loss 0.817825
epoch 384, loss 0.616551
epoch 512, loss 0.9191
epoch 640, loss 1.07733
epoch 768, loss 0.526398
epoch 896, loss 0.338529
epoch 1024, loss 1.21183
epoch 1152, loss 0.926938
epoch 1280, loss 1.12146
epoch 1408, loss 0.239694
epoch 1536, loss 0.498214
epoch 1664, loss 0.701387
epoch 1792, loss 0.0399521
epoch 1920, loss 0.679021
epoch 2048, loss 0.822125
epoch 2176, loss 1.00404
epoch 2304, loss 1.45102
epoch 2432, loss 0.269052
epoch 2560, loss 0.498165
epoch 2688, loss 2.47623
epoch 2816, loss 0.226978
epoch 2944, loss 1.63093
epoch 3072, loss 1.52413
epoch 3200, loss 0.323325
epoch 3328, loss 0.494991
epoch 3456, loss 0.693674
epoch 3584, loss 0.706158
epoch 3712, loss 2.28983
epoch 3840, loss 1.82739
epoch 3968, loss 0.697998
epoch 4096, loss 0.338281
epoch 4224, loss 1.49372
epoch 4352, loss 0.60699
epoch 4480, loss 1.31512
epoch 4608, loss 1.65924
epoch 4736, loss 1.0513
epoch 4864, loss 0.750025
epoch 4992, loss 0.207559
epoch 5120, loss 0.98601
epoch 5248, loss 0.245484
epoch 5376, loss 1.74799
epoch 5504, loss 0.481496
epoch 5632, loss 0.304054
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0222151 0.0274725
-0.0131848 0.0360726
4.57767e-08 0.0405932
0.0131845 0.0364443
0.0871095 0.0351333
-0.0417488 0.0361685
0.0710835 0.0337672
0.059435 0.0409144
0.0907408 0.0336704
0.0857794 0.03143
0.10955 0.0288926
0.0477957 0.03378
0.130005 0.0353324
0.0813321 0.0412155
-0.0776729 0.0311028
-0.0248069 0.0315949
-0.0234626 0.0299725
0.0594325 0.0404195
-0.0204101 0.0398654
0.0407513 0.0379272
0.0594398 0.0287589
-2.99285e-06 0.0347286
4.07333e-06 0.0367505
0.00131924 0.03925
-0.02083 0.0351648
0.0989675 0.0416441
0.0131858 0.0406808
-3.40966e-05 0.0339202
-0.0716682 0.0354287
-0.0144836 0.0373575
0.0710837 0.0371064
2.9023e-05 0.0386885
0.0461385 0.028009
-0.0340213 0.0303355
-0.0529698 0.03168
0.10665 0.0303077
0.0417489 0.0376537
0.029557 0.0334265
0.0417448 0.0406562
-0.0271111 0.036315
1.74905e-07 0.0379815
0.121243 0.037249
-6.66928e-06 0.0369738
-0.00893962 0.0373717
-1.32922e-07 0.0414843
-0.0118764 0.0315294
0.0594396 0.0314587
0.0871097 0.0331754
0.0412702 0.0372691
0.0861487 0.0327875
0.0224432 0.0274633
-0.0300731 0.039783
0.0417629 0.0334205
0.0907408 0.0312845
0.059435 0.0407347
0.0593312 0.0360046
0.0168394 0.0413962
0.0727411 0.0268848
0.121243 0.0349979
0.126127 0.0351012
-0.0267352 0.0321996
0.028139 0.0386659
0.0734394 0.0372833
0.104819 0.0350439
0.0385437 0.0270037
0.0621334 0.0333057
0.0920961 0.0269503
0.062595 0.0294644
-0.0118767 0.0343363
0.102034 0.0354011
0.05943 0.0345073
0.0897923 0.0344181
0.0734322 0.0360761
0.0599612 0.0319795
-0.00891927 0.0365158
0.0707062 0.0391946
0.0396777 0.039263
0.0727409 0.0307408
-4.4075e-07 0.0375709
0.0813321 0.0407399
0.121243 0.0351165
0.0675205 0.0390511
2.9023e-05 0.0359425
-0.0385437 0.0372411
0.0189828 0.042327
0.059181 0.0370449
0.0800821 0.0353905
0.0911173 0.0265369
-0.0385421 0.0285963
-0.0271111 0.031756
0.0109857 0.0276281
0.0989675 0.0425589
-0.053266 0.0265273
0.0532677 0.0279666
0.0594398 0.0288287
0.0321468 0.0351466
0.0188015 0.0391142
0.0593758 0.0419646
0.0594396 0.0280773
1.95996e-07 0.0302738
0.00856342 0.0411441
0.0461382 0.0268383
0.130756 0.0323897
0.0177241 0.0318513
-0.063123 0.032323
0.0477959 0.0371352
-0.129453 0.0352318
-0.0414014 0.0344211
0.0594297 0.0407269
-0.0776729 0.0268423
0.0594396 0.0351697
0.0857795 0.031436
0.0529666 0.0345285
0.0290868 0.028703
0.0326439 0.0342223
-0.0118767 0.0321806
0.0412776 0.0367159
0.0867201 0.0327533
-0.0462049 0.0272407
-0.0323135 0.0338566
-0.0118765 0.0307037
-0.0385427 0.0372967
0.0867119 0.0407822
0.0594397 0.0280799
-0.0532547 0.0339357
-0.0414014 0.0365315
-0.0117817 0.0336475
0.0093143 0.0327964
parameters: [ 9.     0.421  4.382  0.068  5.744]. error: 7.40494597531e+12.
----------------------------
epoch 0, loss 1.00058
epoch 128, loss 1.1339
epoch 256, loss 0.974946
epoch 384, loss 0.997915
epoch 512, loss 1.26771
epoch 640, loss 1.23451
epoch 768, loss 0.927891
epoch 896, loss 1.08124
epoch 1024, loss 1.05497
epoch 1152, loss 0.91446
epoch 1280, loss 1.26358
epoch 1408, loss 1.06491
epoch 1536, loss 1.12397
epoch 1664, loss 1.12267
epoch 1792, loss 0.947465
epoch 1920, loss 1.13386
epoch 2048, loss 0.946503
epoch 2176, loss 1.09966
epoch 2304, loss 1.03934
epoch 2432, loss 1.13623
epoch 2560, loss 1.03615
epoch 2688, loss 1.25085
epoch 2816, loss 0.920833
epoch 2944, loss 1.03839
epoch 3072, loss 1.60256
epoch 3200, loss 0.754042
epoch 3328, loss 0.94828
epoch 3456, loss 0.865933
epoch 3584, loss 1.03372
epoch 3712, loss 0.99028
epoch 3840, loss 1.31211
epoch 3968, loss 0.931221
epoch 4096, loss 0.91612
epoch 4224, loss 1.07962
epoch 4352, loss 1.12721
epoch 4480, loss 0.964177
epoch 4608, loss 0.948436
epoch 4736, loss 0.772056
epoch 4864, loss 0.986783
epoch 4992, loss 1.1585
epoch 5120, loss 0.964219
epoch 5248, loss 0.96571
epoch 5376, loss 0.900288
epoch 5504, loss 1.10332
epoch 5632, loss 0.784012
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0871098 0.0555321
0.0991789 0.0610059
0.0662934 0.0417466
0.0859467 0.0598254
0.0394803 0.0265858
0.0710838 0.0554542
0.059181 0.050285
-0.0365701 0.0313707
0.0857284 0.0420743
0.0851609 0.0400827
0.0594297 0.0504686
0.0950804 0.061743
-0.00968441 0.0416343
-0.0707062 0.0294948
0.0152545 0.0389928
-0.0495796 0.0267081
0.0199306 0.0462682
0.0174335 0.0279082
3.40452e-06 0.0416745
-2.23337e-05 0.0265032
0.076438 0.0304185
0.0867335 0.0381954
0.0907408 0.0492661
0.0261395 0.065906
0.0594449 0.0380047
-0.034636 0.0343355
0.0734366 0.0381962
0.0867334 0.0418669
0.0594397 0.0416956
0.0482639 0.0301753
0.121243 0.0481707
0.059407 0.0490452
0.0131858 0.0373269
0.0624794 0.0490074
-0.0300731 0.0300828
0.0337178 0.0309209
0.0621302 0.0296083
0.130756 0.0350631
-0.0449599 0.0296663
-0.00236357 0.054628
0.0416434 0.0340866
0.00460075 0.02734
-0.0106908 0.0341365
-0.0177194 0.0405823
0.0594301 0.0468288
0.0897923 0.0573031
0.0611263 0.0422159
0.028139 0.0559303
0.0168394 0.0400846
0.0589765 0.0400631
0.0295774 0.0319621
0.0857795 0.0542544
0.0791969 0.0512851
-0.0414059 0.0289772
0.080402 0.0391282
0.0605534 0.0435272
0.0321466 0.0425477
-0.0449599 0.0301903
0.0968255 0.0636037
0.0776855 0.0410174
0.112932 0.0483713
0.0596272 0.046595
-0.014485 0.0522522
0.0188596 0.0430256
0.0599612 0.0535952
0.0991788 0.0448987
0.0964766 0.0484401
0.0594301 0.0331328
0.13219 0.0364585
0.0910569 0.0410913
0.0109857 0.0332056
-0.00894125 0.0440585
-0.00142113 0.0335888
-0.0734361 0.0464835
0.0520969 0.0567468
0.0527575 0.073816
-7.68032e-07 0.030562
0.00863088 0.0381149
-0.0152533 0.0337752
0.0631173 0.0490546
0.088282 0.0385642
0.0859962 0.0497113
0.0329332 0.0442571
0.0496683 0.0546066
0.000786701 0.0300428
0.0927397 0.0613471
0.032933 0.0697619
0.0594397 0.0456151
0.0807993 0.0341042
0.0532557 0.0468395
0.0188603 0.0273161
0.00894166 0.0342737
0.0861488 0.0453701
0.0424529 0.0550872
0.0781227 0.0591821
-0.000801651 0.0348527
0.00131949 0.0335424
0.0920961 0.0543441
-0.00968441 0.0528474
-0.0385421 0.0518083
0.0857795 0.0357933
-0.0631103 0.042691
0.0910567 0.0409555
0.0562648 0.0481212
-0.0394792 0.0260085
-0.0417425 0.0353923
0.0462066 0.0311958
0.104819 0.0322312
0.056285 0.0376306
0.0532557 0.0529817
0.0387924 0.0535628
0.0407515 0.0339335
0.0267443 0.0313549
-0.10012 0.029702
0.0271163 0.02546
0.00931417 0.0376513
0.0594396 0.0323547
0.043921 0.056785
0.0208245 0.0592758
0.0496682 0.0344098
0.0247829 0.0419728
0.0319944 0.0542
0.0234704 0.0294769
0.0106903 0.0447129
-0.0716782 0.0284082
0.0594399 0.0470836
-0.00378521 0.0325823
-0.0362233 0.0317773
parameters: [ 9.     0.421  4.382  1.2    5.744]. error: 165781313.377.
----------------------------
epoch 0, loss 0.858613
epoch 128, loss 0.98373
epoch 256, loss 1.49148
epoch 384, loss 0.884282
epoch 512, loss 0.68536
epoch 640, loss 1.00698
epoch 768, loss 0.781814
epoch 896, loss 0.998735
epoch 1024, loss 1.19788
epoch 1152, loss 0.800212
epoch 1280, loss 1.05212
epoch 1408, loss 1.16745
epoch 1536, loss 0.864806
epoch 1664, loss 1.00977
epoch 1792, loss 1.01153
epoch 1920, loss 0.89252
epoch 2048, loss 1.09124
epoch 2176, loss 0.943104
epoch 2304, loss 0.93845
epoch 2432, loss 0.95901
epoch 2560, loss 0.671942
epoch 2688, loss 0.866213
epoch 2816, loss 0.831039
epoch 2944, loss 0.687571
epoch 3072, loss 0.704
epoch 3200, loss 0.954667
epoch 3328, loss 0.770508
epoch 3456, loss 0.739817
epoch 3584, loss 0.973188
epoch 3712, loss 0.843002
epoch 3840, loss 0.786583
epoch 3968, loss 0.936355
epoch 4096, loss 0.992534
epoch 4224, loss 0.948577
epoch 4352, loss 0.920656
epoch 4480, loss 0.767888
epoch 4608, loss 0.958093
epoch 4736, loss 0.851018
epoch 4864, loss 1.11222
epoch 4992, loss 0.830255
epoch 5120, loss 0.951166
epoch 5248, loss 0.70343
epoch 5376, loss 0.555373
epoch 5504, loss 0.608697
epoch 5632, loss 1.07872
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594204 0.0500608
-0.014471 0.025443
0.059367 0.0461619
0.0346452 0.0233874
0.023648 0.0500996
0.0904958 0.0541709
0.121243 0.0540966
0.0174339 0.0500001
0.0321467 0.0536954
0.00250001 0.0220827
0.0375382 0.05697
0.0964772 0.0412218
0.0594424 0.0582039
0.0813321 0.0500394
0.0661224 0.0512011
0.0188016 0.0533819
0.10955 0.0695143
-0.0267425 0.0310574
0.0234638 0.0256111
-0.0340213 0.0343272
0.0317699 0.0465038
0.0248197 0.0262135
0.05937 0.049144
-0.0414033 0.017988
-0.0267352 0.0244353
-0.0204001 0.0279885
0.0330953 0.0471908
0.0673482 0.045296
0.092056 0.0418693
0.0318766 0.0254413
0.0857788 0.0325498
0.0394802 0.0288333
0.102035 0.0502714
0.0867333 0.0524067
-0.0023636 0.0575039
0.0458117 0.0572515
0.0482639 0.0424955
-0.0662867 0.0254822
0.000213748 0.0504432
0.0927396 0.0474004
-0.00699273 0.033227
0.109566 0.0545314
0.00250001 0.0143097
0.0599031 0.058782
0.0236482 0.0414113
0.0867201 0.0518079
-0.000792737 0.0566062
-3.17974e-05 0.0367702
-0.0131851 0.0479381
0.0248076 0.0329961
0.0907408 0.0404827
0.100667 0.0275694
0.0997468 0.0426188
0.0950802 0.0493112
0.121243 0.0445296
-0.000835048 0.031195
0.0251234 0.0585532
0.0261393 0.0364664
0.0594397 0.0585008
0.0897923 0.0487025
0.0593133 0.0345627
0.0346353 0.0207368
0.0589764 0.0513602
0.0594396 0.0543307
-0.0209824 0.0272149
0.0594273 0.0387079
0.0752422 0.0565427
-0.0417491 0.014932
0.0861486 0.0488772
0.0867201 0.0532697
0.076422 0.0318853
0.0520969 0.0531076
0.0283785 0.054188
0.0675205 0.048915
0.0414051 0.0224974
-0.0417488 0.0178312
0.0333261 0.0469037
0.0594399 0.0370274
0.0857794 0.0364137
0.0529729 0.0256395
-0.0106876 0.0127252
0.0152616 0.037701
-0.00700262 0.0496735
-0.0417425 0.0289132
0.0986862 0.0501689
0.0283785 0.0580688
0.0222151 0.037829
0.0416484 0.0204827
0.0861486 0.0584636
0.0189828 0.0235817
0.086885 0.0395765
0.0217933 0.0438595
0.0776735 0.0309153
0.0734439 0.0286884
0.0529729 0.0240974
0.09274 0.0465127
-0.0776845 0.0479953
0.129925 0.054318
0.0596272 0.034307
-0.0161473 0.0353304
0.0261394 0.0463099
-0.0208232 0.0323408
0.0417454 0.0206057
-0.0131851 0.0290963
0.0236479 0.0529034
0.046799 0.0342771
-0.0776729 0.0303792
-0.0716782 0.0200436
0.0261394 0.0468221
-3.6833e-05 0.0207279
0.0857794 0.0482117
0.104819 0.0543778
0.01134 0.0228769
0.0800819 0.0517265
0.0188597 0.0212483
0.028139 0.0436234
0.0271204 0.0121084
0.0853245 0.0375913
0.0920961 0.0381051
0.0122404 0.0713182
0.0106903 0.0127422
-0.0631101 0.0192697
-0.0860366 0.0315057
0.0209836 0.0224372
0.0986864 0.0439261
0.0907408 0.0411084
0.0594397 0.0468024
0.0496681 0.0601646
parameters: [ 9.     0.421  4.382  0.768  5.744]. error: 8555.83918827.
----------------------------
epoch 0, loss 1.60669
epoch 128, loss 1.13055
epoch 256, loss 1.13546
epoch 384, loss 1.27357
epoch 512, loss 1.02322
epoch 640, loss 1.24012
epoch 768, loss 0.879516
epoch 896, loss 0.983944
epoch 1024, loss 0.749769
epoch 1152, loss 1.11906
epoch 1280, loss 0.853411
epoch 1408, loss 1.20606
epoch 1536, loss 1.16104
epoch 1664, loss 0.672148
epoch 1792, loss 0.851782
epoch 1920, loss 1.09761
epoch 2048, loss 0.949248
epoch 2176, loss 0.472543
epoch 2304, loss 0.772268
epoch 2432, loss 0.871102
epoch 2560, loss 0.935541
epoch 2688, loss 1.08712
epoch 2816, loss 0.727607
epoch 2944, loss 0.57674
epoch 3072, loss 0.773591
epoch 3200, loss 1.22396
epoch 3328, loss 0.498649
epoch 3456, loss 0.771451
epoch 3584, loss 0.642062
epoch 3712, loss 0.989901
epoch 3840, loss 0.806028
epoch 3968, loss 0.795702
epoch 4096, loss 1.13527
epoch 4224, loss 0.901551
epoch 4352, loss 0.829584
epoch 4480, loss 0.712843
epoch 4608, loss 0.973697
epoch 4736, loss 0.565132
epoch 4864, loss 1.13884
epoch 4992, loss 0.783035
epoch 5120, loss 0.732529
epoch 5248, loss 0.809244
epoch 5376, loss 0.958472
epoch 5504, loss 0.77791
epoch 5632, loss 0.566442
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0520969 0.0370485
-0.0248069 0.0153724
0.059181 0.0372159
-6.15975e-07 0.00872259
0.0631163 0.00952157
0.0727411 0.0423278
0.0152591 0.0268477
-0.0417491 0.00876601
0.0971047 0.0410357
0.0529666 0.0151188
0.0791969 0.0358424
0.0243594 0.0402956
0.0335493 0.0363932
0.0319947 0.0481257
-0.0161572 0.0429936
-0.00142797 0.0289129
-0.0189887 0.00127013
0.0290868 0.045205
-2.23337e-05 0.0115399
-0.0144706 0.0163998
0.0317698 0.0407542
-0.014471 0.0165966
0.0594396 0.0433147
-0.0707062 0.0134395
0.0867199 0.0370899
0.0593133 0.0407365
0.0861488 0.04526
0.000807032 0.015345
0.0207304 0.0242486
0.0482639 0.0325903
-0.00236357 0.0428186
0.0177216 0.02149
0.0373484 0.0418547
0.0950804 0.0368244
0.0776855 0.0231038
0.0496682 0.0345422
0.0215366 0.030701
0.102035 0.0439622
0.0927397 0.0271861
0.0412725 0.00881842
0.00863092 0.0304623
-0.0113495 0.00479642
-0.0390898 0.00155105
-0.0346416 0.00345999
0.0950804 0.0411949
4.57767e-08 0.0215228
0.0188019 0.0447221
0.0594398 0.0377237
0.0435741 0.0372151
0.0997462 0.0343126
0.028139 0.0388226
-0.0113495 0.0279455
0.0131858 0.00644232
0.0318693 0.0403135
-0.0412757 0.0226266
0.00863092 0.04366
0.081527 0.0393474
-0.0417425 0.017192
0.0621334 0.0225269
0.0468064 0.0290297
0.0857788 0.029847
0.0390889 0.00413131
0.0321466 0.0409105
0.0199306 0.0383524
0.0217931 0.0400606
0.0857788 0.0388137
-0.0323135 0.015259
-0.0390959 0.020511
0.134995 0.0383199
-0.00236357 0.0442789
0.0594427 0.0407357
-0.0271157 0.00312285
0.00932421 0.0358066
0.0865811 0.0334512
0.0781227 0.0476677
0.0781227 0.0436739
0.0396776 0.0327835
0.0648671 0.0275853
0.0991788 0.0407781
0.0177241 0.0179826
-0.00236368 0.0457511
0.0461382 0.0454368
0.05937 0.0422867
-0.0734361 0.0189934
0.0868848 0.04608
0.0625951 0.0395295
0.0594251 0.0332289
0.0871095 0.043465
0.0752321 0.0338186
0.0417489 0.00248865
0.0611271 0.00459684
0.0968257 0.0429575
0.0131849 0.0214614
0.0337178 0.0234739
0.110244 0.0434229
0.0458115 0.0421323
0.00378707 0.00972806
2.27454e-05 0.00543483
0.0562851 0.0433839
0.0594027 0.0436911
0.0827282 0.0078405
0.0261393 0.0207549
0.0217931 0.0322852
0.0562851 0.0394248
-0.0416468 0.0135973
-0.00553123 0.0410075
0.0594398 0.0308628
-0.0417573 0.0229171
0.0650733 0.0412145
0.0594397 0.0411619
0.0592543 0.0360074
-0.0131854 0.00586221
2.9023e-05 0.0101038
0.0867067 0.0384476
0.0920508 0.0339541
0.0188018 0.0386386
-0.0734361 0.0176418
0.0594399 0.0404541
0.0144707 0.0151483
0.0594397 0.0352641
0.0182077 0.0390922
0.0396776 0.0317095
0.0122452 0.0387965
0.0300757 0.00870385
0.0964772 0.0383143
0.032933 0.0392742
6.39156e-06 0.00852295
0.0920961 0.0447391
parameters: [ 9.     0.421  4.382  0.501  5.744]. error: 1.51578086735e+12.
----------------------------
epoch 0, loss 1.58551
epoch 128, loss 1.6433
epoch 256, loss 1.45516
epoch 384, loss 1.42772
epoch 512, loss 1.13447
epoch 640, loss 1.17248
epoch 768, loss 1.29758
epoch 896, loss 1.29862
epoch 1024, loss 1.18416
epoch 1152, loss 1.04509
epoch 1280, loss 1.32215
epoch 1408, loss 1.17615
epoch 1536, loss 1.25571
epoch 1664, loss 1.1436
epoch 1792, loss 0.818821
epoch 1920, loss 1.49324
epoch 2048, loss 1.31969
epoch 2176, loss 1.05728
epoch 2304, loss 1.14879
epoch 2432, loss 1.03797
epoch 2560, loss 1.09705
epoch 2688, loss 1.10188
epoch 2816, loss 1.15238
epoch 2944, loss 0.84726
epoch 3072, loss 1.12947
epoch 3200, loss 1.34746
epoch 3328, loss 1.02838
epoch 3456, loss 1.14102
epoch 3584, loss 1.05252
epoch 3712, loss 1.02614
epoch 3840, loss 1.00107
epoch 3968, loss 0.917461
epoch 4096, loss 0.994889
epoch 4224, loss 0.951638
epoch 4352, loss 1.07328
epoch 4480, loss 0.877233
epoch 4608, loss 1.0502
epoch 4736, loss 0.917418
epoch 4864, loss 0.920729
epoch 4992, loss 0.964003
epoch 5120, loss 0.691508
epoch 5248, loss 1.03377
epoch 5376, loss 0.998011
epoch 5504, loss 1.19453
epoch 5632, loss 0.739579
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0111018 0.06639
0.0871096 0.0751072
0.0691068 0.0569222
0.0593346 0.0365545
0.0496681 0.0452077
0.0295687 0.0357125
0.0436381 0.0459319
0.0109781 0.0496407
0.0496681 0.0615863
-0.00236368 0.0719341
0.0322983 0.0591089
0.0290868 0.0636804
0.0492474 0.0377341
0.0977799 0.0351977
-3.66166e-06 0.0229405
0.0964307 0.0709536
0.0511643 0.0282847
-6.66928e-06 0.0256845
0.0509553 0.0337552
0.101441 0.0550867
0.0710838 0.0633396
0.0621302 0.0422446
-0.0234626 0.0438947
0.0927398 0.029338
0.0532578 0.0329733
0.0174335 0.0616051
0.0329021 0.0419677
0.0174339 0.0576245
0.100097 0.0422802
-0.0529722 0.0505424
-0.0177194 0.0341017
0.0251234 0.0392241
0.0321586 0.0400621
0.076422 0.036314
-0.0385437 0.0467371
0.0964772 0.0561376
-0.0113401 0.0443666
0.0278229 0.0619134
0.0497674 0.0525422
0.0414001 0.0358383
-0.0776845 0.0660926
-0.0271157 0.0304112
0.0330948 0.062395
0.0278228 0.0729125
0.0224432 0.0436256
0.0727411 0.050085
-0.0707068 0.0247201
0.0562649 0.0434727
-0.0716787 0.0417319
0.104819 0.0382021
0.0337179 0.0329805
2.5668e-06 0.0421692
0.0322982 0.0702689
0.0177241 0.0487976
0.0373484 0.050382
-2.23337e-05 0.0216658
-0.0300728 0.0245513
-0.10012 0.034245
-0.0468045 0.034121
0.000834798 0.0317535
0.062595 0.0559803
-0.00728242 0.0425414
0.0318693 0.0616536
0.109566 0.0447046
0.0305923 0.042871
0.0375434 0.0272648
0.0592247 0.0530321
-0.0161473 0.0506946
-0.0131858 0.0320876
-0.0106908 0.0386353
0.0199307 0.0476612
0.0113515 0.0414112
0.021525 0.0511893
0.0201879 0.0575849
0.0707072 0.0367219
0.0462066 0.0512778
0.0621334 0.0460567
0.0370769 0.0627318
0.0390986 0.0296301
0.022215 0.0545184
0.0168394 0.0519106
0.0319945 0.0365149
0.0599612 0.0586433
-0.0417573 0.0378169
0.0734439 0.0302938
-0.0177216 0.051103
0.0562649 0.0476547
-0.0247813 0.033347
0.0594349 0.0507071
0.0271115 0.0250376
0.0477959 0.0480516
0.0122142 0.0460051
0.0710838 0.0412929
0.0271115 0.0298402
-0.0734426 0.0560541
0.101441 0.0530987
0.126222 0.0408857
0.0211185 0.0557506
0.0197008 0.0441303
0.0631163 0.0412811
-0.0118767 0.0408581
0.0390986 0.0289896
0.0497672 0.0619395
0.0204028 0.0286427
0.0734366 0.0344011
0.0305919 0.0640507
0.126206 0.0423309
0.0329331 0.0319435
-0.0189864 0.0439827
-0.0532547 0.0256339
-0.0234699 0.0368711
0.0458116 0.0439101
0.059181 0.0305596
0.0661223 0.0664432
0.0247806 0.0442359
0.0593665 0.0364338
0.0362237 0.0223054
0.0191272 0.0577066
0.0907408 0.0286579
0.0691985 0.0567036
0.102034 0.0563134
0.085161 0.0774688
0.0375382 0.0382654
0.0321467 0.0540168
-0.0417624 0.0382217
0.0462156 0.0403366
0.0937752 0.0387868
0.0964305 0.0477795
parameters: [ 9.     0.421  4.382  0.984  5.744]. error: 345944473.856.
----------------------------
epoch 0, loss 1.17854
epoch 128, loss 1.00812
epoch 256, loss 1.26799
epoch 384, loss 1.19921
epoch 512, loss 1.13649
epoch 640, loss 1.50155
epoch 768, loss 0.971913
epoch 896, loss 1.20553
epoch 1024, loss 1.15578
epoch 1152, loss 1.05711
epoch 1280, loss 1.13494
epoch 1408, loss 1.15994
epoch 1536, loss 1.09974
epoch 1664, loss 1.39524
epoch 1792, loss 1.14699
epoch 1920, loss 1.15478
epoch 2048, loss 0.908808
epoch 2176, loss 1.20073
epoch 2304, loss 0.994437
epoch 2432, loss 1.0067
epoch 2560, loss 1.0311
epoch 2688, loss 1.21407
epoch 2816, loss 0.973444
epoch 2944, loss 0.931113
epoch 3072, loss 1.12296
epoch 3200, loss 0.927941
epoch 3328, loss 1.21667
epoch 3456, loss 0.962065
epoch 3584, loss 1.00513
epoch 3712, loss 1.03634
epoch 3840, loss 0.660288
epoch 3968, loss 0.726168
epoch 4096, loss 1.1617
epoch 4224, loss 1.01536
epoch 4352, loss 1.34925
epoch 4480, loss 1.43012
epoch 4608, loss 1.01619
epoch 4736, loss 0.789827
epoch 4864, loss 0.854273
epoch 4992, loss 0.884869
epoch 5120, loss 1.04167
epoch 5248, loss 0.952051
epoch 5376, loss 0.892345
epoch 5504, loss 1.21028
epoch 5632, loss 0.912912
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0414033 0.0238913
0.032902 0.0341049
0.0966587 0.0226168
0.0414025 0.02369
-0.080798 0.0100934
0.0594397 0.0321549
-0.0495799 0.0182712
0.0631163 0.00904305
-2.15513e-06 0.00539399
-0.080798 0.0148197
6.7959e-07 0.0229401
0.0626002 0.0234157
0.081527 0.0369149
0.0247038 0.0258846
0.0192074 0.0288484
-0.0271153 0.0137664
0.0117782 0.0080695
0.0174335 0.0293419
0.0589764 0.0253889
0.0496683 0.0279774
0.109566 0.034188
0.0414075 0.0236903
-0.039089 0.00186051
-0.016115 0.0354531
0.0384776 0.0230728
0.046209 0.00772679
-0.0385427 0.0101038
0.032158 0.0304942
0.0322982 0.0248715
-0.0495734 0.0186202
0.0594452 0.0268281
-0.00628321 0.0417435
0.0867333 0.0341084
0.0177241 0.0181826
0.0321586 0.0267376
0.132184 0.0228577
0.0327313 0.0227532
0.126086 0.0239644
0.0667826 0.0394996
0.109566 0.0276527
0.0375434 0.0331612
0.0281392 0.0321246
0.0281393 0.0234185
0.0594399 0.0307214
0.0385437 0.030052
0.0716711 0.0187524
0.0964307 0.0222879
0.0710835 0.0280425
-0.0394792 0.0166341
0.0920961 0.0195598
0.0819546 0.0284719
0.0594397 0.0315463
0.0209836 0.035733
-0.0417573 0.0189964
0.080392 0.0307413
0.0857789 0.0220694
-0.053257 0.0206461
0.0182076 0.0317431
0.0752321 0.0193877
0.000835111 0.0137937
0.0477958 0.0345681
0.0144707 0.0270297
0.101814 0.0331926
0.0937752 0.0332235
0.0188609 0.0179567
-0.0131851 0.012597
0.0594297 0.0299752
0.0859465 0.0277838
0.0318695 0.0288814
0.0594399 0.0251089
0.0983621 0.0294307
0.0424529 0.0220592
0.0329332 0.0296671
0.0820225 0.0427284
0.109566 0.0304078
0.023648 0.0298143
0.0783558 0.0337709
0.101441 0.035013
0.0416418 0.0243187
-0.0532547 0.0181697
0.0152616 0.0153183
0.0626002 0.0258479
0.0210915 0.0278446
0.0223972 0.0315395
0.080392 0.0307413
-0.027288 0.017995
-0.0188062 0.0127303
0.0268143 0.024373
0.0541227 0.0374668
-0.0204001 0.00775144
0.0594379 0.0241483
2.41162e-06 0.0142287
0.00142251 0.00888663
0.0390889 0.0139845
0.0997462 0.0334444
2.41162e-06 0.00769191
0.0861488 0.0228053
0.0407515 0.0312179
0.0333261 0.017175
0.0710838 0.0280068
0.0625949 0.0326817
0.0387924 0.0431941
0.0329021 0.0171629
0.0593312 0.0147161
-0.0295746 0.014015
0.0997462 0.0329927
0.0594426 0.0203421
0.0207193 0.03237
0.036794 0.0342033
0.094504 0.023664
0.0194981 0.0136473
0.0497673 0.0287832
0.00596587 0.0329712
0.0861486 0.0332642
0.0594562 0.0234348
-0.00894125 0.0152427
0.0594398 0.0310323
0.0210958 0.018988
0.0532535 0.0272151
0.0624794 0.0262113
0.0417454 0.0202832
0.000794682 0.0114656
0.0281393 0.0321232
0.0385437 0.0182229
-0.0131848 0.0127056
0.0424215 0.0268729
0.0952315 0.0345914
0.0215366 0.0266373
parameters: [ 9.     0.421  4.382  0.666  5.744]. error: 2093500400.62.
----------------------------
epoch 0, loss 1.10792
epoch 128, loss 1.11059
epoch 256, loss 1.19877
epoch 384, loss 1.08674
epoch 512, loss 1.30688
epoch 640, loss 1.32659
epoch 768, loss 0.949239
epoch 896, loss 1.02684
epoch 1024, loss 1.04772
epoch 1152, loss 1.29742
epoch 1280, loss 0.681826
epoch 1408, loss 1.13257
epoch 1536, loss 1.00492
epoch 1664, loss 0.918128
epoch 1792, loss 1.3199
epoch 1920, loss 0.917717
epoch 2048, loss 0.902559
epoch 2176, loss 1.2603
epoch 2304, loss 0.87262
epoch 2432, loss 0.894841
epoch 2560, loss 1.0626
epoch 2688, loss 0.757731
epoch 2816, loss 1.0206
epoch 2944, loss 0.963278
epoch 3072, loss 1.17549
epoch 3200, loss 1.28952
epoch 3328, loss 1.03335
epoch 3456, loss 1.24402
epoch 3584, loss 1.03775
epoch 3712, loss 0.802875
epoch 3840, loss 0.882934
epoch 3968, loss 0.932912
epoch 4096, loss 0.894024
epoch 4224, loss 1.01815
epoch 4352, loss 1.25435
epoch 4480, loss 1.03024
epoch 4608, loss 1.11589
epoch 4736, loss 1.0597
epoch 4864, loss 1.13005
epoch 4992, loss 1.22431
epoch 5120, loss 1.39446
epoch 5248, loss 1.0371
epoch 5376, loss 1.12367
epoch 5504, loss 1.10005
epoch 5632, loss 1.11686
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0322983 0.0336705
0.0813321 0.0321643
-0.0248165 0.031607
-0.00700272 0.0354408
0.0813321 0.0308576
0.0897923 0.0336801
0.0593346 0.0293065
0.0867333 0.0348663
8.39808e-07 0.0351332
0.0495801 0.0298826
0.032902 0.0315724
0.0414025 0.0295181
0.028139 0.0292543
0.0941706 0.0334404
3.5013e-07 0.0297645
-0.0716787 0.0320394
-0.0529698 0.0306618
0.0710837 0.0295499
0.109566 0.0362969
1.74905e-07 0.0344986
0.0594398 0.0288739
0.0904958 0.0299715
0.0208245 0.035466
1.74905e-07 0.0346199
0.0594297 0.0317658
0.0541227 0.028379
0.081797 0.0357259
0.0859467 0.031063
-0.0394796 0.0345003
-0.0394792 0.0323878
0.0131851 0.0297518
0.0594395 0.0290106
0.0272988 0.0299721
0.0673482 0.0335132
0.0815268 0.0339998
0.00863088 0.0336477
0.0978866 0.03325
0.032933 0.0305153
0.129453 0.0347077
0.0261391 0.0278941
0.0201879 0.0299476
-0.0271126 0.0343983
-0.0117788 0.0343019
-0.0152556 0.0310368
-0.0109566 0.0328788
0.0791961 0.0343663
0.0365698 0.0338038
0.0318695 0.0336529
0.056285 0.0336601
-0.00236347 0.0308623
0.0222152 0.0337351
-6.15975e-07 0.0301276
0.0589765 0.0299322
0.0822309 0.0307574
0.0144853 0.0340104
0.124311 0.0352072
0.0321468 0.0372441
0.0950802 0.0307396
0.0272894 0.0334215
0.0417702 0.0271948
0.0317696 0.0318684
0.0594273 0.0313618
0.000835111 0.0291065
0.0337178 0.0238786
0.0594452 0.0322889
0.0594399 0.0306643
0.0562851 0.0323202
0.0594398 0.0287634
0.0716773 0.0303905
0.0301505 0.0326829
0.0611263 0.0301219
0.022444 0.0323524
0.0144707 0.0310193
0.0326439 0.0310836
0.000786701 0.0281767
0.0594398 0.0339877
-0.00737805 0.0317135
4.57767e-08 0.0322085
0.00250079 0.0326322
0.0752422 0.0327654
0.0295687 0.0317844
-0.0611177 0.0302232
-0.0207292 0.033819
0.0236482 0.0299478
-0.00726943 0.0328191
3.5013e-07 0.0333394
0.0861488 0.0340095
0.0234681 0.0309712
0.0290868 0.0331773
0.0394799 0.0298819
-0.0110929 0.0321707
0.0901948 0.0361943
0.0952315 0.0278839
0.0188016 0.0344204
0.0152545 0.0325515
0.101441 0.0292926
0.0871097 0.0311978
0.0122194 0.033233
0.0174339 0.0291531
0.118684 0.0345567
0.0716773 0.0278975
0.00863032 0.0340143
0.0300757 0.0292425
0.0220538 0.0331032
0.0412752 0.0318887
-0.0390898 0.0312925
0.0281391 0.0298317
-0.0807974 0.0339992
0.0122243 0.0284698
-0.0414033 0.0334248
-0.0177194 0.0334826
0.0859467 0.0295498
0.0117782 0.035042
-0.00236348 0.0312994
0.027823 0.0338524
-0.00628221 0.0343491
0.0724734 0.031104
0.000794682 0.0354124
0.0724734 0.0305429
0.0605996 0.0280321
0.0321468 0.0320425
-0.0144856 0.0303242
0.0937753 0.0310621
-0.0188599 0.0312352
0.130756 0.0297408
-0.0385424 0.0319468
0.104819 0.0306671
0.046209 0.0322853
parameters: [ 9.     0.421  4.382  0.85   5.744]. error: 4.15255549372e+12.
----------------------------
epoch 0, loss 1.24093
epoch 128, loss 0.916423
epoch 256, loss 1.17905
epoch 384, loss 1.23844
epoch 512, loss 1.3027
epoch 640, loss 0.991398
epoch 768, loss 1.4798
epoch 896, loss 0.993081
epoch 1024, loss 1.22011
epoch 1152, loss 1.41772
epoch 1280, loss 1.19318
epoch 1408, loss 1.11709
epoch 1536, loss 1.39937
epoch 1664, loss 0.897118
epoch 1792, loss 1.00843
epoch 1920, loss 0.881803
epoch 2048, loss 0.978462
epoch 2176, loss 1.15017
epoch 2304, loss 1.09209
epoch 2432, loss 1.35919
epoch 2560, loss 0.960246
epoch 2688, loss 1.07489
epoch 2816, loss 0.903961
epoch 2944, loss 0.947595
epoch 3072, loss 0.844878
epoch 3200, loss 0.843853
epoch 3328, loss 0.882196
epoch 3456, loss 0.747602
epoch 3584, loss 0.956326
epoch 3712, loss 1.09468
epoch 3840, loss 0.930403
epoch 3968, loss 1.32101
epoch 4096, loss 0.774768
epoch 4224, loss 1.20701
epoch 4352, loss 1.1823
epoch 4480, loss 1.08438
epoch 4608, loss 0.790902
epoch 4736, loss 1.06263
epoch 4864, loss 0.835238
epoch 4992, loss 0.886743
epoch 5120, loss 1.03651
epoch 5248, loss 1.03086
epoch 5376, loss 0.753054
epoch 5504, loss 1.13098
epoch 5632, loss 0.802002
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0223968 0.0276547
0.0966596 0.0318211
0.000834798 0.0298504
-0.000793636 0.0353737
-0.0529682 0.0309171
-0.0659495 0.0241653
-3.91322e-05 0.0282584
0.0791961 0.0222605
-0.0468001 0.0311235
-0.039089 0.0273634
0.0867067 0.0201622
0.0594324 0.0256559
0.0337182 0.0486623
0.046799 0.036302
0.0317697 0.034331
-0.0659495 0.0272419
0.059435 0.0331413
0.0594562 0.0231493
0.0966596 0.032983
0.0594398 0.0415876
0.028139 0.0326755
0.0396776 0.0313801
0.0223972 0.0320889
0.106624 0.0358193
0.0318766 0.0314262
0.0199307 0.0295934
-0.00331175 0.0297852
-0.0860366 0.0213652
0.0243585 0.0301992
-0.041635 0.0279818
-0.0611267 0.026086
0.0122352 0.0322959
0.059435 0.0360813
-0.0208232 0.028666
0.0901953 0.0314263
0.132184 0.0405895
-0.0318748 0.0311549
0.060558 0.029318
0.0295774 0.0324969
0.0857794 0.0360074
0.0859467 0.0299702
0.090741 0.0299995
-0.0413988 0.0362683
0.0462066 0.020181
0.0487559 0.0311716
0.0648671 0.038951
0.0177241 0.0344568
0.0859465 0.044228
0.0594376 0.0317555
0.0582651 0.0321861
0.042448 0.0294446
0.0271289 0.0233331
0.0861488 0.0330859
0.0710835 0.0320329
0.0589764 0.0366156
0.0317696 0.0299808
0.0625951 0.0241043
0.0594398 0.0445109
-0.0188023 0.0283299
0.0117845 0.0247868
0.0594396 0.0242617
0.100116 0.0287032
-0.0194897 0.0318112
-0.0161572 0.0223798
0.0207304 0.0363353
0.0247806 0.0235423
0.143948 0.0289321
0.125146 0.0342532
0.01684 0.0295035
0.0871095 0.0367821
0.0594396 0.0331277
0.0662931 0.0308841
-0.000792737 0.0238837
-0.00700262 0.0476083
-0.0734426 0.0315747
0.0907409 0.0350856
-3.76332e-06 0.0252192
0.0134225 0.0345329
-0.0234744 0.0386216
0.12945 0.0246744
-0.0295746 0.0292849
-0.0188606 0.0364292
0.0594397 0.0315199
0.015257 0.0317621
0.0904958 0.0311692
0.088282 0.0384115
0.0416418 0.0307585
0.0691984 0.0404936
0.126206 0.0261543
0.0822309 0.0357856
0.0188015 0.0399775
0.0997468 0.0248266
0.0396778 0.0326436
-0.0394792 0.0182697
0.0605996 0.0352813
0.0318787 0.0313672
0.129458 0.0329032
0.0271187 0.0284259
0.0897923 0.0286263
-0.00236357 0.0385903
0.0691985 0.0368697
0.0317698 0.0357872
-0.0385437 0.0292701
0.0927398 0.0362988
0.0385434 0.0307243
0.00932522 0.0297025
0.044975 0.0259457
0.032158 0.0286066
-0.0529722 0.0358485
-0.0144856 0.0259918
0.01684 0.0216659
-0.0188062 0.0325362
0.0582236 0.0330178
0.0648671 0.0344709
-1.48505e-07 0.0320462
-0.0417491 0.0344285
0.0596272 0.0387103
0.0462066 0.0300511
-4.71368e-07 0.0240672
0.0109586 0.0251993
0.0606162 0.0329037
0.0594397 0.0327766
0.0887289 0.0266778
0.0364958 0.0386847
-0.02083 0.0345283
0.0855721 0.0340148
-0.0385424 0.0281974
0.0659615 0.030926
parameters: [ 9.     0.421  4.382  0.729  5.744]. error: 77.0880470354.
----------------------------
epoch 0, loss 1.38326
epoch 128, loss 1.29243
epoch 256, loss 1.19722
epoch 384, loss 1.16101
epoch 512, loss 0.900361
epoch 640, loss 0.976442
epoch 768, loss 0.900832
epoch 896, loss 1.25754
epoch 1024, loss 1.01841
epoch 1152, loss 1.00625
epoch 1280, loss 0.968345
epoch 1408, loss 1.12912
epoch 1536, loss 1.38137
epoch 1664, loss 1.10876
epoch 1792, loss 1.00666
epoch 1920, loss 0.966465
epoch 2048, loss 0.952311
epoch 2176, loss 0.792097
epoch 2304, loss 1.39853
epoch 2432, loss 1.00619
epoch 2560, loss 1.1261
epoch 2688, loss 0.858777
epoch 2816, loss 0.869897
epoch 2944, loss 1.18029
epoch 3072, loss 1.00443
epoch 3200, loss 0.933925
epoch 3328, loss 1.04656
epoch 3456, loss 1.29594
epoch 3584, loss 1.31532
epoch 3712, loss 0.899603
epoch 3840, loss 0.874587
epoch 3968, loss 0.978341
epoch 4096, loss 1.03683
epoch 4224, loss 0.972544
epoch 4352, loss 0.846043
epoch 4480, loss 0.945004
epoch 4608, loss 1.10979
epoch 4736, loss 1.18355
epoch 4864, loss 0.927517
epoch 4992, loss 1.03675
epoch 5120, loss 0.63912
epoch 5248, loss 0.946324
epoch 5376, loss 0.815084
epoch 5504, loss 0.897527
epoch 5632, loss 0.936929
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0920961 0.03681
-0.0267398 0.039475
-3.40966e-05 0.0374301
-0.0110929 0.0425625
0.0776855 0.021614
0.0191272 0.0392063
0.0482638 0.0422345
-0.0707068 0.0301933
0.0417489 0.039126
0.0295774 0.0307265
0.0237989 0.0288722
0.0467944 0.0340845
0.000213351 0.0416231
-0.0271157 0.0325145
-0.0468045 0.0355119
0.0562849 0.0417985
0.0113515 0.0300786
0.0122452 0.0410224
-0.0209824 0.0288015
0.0464005 0.0338014
0.022215 0.0361369
0.0791961 0.0441905
0.0861486 0.0379666
0.028139 0.038309
-0.0152533 0.0237585
-0.0209708 0.0327332
0.0144846 0.0299316
-0.10012 0.0298554
0.0752422 0.0322163
0.0197009 0.0395868
0.0283783 0.0418187
-0.032317 0.0246137
0.0753057 0.031274
0.101814 0.0367151
0.0188015 0.0412919
-0.0417624 0.0181724
0.0390986 0.0313142
0.0589763 0.0349972
0.018802 0.0298281
0.0243594 0.0349285
0.0234681 0.0304218
0.0330949 0.0401272
0.0599031 0.037887
0.025123 0.0366588
0.0594396 0.0378354
-0.00894125 0.0336635
-0.0662843 0.0294319
0.000796249 0.0284349
0.0188603 0.0325207
0.0599029 0.0390214
0.088282 0.0380535
0.0594398 0.041155
0.0594395 0.0373255
0.0414051 0.0367282
-0.0621343 0.0322117
0.0529686 0.0343788
0.0594399 0.0366526
0.0867334 0.0366973
0.0109857 0.0168395
0.0734322 0.035914
-0.0326435 0.0366337
-0.0267352 0.0298658
0.0177241 0.0338264
0.0950803 0.0370475
-0.041769 0.0318527
0.0706155 0.0413296
0.0131851 0.0317816
-0.0857218 0.0375383
0.0211031 0.0328328
0.0182077 0.046288
0.0871095 0.0374199
-0.0250779 0.0411594
-0.0707068 0.0356997
0.0140549 0.0439067
0.0177241 0.0231222
0.0783558 0.0406744
0.0131849 0.033359
0.0467944 0.0270522
0.0417484 0.0337205
0.0267779 0.0370575
0.0589133 0.0406205
0.0290868 0.0360271
-0.0131854 0.0208291
-0.00856301 0.0388317
-0.0188593 0.0385787
0.0631163 0.0237601
0.0278229 0.0410442
0.000807032 0.0265184
-0.0234677 0.0318105
0.0594396 0.039483
0.0327313 0.0420655
0.0330953 0.0403501
4.07609e-05 0.0298675
-0.0467928 0.0272456
0.0661224 0.0366525
0.0538009 0.041807
8.40829e-06 0.0385837
0.0424581 0.0329586
0.0131848 0.0403046
0.01684 0.0382179
0.018802 0.0390254
-0.0857218 0.0248067
0.0707072 0.024125
0.0414001 0.0328289
-0.0295746 0.0353176
-0.0860431 0.0339494
0.0966587 0.0404232
-0.00856301 0.034235
0.0867333 0.0302967
0.0594396 0.0411556
-0.0271126 0.033315
0.0621334 0.0389848
0.0329021 0.0405181
-0.0210947 0.0271387
0.0321586 0.0409009
0.0589127 0.0403532
0.0208321 0.0374125
0.05937 0.0394542
0.014484 0.0400341
0.0424215 0.0435377
0.09274 0.0454052
0.00931426 0.0347838
0.0482638 0.0403082
-0.0132904 0.0404847
0.0800821 0.046274
0.0199309 0.0399701
-0.000835392 0.0371158
0.0205116 0.0420356
parameters: [ 9.     0.421  4.382  0.748  5.744]. error: 14263904.6614.
----------------------------
epoch 0, loss 1.09864
epoch 128, loss 0.915121
epoch 256, loss 1.01491
epoch 384, loss 1.53514
epoch 512, loss 0.918569
epoch 640, loss 1.01815
epoch 768, loss 1.11994
epoch 896, loss 1.42087
epoch 1024, loss 0.953577
epoch 1152, loss 1.34286
epoch 1280, loss 1.04667
epoch 1408, loss 1.4774
epoch 1536, loss 0.901095
epoch 1664, loss 0.951098
epoch 1792, loss 1.01913
epoch 1920, loss 1.21548
epoch 2048, loss 1.00627
epoch 2176, loss 1.24315
epoch 2304, loss 0.969499
epoch 2432, loss 1.25767
epoch 2560, loss 1.29604
epoch 2688, loss 0.87939
epoch 2816, loss 0.793766
epoch 2944, loss 1.29685
epoch 3072, loss 0.803743
epoch 3200, loss 0.80621
epoch 3328, loss 1.33688
epoch 3456, loss 0.882402
epoch 3584, loss 0.918195
epoch 3712, loss 1.14706
epoch 3840, loss 1.29843
epoch 3968, loss 1.14712
epoch 4096, loss 0.794596
epoch 4224, loss 0.977071
epoch 4352, loss 0.948909
epoch 4480, loss 0.638928
epoch 4608, loss 0.693042
epoch 4736, loss 1.04893
epoch 4864, loss 0.951166
epoch 4992, loss 1.07019
epoch 5120, loss 0.897641
epoch 5248, loss 0.778084
epoch 5376, loss 0.940851
epoch 5504, loss 0.847739
epoch 5632, loss 0.975711
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0861488 0.0561976
0.0317698 0.0472059
0.0594376 0.0368079
0.0199306 0.0303777
-0.0734361 0.0402876
0.0595095 0.0324553
0.0662934 0.0262308
0.081797 0.0344282
0.0251233 0.0340041
-0.0857218 0.0299558
-3.11621e-07 0.0292628
0.0319944 0.041516
0.0950805 0.0459984
0.0691987 0.0474131
0.0857183 0.0296953
0.101441 0.0372846
0.0859467 0.0629506
0.0764118 0.0545797
0.0589133 0.0530434
0.0417484 0.0253356
0.0532657 0.0379175
-3.66166e-06 0.0327578
0.0394799 0.0243226
0.0859466 0.050511
0.0237988 0.0520254
-0.0367964 0.0332442
-0.0177194 0.0459568
0.0927398 0.0585846
0.0477959 0.0585697
0.000835111 0.0333649
0.0329332 0.0558613
5.31452e-07 0.0442396
0.0037846 0.0338212
0.01684 0.0346401
0.00931426 0.0375139
0.0968255 0.0429773
0.0807977 0.0441861
0.10955 0.0475351
0.0707072 0.0360519
-0.000786289 0.0380045
0.112932 0.0276188
0.036223 0.0271933
0.0414025 0.0383717
0.0122404 0.0438928
0.130756 0.0579788
0.0594325 0.0437062
0.0662931 0.0261329
0.0122404 0.032496
0.125157 0.0596774
-0.0215356 0.0345799
0.0594376 0.0292027
0.025123 0.0460748
0.0201879 0.0481969
0.0201879 0.0477578
0.0201879 0.0382939
-2.99285e-06 0.0263373
0.0318787 0.0361761
-0.014471 0.0370789
-0.00731719 0.0306864
0.0920961 0.0451092
0.0224432 0.0522957
0.0396776 0.0416654
1.95996e-07 0.0260358
0.0449639 0.0366925
0.0594402 0.0310462
0.0527576 0.0577437
-0.0385421 0.0275517
-0.0318748 0.040211
0.0375434 0.0279917
-0.065961 0.0338879
0.0594204 0.0354058
-0.0385421 0.0307298
0.033718 0.06669
-0.00700262 0.0499581
0.0716773 0.0365005
0.0594397 0.0518613
0.0594399 0.0581618
0.0594375 0.0339083
0.0477956 0.0490606
0.0626002 0.0293799
0.0857789 0.0353225
0.0468064 0.0341883
0.0562649 0.0319412
0.094504 0.0632138
0.0594398 0.0571068
0.0367961 0.0212118
0.0991789 0.048462
0.0920613 0.0493463
-0.0271121 0.0241463
-0.0529682 0.0396137
0.110244 0.0582237
0.0140549 0.0446084
0.00080867 0.0227866
-0.0295823 0.0436013
0.088282 0.0397296
-0.00700262 0.0533239
0.0989678 0.0301195
0.0497673 0.050132
-1.17339e-07 0.0297295
0.109566 0.0450544
0.0911183 0.0573375
0.0691985 0.0487574
0.059435 0.0317976
0.062595 0.0609769
-0.0210896 0.0378245
-0.000786289 0.0338488
0.0594398 0.0606156
0.0594398 0.0352453
0.0318787 0.0366092
-0.0734384 0.0294953
0.028139 0.0592683
0.0152591 0.0346979
0.118684 0.0406572
4.17498e-06 0.0254128
0.0496683 0.0378528
0.129916 0.0336046
0.0904958 0.0374112
-0.0707068 0.0315685
0.0375381 0.0428451
0.0236481 0.0624631
-0.00894944 0.0274668
0.0267443 0.0448137
0.0594466 0.0414019
0.0210212 0.0366163
-0.0131854 0.027701
0.0281391 0.0601866
0.0209836 0.0496471
0.0327315 0.056123
parameters: [ 9.     0.421  4.382  0.705  5.744]. error: 64314706263.7.
----------------------------
epoch 0, loss 1.1905
epoch 128, loss 1.30481
epoch 256, loss 1.10305
epoch 384, loss 1.20161
epoch 512, loss 1.31946
epoch 640, loss 1.43659
epoch 768, loss 1.2928
epoch 896, loss 1.25895
epoch 1024, loss 1.24334
epoch 1152, loss 1.35872
epoch 1280, loss 1.43768
epoch 1408, loss 1.2371
epoch 1536, loss 1.44182
epoch 1664, loss 1.38224
epoch 1792, loss 1.06842
epoch 1920, loss 1.0886
epoch 2048, loss 1.23808
epoch 2176, loss 0.818291
epoch 2304, loss 1.08374
epoch 2432, loss 1.42162
epoch 2560, loss 1.11825
epoch 2688, loss 1.15756
epoch 2816, loss 1.08967
epoch 2944, loss 1.04789
epoch 3072, loss 1.25212
epoch 3200, loss 1.22246
epoch 3328, loss 1.01428
epoch 3456, loss 1.3504
epoch 3584, loss 0.969366
epoch 3712, loss 1.03443
epoch 3840, loss 1.27128
epoch 3968, loss 1.13056
epoch 4096, loss 1.41807
epoch 4224, loss 0.936467
epoch 4352, loss 1.10479
epoch 4480, loss 1.02561
epoch 4608, loss 1.49308
epoch 4736, loss 1.36629
epoch 4864, loss 1.04318
epoch 4992, loss 1.04009
epoch 5120, loss 0.936041
epoch 5248, loss 0.99952
epoch 5376, loss 1.30798
epoch 5504, loss 0.837446
epoch 5632, loss 1.12982
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0716682 0.0228672
0.0724741 0.0255072
-0.053266 0.0266435
0.0871097 0.0171996
0.0237986 0.0236024
0.0867333 0.0319446
-0.0131858 0.0201094
0.0237986 0.0325575
-0.0412711 0.0221144
-3.66166e-06 0.0216579
0.000834798 0.0221738
-0.0106876 0.02126
0.0764118 0.0303673
0.0867335 0.0319344
0.00460075 0.024992
0.0867333 0.0274489
0.0594027 0.0268014
-0.0467978 0.0260202
-0.10012 0.0244787
0.0716719 0.024636
2.05834e-07 0.0226992
0.0989677 0.0314907
0.00460075 0.0229355
0.0532657 0.0278164
0.0424316 0.0260234
-3.66166e-06 0.0223883
0.0800819 0.0212476
-0.0529682 0.024716
0.0461382 0.0253132
0.0701141 0.0270818
-0.0860447 0.0203782
0.0950804 0.0229762
0.0529729 0.0195722
0.0997468 0.0255618
0.0706155 0.0167748
-0.0295746 0.0289937
-0.0318771 0.0170169
0.00931417 0.0325511
0.0594301 0.0235081
0.0752955 0.0231179
0.0317698 0.0253184
0.0412702 0.0278714
0.0271289 0.0202051
-0.0248182 0.0251187
0.0037846 0.022282
0.036223 0.0176678
0.0424581 0.0299112
0.0907409 0.0319838
0.0261391 0.0335371
0.00250079 0.021269
0.0236479 0.0319965
0.0208321 0.022231
-0.0113495 0.0195288
0.0594398 0.0285017
0.0367961 0.0262944
0.121243 0.0331932
-0.0346424 0.0192176
0.0496681 0.0203693
-0.041635 0.0206869
0.0857183 0.0201015
0.0261395 0.0281509
-0.0495799 0.0225289
-0.00733491 0.0249833
-0.00378521 0.0253006
0.0482639 0.0160685
0.0870053 0.0180975
0.100667 0.0340372
0.0387924 0.0261842
0.0983621 0.0253371
0.0964772 0.0269454
0.0317699 0.0245657
0.0589133 0.0178543
-0.0323135 0.0148062
-0.0131841 0.0202772
0.0861487 0.0287771
-0.0271111 0.0249477
0.0968256 0.0237993
-0.080798 0.0247311
0.0174339 0.0256446
0.0396777 0.0202502
0.0853255 0.0244022
-0.0215356 0.0268763
0.0188603 0.0253277
0.124311 0.0254475
0.0174339 0.0243381
0.0594204 0.0214031
-0.0340213 0.0205229
0.0317697 0.0226577
0.0375434 0.0299616
0.0321467 0.0313213
0.0113515 0.0252947
0.0237986 0.0289307
0.0661222 0.0265726
0.143948 0.0218103
0.0867119 0.0210067
0.0220537 0.0252443
0.0857183 0.0232648
-0.0267352 0.018002
4.57767e-08 0.0284252
-0.0295746 0.0237003
-0.000795203 0.0212002
0.0978866 0.0303003
0.0205126 0.032576
0.0390986 0.0187233
-0.0208232 0.014842
-3.66166e-06 0.0214871
0.0375434 0.0279993
0.0983621 0.0255722
0.0594494 0.0244687
0.0977801 0.0245081
0.0497673 0.021909
0.0321468 0.0274339
0.0662931 0.0210314
0.046209 0.0247349
0.0594398 0.0163962
0.0192074 0.0182166
0.0122142 0.0220523
-0.0152533 0.0230441
0.0407513 0.0229751
0.080382 0.0238338
0.0462066 0.0239932
0.059181 0.0245559
-0.0417488 0.0238061
0.0477958 0.0270058
0.086885 0.0349126
0.0866747 0.0230737
-0.00726943 0.0227819
-0.041769 0.0262133
parameters: [ 9.     0.421  4.382  0.72   5.744]. error: 3.03842077507e+12.
----------------------------
epoch 0, loss 0.939011
epoch 128, loss 1.22292
epoch 256, loss 0.940804
epoch 384, loss 0.835404
epoch 512, loss 1.23065
epoch 640, loss 1.16909
epoch 768, loss 1.30273
epoch 896, loss 1.16021
epoch 1024, loss 0.896064
epoch 1152, loss 1.16504
epoch 1280, loss 1.23634
epoch 1408, loss 0.898049
epoch 1536, loss 1.16621
epoch 1664, loss 1.00201
epoch 1792, loss 0.948356
epoch 1920, loss 1.03866
epoch 2048, loss 0.933659
epoch 2176, loss 1.21417
epoch 2304, loss 0.853227
epoch 2432, loss 1.02853
epoch 2560, loss 1.15417
epoch 2688, loss 0.897299
epoch 2816, loss 1.01241
epoch 2944, loss 1.1294
epoch 3072, loss 1.14806
epoch 3200, loss 0.920313
epoch 3328, loss 0.778147
epoch 3456, loss 1.29175
epoch 3584, loss 1.11065
epoch 3712, loss 0.723826
epoch 3840, loss 0.968938
epoch 3968, loss 0.809318
epoch 4096, loss 0.864897
epoch 4224, loss 1.01838
epoch 4352, loss 1.04654
epoch 4480, loss 0.948186
epoch 4608, loss 0.990878
epoch 4736, loss 0.884585
epoch 4864, loss 0.918514
epoch 4992, loss 0.690259
epoch 5120, loss 0.862155
epoch 5248, loss 0.821775
epoch 5376, loss 0.892528
epoch 5504, loss 1.17755
epoch 5632, loss 0.618137
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0248069 0.0123491
0.0538015 0.0449165
0.085161 0.0251592
0.0991789 0.0410254
0.09274 0.0301351
-0.0131858 0.0203841
0.0327313 0.0267828
0.0237989 0.0461006
0.0562849 0.0460183
0.0109857 0.0287065
0.0991788 0.0373986
0.00460075 0.00874362
0.028139 0.03101
-0.0716686 0.0200291
0.0497672 0.0384329
-0.0118767 0.0184965
0.0819546 0.0254283
0.0405236 0.0239441
-0.0462076 0.0145571
0.110244 0.019662
0.00894003 0.0133717
0.106634 0.0316223
0.0462156 0.0108385
0.112932 0.0348235
0.125146 0.0365937
0.0495796 0.0139625
0.0207193 0.0224518
0.0857183 0.0143413
0.0385441 0.00939336
0.0562851 0.0407407
0.0122352 0.0199103
0.088729 0.0421262
0.0199306 0.0313248
-0.0188023 0.0137176
0.0362233 0.0200218
0.10665 0.0268383
0.0996717 0.0385106
0.0621334 0.0122923
-0.000835063 0.0230011
0.0594349 0.0316317
0.106629 0.0233627
-0.00131975 0.0137014
-0.0271157 0.00893051
-0.0707068 0.0190878
0.081797 0.0210912
0.0319944 0.0365797
0.110244 0.0330256
0.024704 0.0357096
0.0247829 0.0165906
0.101441 0.0272564
0.100116 0.014462
0.0335492 0.034699
-0.0611267 0.00898141
-0.0417425 0.0221111
0.000835111 0.0174104
0.0599029 0.0453632
0.0594398 0.0463676
0.000213351 0.0310757
0.0384777 0.0316165
-0.0117822 0.0197534
0.125883 0.0248829
0.0168394 0.0420696
1.95996e-07 0.0167681
0.0594396 0.026824
0.0330948 0.0342516
0.0599612 0.0406596
0.0752422 0.0271984
-0.0209708 0.0140783
0.0197009 0.0311203
-0.000835048 0.012581
0.0497671 0.0299162
0.0626106 0.0311687
0.0860523 0.0237083
0.062595 0.0347094
0.0208245 0.0200737
0.0217931 0.0238795
-0.0207342 0.0159481
-0.0394792 0.01872
0.0855721 0.0307983
-0.0807974 0.0165253
-0.0118765 0.0257208
-0.0267398 0.0169809
0.020735 0.0118422
-0.000835392 0.0129731
-0.0467978 0.0184307
0.10546 0.0390022
0.0901952 0.0365883
0.0968257 0.0352109
0.059435 0.0399166
0.0131861 0.0138123
0.0508582 0.0234094
0.0375434 0.0365601
0.0417448 0.0170811
-0.0300731 0.0114446
0.0211182 0.0406753
0.0387924 0.0407235
-0.02083 0.0211369
0.0594397 0.0250168
-0.000795203 0.0158136
0.076417 0.0294953
0.0435639 0.0397118
0.0168394 0.0250785
0.076417 0.032523
0.0865811 0.0342137
0.0330949 0.043423
-0.0494977 0.0201735
0.0317698 0.0249239
0.0594397 0.0302081
-0.0662833 0.00768449
0.0346447 0.0110282
-0.0131845 0.0110158
0.0964305 0.0220974
0.0435741 0.0320375
0.0626054 0.035889
-0.0132904 0.0457514
0.0952317 0.0447987
0.0496683 0.0346246
0.0867067 0.0374125
0.0626054 0.0253595
0.0582389 0.0303431
-0.0152533 0.0161832
0.0281391 0.0430658
0.0978866 0.0431691
0.0384676 0.0229197
0.0594396 0.0262463
0.0691986 0.0318521
-0.00893962 0.010414
0.101441 0.0266257
parameters: [ 9.     0.421  4.382  0.736  5.744]. error: 21408684971.9.
----------------------------
epoch 0, loss 1.08845
epoch 128, loss 1.16096
epoch 256, loss 1.12374
epoch 384, loss 1.19257
epoch 512, loss 0.977324
epoch 640, loss 1.20025
epoch 768, loss 0.962303
epoch 896, loss 0.912018
epoch 1024, loss 0.951367
epoch 1152, loss 1.2006
epoch 1280, loss 1.36963
epoch 1408, loss 1.18534
epoch 1536, loss 0.910288
epoch 1664, loss 1.16071
epoch 1792, loss 1.20869
epoch 1920, loss 0.933478
epoch 2048, loss 0.940175
epoch 2176, loss 1.26374
epoch 2304, loss 1.16016
epoch 2432, loss 0.801238
epoch 2560, loss 1.12847
epoch 2688, loss 0.998962
epoch 2816, loss 0.949519
epoch 2944, loss 1.04373
epoch 3072, loss 1.31843
epoch 3200, loss 0.942987
epoch 3328, loss 1.03133
epoch 3456, loss 0.839281
epoch 3584, loss 1.09906
epoch 3712, loss 1.05304
epoch 3840, loss 1.15317
epoch 3968, loss 1.24054
epoch 4096, loss 1.07386
epoch 4224, loss 0.824897
epoch 4352, loss 0.838355
epoch 4480, loss 1.22384
epoch 4608, loss 1.13913
epoch 4736, loss 0.904035
epoch 4864, loss 1.05469
epoch 4992, loss 1.01292
epoch 5120, loss 0.720868
epoch 5248, loss 1.08208
epoch 5376, loss 1.0887
epoch 5504, loss 1.01659
epoch 5632, loss 0.957033
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0662877 0.0219992
-0.0152583 0.038286
0.0594398 0.030317
0.0394802 0.0305678
-0.129449 0.0256787
0.0496682 0.0288863
0.0855722 0.0235486
-0.0362226 0.0274181
-0.0247793 0.0200811
0.0594397 0.0366388
0.0394799 0.0264497
-6.15975e-07 0.0227145
0.0329021 0.0450184
-0.0462049 0.0157602
0.0207377 0.0105346
0.0907408 0.0289347
0.00863088 0.027292
0.0859467 0.033266
-0.0211014 0.0290558
-0.0234699 0.0204787
0.0625951 0.0388623
0.09274 0.0269659
0.090741 0.0365376
0.101441 0.0297767
0.105471 0.045394
0.0188023 0.0146089
0.0593133 0.0413604
0.0435639 0.0368334
-0.0295676 0.0330888
0.0333261 0.0233266
0.100097 0.0461015
-0.0204028 0.017752
0.0599612 0.0288282
0.118684 0.0357362
0.0333262 0.0371641
0.110244 0.0398066
0.0691987 0.0287848
0.0122504 0.037242
0.0968257 0.035769
0.0594397 0.0270859
-0.0467978 0.0172173
-0.0807993 0.0207694
0.0532578 0.027812
0.0997462 0.038254
-0.0462169 0.015895
-0.0529659 0.0225013
-0.0271126 0.0176929
-0.0529698 0.0300756
0.0964772 0.0324478
0.0781227 0.0287925
0.0964307 0.0315489
3.5013e-07 0.0336182
0.0527578 0.0304718
0.0527578 0.0271091
0.0594395 0.0204808
-0.0295746 0.0297918
0.0495763 0.030576
0.0611197 0.025528
0.0997462 0.0302835
0.0594452 0.0497064
0.0901949 0.0296517
0.0813268 0.0501778
0.0981549 0.0376385
0.0997468 0.0434261
0.032933 0.0363433
0.0152616 0.0113102
0.0497671 0.0352083
0.0662803 0.0164722
0.0140549 0.0395455
0.130756 0.0368171
0.0822309 0.0380537
-0.0416468 0.0166464
0.0168394 0.0347097
0.0335492 0.0263555
0.0859466 0.030224
-0.000793636 0.0358151
-0.0131858 0.0230017
0.0706155 0.0303658
0.0625951 0.0240913
0.0861486 0.0365252
0.00863092 0.0289548
0.0650734 0.0202361
0.0482639 0.0329476
0.0781223 0.0211747
0.100097 0.0480755
-0.0412685 0.0211561
0.0594396 0.0381279
0.085161 0.0242873
0.0317699 0.0182467
-0.0111821 0.0314672
0.0691985 0.0327461
-3.66166e-06 0.016751
0.0621339 0.0174082
-0.0023637 0.0196659
0.0384776 0.0376852
0.0594466 0.040798
0.0867334 0.0331877
-0.0611177 0.0184033
0.0281393 0.0300473
0.0152591 0.0188268
0.088282 0.0398865
0.0204028 0.0180892
0.094504 0.0374589
0.0414051 0.024497
0.0673482 0.0457409
-0.0631101 0.0253814
-0.0529698 0.0274928
0.076438 0.0439836
0.0730675 0.034304
-0.0023636 0.028189
-1.48505e-07 0.0368082
0.0691986 0.0287697
0.105506 0.0456436
0.0910567 0.0192826
0.0321466 0.0348998
0.0271115 0.0298278
0.0373475 0.0352981
0.0321897 0.0390947
0.0329332 0.0348505
-0.0210969 0.0218825
0.10664 0.0330625
-0.0144856 0.0227692
-0.0271278 0.0296415
0.0223972 0.0298827
0.134995 0.0431942
0.0650733 0.0203482
0.0710837 0.0268493
0.0691985 0.0296662
parameters: [ 9.     0.421  4.382  0.729  5.744]. error: 28566863345.2.
----------------------------
epoch 0, loss 1.22107
epoch 128, loss 1.12046
epoch 256, loss 1.12284
epoch 384, loss 1.02041
epoch 512, loss 0.983368
epoch 640, loss 0.788868
epoch 768, loss 1.02395
epoch 896, loss 0.780497
epoch 1024, loss 0.793235
epoch 1152, loss 1.02016
epoch 1280, loss 0.953041
epoch 1408, loss 0.939093
epoch 1536, loss 0.940194
epoch 1664, loss 0.893119
epoch 1792, loss 0.899238
epoch 1920, loss 1.16896
epoch 2048, loss 0.894008
epoch 2176, loss 0.881318
epoch 2304, loss 0.709421
epoch 2432, loss 0.76327
epoch 2560, loss 0.86956
epoch 2688, loss 0.868612
epoch 2816, loss 0.999712
epoch 2944, loss 0.69781
epoch 3072, loss 0.775708
epoch 3200, loss 0.992108
epoch 3328, loss 0.848535
epoch 3456, loss 0.632144
epoch 3584, loss 0.741691
epoch 3712, loss 0.88566
epoch 3840, loss 0.851965
epoch 3968, loss 0.863199
epoch 4096, loss 0.896356
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0335492 0.0310057
0.0853255 0.0397158
0.0321468 0.0368178
-0.000801651 0.0139359
-7.99663e-06 0.0208286
0.0716773 0.00813357
0.0710835 0.0584879
-0.0131848 0.0241165
0.0390889 0.014529
0.0295843 0.0101971
0.0412725 0.00731438
0.0964772 0.0459595
0.0952318 0.045773
3.5013e-07 0.0159937
0.0691984 0.0448226
0.0594452 0.0499066
0.0968257 0.0536128
-1.48505e-07 0.0191686
-0.0118767 0.0550321
0.0416484 0.0166317
0.0333262 0.0464629
-0.0412757 0.0137588
-0.041635 0.0197244
0.0964307 0.0453771
0.0857284 0.0180007
-0.065961 0.0195929
0.00863029 0.0338339
-0.0188605 0.00586855
0.0781227 0.0415173
0.0268143 0.0488674
0.0861488 0.049904
-0.0346352 0.0148754
-0.0449599 0.0225808
0.0691072 0.0382656
0.0224432 0.0401034
-0.0529722 0.018992
0.0529709 0.0142794
0.0248197 0.0165497
0.0174335 0.0382957
0.0920961 0.0475961
0.0210209 0.0445538
-0.0394792 0.01955
0.059435 0.0501479
0.0414025 0.0113043
0.101441 0.0377202
0.0594482 0.0499099
-0.00726943 0.0419443
0.0346452 0.0164451
0.0271185 0.00754117
0.0346348 0.00992189
0.0405236 0.0397389
-0.014471 0.00502465
0.0851609 0.048981
0.0730676 0.0419201
0.0734322 0.0195689
0.105471 0.0472116
-0.0109774 0.00904272
0.118684 0.0500168
0.0964307 0.0433739
0.0322983 0.0456743
0.0396776 0.0423708
0.0981543 0.0515269
0.0414051 0.0119397
0.0390991 0.0101052
-0.0248092 0.0152649
0.0326469 0.00898372
0.0594398 0.0563312
0.0330949 0.0412726
0.0594349 0.0426832
0.041765 0.0224461
0.0189892 0.0194017
-7.99663e-06 0.0151986
0.0268195 0.0472157
0.0819546 0.0444126
-0.0272995 0.0119972
0.023648 0.0509799
-0.0144706 0.0082668
0.0594398 0.0563312
0.0878297 0.0483403
-0.0194897 0.00665241
0.0593105 0.0418977
0.102034 0.0380901
0.0278228 0.052654
0.0859467 0.0564185
-0.000835063 0.0116969
0.00931421 0.0372301
0.10955 0.0440209
0.0851613 0.051654
0.00080867 0.0227856
0.0706156 0.0587556
-0.0234677 0.0179142
0.0594397 0.0588227
-0.0209824 0.019239
-0.0118768 0.0447374
0.0966595 0.0313818
0.0827282 0.0128075
0.0621308 0.0113547
-2.23337e-05 0.0119324
0.0416418 0.017155
0.0691068 0.0415062
0.0412776 0.0209376
0.0860587 0.0121316
0.0541227 0.0446528
0.0625951 0.0509626
-0.0467978 0.0211176
0.0174339 0.0429705
0.0300726 0.0086457
0.0122243 0.0439576
0.0417702 0.012468
-0.0416401 0.0138333
0.105471 0.0484711
-0.000835392 0.0283762
0.088729 0.0495195
0.126169 0.0441558
0.106655 0.0356856
-0.0449599 0.0167058
-0.0295559 0.0167704
0.0188609 0.0192336
0.0791268 0.0408659
-0.0734312 0.0151599
0.0594204 0.0372914
-0.000807624 0.0118262
0.0188023 0.00471342
0.0691986 0.0543207
0.0207193 0.0455838
0.0815268 0.0414798
0.0337178 0.0424127
-0.0204028 0.0141151
parameters: [ 8.551  0.774  4.     1.347  4.125]. error: 4459638337.69.
----------------------------
epoch 0, loss 1.15006
epoch 128, loss 0.930401
epoch 256, loss 1.16989
epoch 384, loss 1.07173
epoch 512, loss 1.19097
epoch 640, loss 1.10014
epoch 768, loss 1.0209
epoch 896, loss 1.10354
epoch 1024, loss 0.935879
epoch 1152, loss 0.996616
epoch 1280, loss 0.897634
epoch 1408, loss 1.01833
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.000835111 0.0331314
0.0271289 0.0322945
-0.0109489 0.0357376
0.0724741 0.0399934
0.0439209 0.0409813
0.0271115 0.0275769
-0.0827278 0.0339262
0.0199307 0.0295373
-0.00737805 0.0334263
0.0871097 0.0423803
0.0541426 0.0308845
0.0261392 0.040395
0.0237987 0.0423401
-0.0326435 0.0336956
0.0424268 0.04283
0.0191272 0.0434941
0.0494968 0.0413441
0.0589127 0.0342641
0.0978866 0.0343893
-0.00236368 0.0466532
0.038792 0.0418279
0.0968255 0.0406857
0.0650733 0.0289275
0.033718 0.0326019
0.0710835 0.0449886
-0.065961 0.0329515
0.0724734 0.0405967
0.0168394 0.0374747
0.0675205 0.035499
0.0109586 0.036526
0.049499 0.0388388
0.000782366 0.0300719
-0.053266 0.0400272
0.0538015 0.0374479
0.0317699 0.0368022
0.00250079 0.0298559
0.0952315 0.0413594
0.0977799 0.0331189
0.100097 0.0345652
0.0950804 0.0394236
0.0384877 0.0342295
0.00931417 0.0412142
0.104819 0.0410111
0.038544 0.0334913
0.0417583 0.0251296
-0.0189887 0.0300362
0.0412752 0.0372807
-0.0177194 0.0321442
0.0368959 0.0395886
-0.0416423 0.0389311
0.0281392 0.0487841
0.0867334 0.0368349
0.0593133 0.0373614
0.0140549 0.0419916
3.40452e-06 0.0350599
0.0220537 0.0396846
-0.013282 0.0355836
-0.0707068 0.0265233
0.0710835 0.0457752
0.0594398 0.0344597
0.0209719 0.0427237
-0.0109566 0.0345831
0.0727412 0.0440445
0.060558 0.0290236
0.0222151 0.0327749
-0.0394792 0.0340598
0.05627 0.0329437
0.0803921 0.0349528
-0.049248 0.0361445
-0.0385437 0.0340987
0.0458116 0.0318527
0.0594473 0.0363978
0.0594452 0.038947
0.029557 0.0377833
0.0594399 0.038764
0.0407513 0.0336405
0.0271289 0.0334168
-0.0385437 0.0366408
0.0133878 0.0332929
0.118684 0.0352649
-0.0113473 0.0362077
0.101441 0.0406926
0.0397476 0.0380565
0.0871097 0.0378522
-0.00459356 0.0353491
0.0283785 0.0357114
-0.0385421 0.0362541
0.0217931 0.0392312
-0.0161248 0.0405277
0.110244 0.0322839
-0.00142797 0.028744
-0.0621274 0.0377144
0.0333262 0.0299541
0.0301504 0.0393459
-0.0340213 0.0358583
0.090741 0.0472028
0.0482639 0.0429997
0.121243 0.0383619
0.125146 0.0352994
0.0859467 0.0450594
0.0676335 0.0364678
0.0424529 0.0391318
0.0730674 0.0371903
0.0594398 0.0450401
0.0109781 0.0349566
3.82209e-05 0.0327875
0.0117785 0.0303438
-0.0234677 0.0382635
0.0301505 0.0344587
-0.0532547 0.03593
0.0593665 0.0339269
0.0189892 0.0333152
0.110244 0.0384202
0.0868851 0.0375132
0.133575 0.0301338
-0.0631232 0.0334035
0.0424316 0.0389471
0.0871098 0.0431085
-0.0385433 0.0354359
0.0384877 0.0309276
0.014471 0.0364653
0.00931426 0.0395907
0.0594379 0.034272
0.0109781 0.0338809
-4.6184e-07 0.0287424
0.00863032 0.0342722
0.106645 0.0301857
0.00460075 0.0339845
parameters: [ 7.824  2.707  3.382  2.347  1.507]. error: 107618148.28.
----------------------------
epoch 0, loss 1.14198
epoch 128, loss 1.00215
epoch 256, loss 1.0768
epoch 384, loss 1.04617
epoch 512, loss 1.11039
epoch 640, loss 0.987892
epoch 768, loss 1.03944
epoch 896, loss 0.874046
epoch 1024, loss 1.11008
epoch 1152, loss 0.824555
epoch 1280, loss 0.842518
epoch 1408, loss 0.854692
epoch 1536, loss 0.760401
epoch 1664, loss 0.964396
epoch 1792, loss 0.861698
epoch 1920, loss 0.891999
epoch 2048, loss 0.952949
epoch 2176, loss 0.800537
epoch 2304, loss 0.940945
epoch 2432, loss 0.970889
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.086885 0.0282313
4.17498e-06 0.0236342
0.0113418 0.03485
0.0661224 0.035759
0.121243 0.0459636
0.0267779 0.0427503
0.000835111 0.0247264
0.0131845 0.0299514
-0.00142797 0.025852
0.000807032 0.024575
0.0394799 0.026531
0.0594297 0.0390223
0.100668 0.0388714
0.0271204 0.0346755
0.0764481 0.0480857
0.0295774 0.0258311
2.96104e-08 0.0253566
0.0140549 0.0487985
0.0290868 0.0447792
0.0277564 0.0480767
-1.63011e-07 0.0205714
0.0414075 0.0294218
0.0691985 0.0510408
0.0871097 0.0464023
0.0449618 0.0382074
0.0477958 0.0483757
0.0604514 0.046488
0.0407515 0.0494582
0.143948 0.042868
0.076422 0.0430495
0.0188023 0.0237107
-0.0272995 0.02941
0.0497674 0.0415865
0.0396776 0.0387502
0.0417454 0.0207599
-0.0385431 0.0296965
-0.016115 0.047813
0.062595 0.0392965
0.0414075 0.027959
-0.0215238 0.0246912
0.0859467 0.043371
-0.0188023 0.0218694
0.032902 0.0436786
0.0991789 0.0493393
0.0461383 0.0476884
-0.049248 0.0239633
0.0907409 0.0395598
-0.0234677 0.0302994
0.0321468 0.0477227
0.0318695 0.0429219
0.0594397 0.0448182
0.0131849 0.0207833
-0.0267352 0.0280553
0.0317699 0.0354583
0.0211031 0.0268104
0.0911173 0.0510544
-0.00236343 0.0528433
0.12945 0.0217846
0.0897923 0.0508741
0.0807987 0.0290748
0.0318766 0.0289882
0.0907409 0.0497767
0.0321467 0.0432395
0.0626002 0.0448629
0.0272988 0.0315682
0.112932 0.0419385
0.0211031 0.0273895
0.0385437 0.0271241
-0.0144706 0.0276606
0.0589764 0.0502465
0.0861488 0.0452531
0.0133907 0.0514408
0.0113491 0.0298996
0.0407515 0.032255
-0.0857226 0.0203437
-6.15975e-07 0.030759
0.0594397 0.0432187
0.123121 0.0527526
0.0278228 0.051845
0.0871097 0.0357168
0.124277 0.0374201
0.106629 0.0324482
-0.00459356 0.0242252
-0.0247813 0.0175951
0.0407513 0.0285015
-0.0109566 0.0269514
0.124277 0.0369906
0.0109517 0.0274825
0.0800821 0.0470847
0.0648671 0.0462853
0.0204119 0.0236562
0.0396776 0.0283845
0.132184 0.049879
0.0131845 0.0194164
0.0667826 0.0419222
0.0871098 0.0544119
0.0927399 0.0460606
0.0594397 0.0581252
0.0327313 0.0465006
0.0168394 0.035772
0.0468012 0.0210932
0.0964307 0.0429607
0.0321467 0.042499
-0.0621274 0.0246074
0.0594273 0.0485438
-0.0492483 0.0255774
0.0599612 0.0432768
0.0417702 0.0229529
0.0416434 0.0318333
-0.0734384 0.0313443
0.0318693 0.0436027
0.0122504 0.0309659
0.0317699 0.0578566
0.0727412 0.0477601
0.0174335 0.0427806
-0.0109842 0.0262555
0.0210915 0.0341116
0.0878297 0.0459835
-0.0113495 0.0316439
0.0122352 0.0482778
0.10955 0.031491
0.0952315 0.0494177
0.0477959 0.0432838
0.027745 0.0463765
0.0538297 0.0393636
0.09274 0.0430162
-0.00737805 0.0370087
0.0482638 0.0458508
parameters: [ 8.114  1.937  3.628  1.948  2.55 ]. error: 6.78600955846e+12.
----------------------------
epoch 0, loss 1.30727
epoch 128, loss 1.40957
epoch 256, loss 1.25861
epoch 384, loss 1.36066
epoch 512, loss 1.27053
epoch 640, loss 1.42441
epoch 768, loss 1.0371
epoch 896, loss 0.809025
epoch 1024, loss 1.01062
epoch 1152, loss 0.980391
epoch 1280, loss 1.38726
epoch 1408, loss 1.0647
epoch 1536, loss 1.04669
epoch 1664, loss 1.16562
epoch 1792, loss 1.07893
epoch 1920, loss 1.12591
epoch 2048, loss 0.875697
epoch 2176, loss 0.851387
epoch 2304, loss 1.00232
epoch 2432, loss 1.16828
epoch 2560, loss 1.00771
epoch 2688, loss 1.00968
epoch 2816, loss 1.03287
epoch 2944, loss 1.02998
epoch 3072, loss 1.09289
epoch 3200, loss 1.04709
epoch 3328, loss 1.08182
epoch 3456, loss 1.0586
epoch 3584, loss 1.04111
epoch 3712, loss 1.0465
epoch 3840, loss 1.06893
epoch 3968, loss 0.928409
epoch 4096, loss 0.857818
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0596272 0.0474257
-0.0188057 0.0440971
0.0497671 0.049568
0.0815268 0.0430458
0.0562649 0.0390403
0.0390991 0.0434112
0.0327314 0.0524486
-0.00731719 0.0383853
0.0937752 0.0524773
0.0989677 0.0379308
-0.000835063 0.0374378
0.0730675 0.0383017
0.0223968 0.0544355
0.0739226 0.0433475
-0.0177216 0.0404736
-0.0117783 0.0399832
-0.0234677 0.0461627
0.0595095 0.040002
-0.0271121 0.0388641
1.01848e-06 0.0334748
-0.0621272 0.0379266
-0.016115 0.048524
0.0710837 0.0521199
0.0710837 0.0514273
-0.0367964 0.0411527
0.0911173 0.0481918
0.000793148 0.0391676
0.0594325 0.0473559
0.0189892 0.0361179
0.0286795 0.0381463
0.0594396 0.0512832
0.130757 0.0445665
3.40452e-06 0.0366345
0.129445 0.0368521
0.0950804 0.0407414
0.0122404 0.0521367
0.0237987 0.0377002
0.000794682 0.032961
0.0677309 0.0472187
0.0317697 0.040007
0.0594396 0.0383793
0.0527575 0.0335231
0.0818743 0.04498
-0.0204001 0.0350033
0.0305923 0.0424444
-0.0611177 0.0395305
0.0267369 0.034113
0.0827282 0.0324264
-0.0144843 0.0481526
0.0968256 0.0315344
-0.0295823 0.0373211
0.0606162 0.0427187
0.0977799 0.0505386
0.106619 0.0506961
0.0815268 0.0517372
0.0582389 0.0437104
0.0594398 0.034663
0.0236481 0.0435401
0.0329021 0.052341
0.0594397 0.0437025
0.0861487 0.0391992
0.0464005 0.0371878
-0.0109774 0.043039
0.0492512 0.0390387
0.0594402 0.0444774
-0.0494953 0.0477162
0.0606162 0.0482907
0.0964305 0.0358344
0.0464005 0.0499631
0.0322982 0.0414206
0.109566 0.0433935
0.0968257 0.0413733
0.0661222 0.0430045
0.00250079 0.0380881
0.080392 0.0479768
0.0887289 0.0432446
0.0346447 0.0394855
1.96444e-05 0.0347719
0.0907409 0.046775
0.0624794 0.0376247
-0.0106908 0.037195
0.0424529 0.0504781
0.0261391 0.0353932
0.0904958 0.0467726
0.0952317 0.0410384
0.0177241 0.041114
0.0197009 0.046653
0.0594396 0.0414021
-0.0385437 0.0451543
-0.0621346 0.0349857
0.0435741 0.0527908
-0.00236343 0.0498303
0.0624794 0.0469965
0.0321897 0.0548786
-0.0716787 0.0466766
0.0734394 0.0336414
0.0791961 0.03612
-0.0248092 0.0528138
0.0236481 0.0410875
-0.000835063 0.043014
-0.129457 0.0292819
0.043628 0.041855
0.0416368 0.0395955
-0.0207292 0.0450494
0.0529666 0.0455939
1.96444e-05 0.0374234
-0.0414059 0.0481015
0.0310444 0.0451442
3.36628e-05 0.0439029
0.0248197 0.0519318
0.0594399 0.0476642
0.0868851 0.0343484
0.129458 0.0278139
0.0625949 0.0461404
0.0582389 0.0409749
0.0333261 0.0413231
0.126086 0.0549776
0.05943 0.0559741
0.0730674 0.0351298
0.029557 0.0451462
0.0271204 0.0470704
0.0346353 0.032372
0.0131861 0.0345851
-0.0662877 0.0373237
0.0220537 0.0412329
0.0527578 0.0422506
0.0865811 0.0456519
0.0509553 0.0460205
parameters: [ 8.551  0.774  4.     1.347  4.125]. error: 1958463661.38.
----------------------------
epoch 0, loss 1.02311
epoch 128, loss 0.885237
epoch 256, loss 1.13858
epoch 384, loss 0.726783
epoch 512, loss 1.09395
epoch 640, loss 1.03568
epoch 768, loss 1.12988
epoch 896, loss 0.900516
epoch 1024, loss 1.16787
epoch 1152, loss 1.01578
epoch 1280, loss 0.865495
epoch 1408, loss 1.09812
epoch 1536, loss 1.09008
epoch 1664, loss 0.921102
epoch 1792, loss 0.813263
epoch 1920, loss 0.782793
epoch 2048, loss 0.913799
epoch 2176, loss 0.968075
epoch 2304, loss 0.955512
epoch 2432, loss 1.07768
epoch 2560, loss 1.08537
epoch 2688, loss 1.01205
epoch 2816, loss 0.969807
epoch 2944, loss 0.994617
epoch 3072, loss 1.11081
epoch 3200, loss 1.02339
epoch 3328, loss 0.862144
epoch 3456, loss 1.27452
epoch 3584, loss 0.836412
epoch 3712, loss 0.785845
epoch 3840, loss 0.712218
epoch 3968, loss 1.12394
epoch 4096, loss 0.994784
epoch 4224, loss 0.948879
epoch 4352, loss 0.905349
epoch 4480, loss 0.809414
epoch 4608, loss 0.723447
epoch 4736, loss 0.926901
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0950803 0.0493581
0.0207377 0.0334615
0.0191276 0.0386694
0.0234681 0.0386651
0.0237986 0.0490636
0.130756 0.0372521
0.125873 0.0392746
0.0594398 0.0465572
0.0375434 0.0341865
0.081527 0.0390471
0.0661222 0.0471178
0.0594301 0.0268023
0.106655 0.0374524
-0.0385433 0.0354204
-0.0611272 0.0275495
0.0117785 0.0343027
-0.129453 0.0236019
0.080402 0.0388999
0.0791969 0.038871
0.106624 0.0458018
-0.0117822 0.0292953
0.0611204 0.0310812
0.0496683 0.0427791
0.126086 0.0441449
0.0734439 0.0324723
0.106629 0.043623
0.109566 0.0360861
-0.0707068 0.0305236
0.0197009 0.0407988
0.043628 0.0404499
0.0981543 0.0403239
6.7959e-07 0.0176844
0.0495763 0.0311544
-0.0621272 0.0295952
-0.0621346 0.0292503
0.022215 0.03295
-0.0385433 0.031783
0.0857795 0.0350256
3.36628e-05 0.0364745
-0.080798 0.0349526
0.0611263 0.0293376
0.0594398 0.0420496
-0.0394796 0.0290191
0.0710835 0.036162
0.0706155 0.0377259
-0.0295676 0.0267391
0.000834782 0.0244674
0.022444 0.0407529
0.0594398 0.0420757
0.0417484 0.0267929
0.0385431 0.0304458
0.0416434 0.0300979
-0.0131845 0.0299647
0.0319947 0.0393063
0.0385437 0.0283775
0.0594398 0.0319306
0.090741 0.0332726
0.0594398 0.043085
-0.0716686 0.0307191
0.025123 0.0389796
6.39156e-06 0.0255894
-2.15513e-06 0.0346801
-0.0390966 0.0273376
0.104819 0.0348985
0.0966596 0.0331234
0.080402 0.0393856
0.0462156 0.0281233
0.0813321 0.0351893
0.106655 0.0407986
0.101441 0.0345142
0.0329332 0.0353479
0.0317698 0.0491434
0.0706155 0.037912
-0.0248165 0.0380293
-0.0662833 0.0336497
0.0412776 0.0331905
0.0734439 0.0379603
0.0859466 0.0361912
2.96104e-08 0.0268306
0.126127 0.0335809
0.0594402 0.0306364
0.0650733 0.0364908
0.0281392 0.0370283
0.0853245 0.0383273
0.015257 0.029625
0.0208245 0.0287565
-0.0295559 0.0285526
0.0317699 0.0505214
0.028139 0.0361482
0.0594325 0.0334799
0.0495763 0.0292558
-0.0144836 0.0254024
0.0527575 0.0413546
0.0234754 0.0289706
-0.0346352 0.0327748
0.0594116 0.0227777
0.0261395 0.0430024
0.0495801 0.0326217
0.0538177 0.0335085
0.0659615 0.03448
0.0144846 0.0285419
0.0662801 0.0315433
0.0532557 0.0329605
-0.0271278 0.0322201
0.0527577 0.0456734
0.000213748 0.0405231
-0.0271152 0.0305612
0.0897923 0.047532
0.0945153 0.0268428
0.125883 0.0404506
0.059435 0.043628
0.0286789 0.0366004
0.10665 0.0456944
0.0394802 0.0294488
0.0952318 0.042071
0.105471 0.040701
0.0271109 0.0322308
0.0482639 0.0360829
0.0272894 0.0321766
0.0412752 0.0334846
-0.00891927 0.0283785
0.0394803 0.0282023
0.0122504 0.0437849
0.0648671 0.0310156
0.0337179 0.0497525
0.059181 0.034189
-0.00700262 0.0390193
0.0865811 0.0359267
parameters: [ 8.722  0.318  4.146  1.111  4.744]. error: 7.81566369267e+12.
----------------------------
epoch 0, loss 1.16005
epoch 128, loss 1.14158
epoch 256, loss 1.38223
epoch 384, loss 1.20326
epoch 512, loss 1.19594
epoch 640, loss 1.02298
epoch 768, loss 0.981936
epoch 896, loss 0.987259
epoch 1024, loss 0.930649
epoch 1152, loss 1.05034
epoch 1280, loss 1.29856
epoch 1408, loss 1.08421
epoch 1536, loss 0.997922
epoch 1664, loss 1.12003
epoch 1792, loss 0.896596
epoch 1920, loss 1.12586
epoch 2048, loss 0.985978
epoch 2176, loss 1.01967
epoch 2304, loss 1.03036
epoch 2432, loss 1.00868
epoch 2560, loss 1.02312
epoch 2688, loss 0.849484
epoch 2816, loss 1.06214
epoch 2944, loss 1.05442
epoch 3072, loss 0.824925
epoch 3200, loss 0.943534
epoch 3328, loss 0.966884
epoch 3456, loss 1.13349
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.038792 0.0319573
0.0271187 0.0250721
0.0416484 0.0267709
0.0197009 0.0334943
0.0317697 0.0308697
0.102035 0.025386
0.0599031 0.035566
-0.0110929 0.0319748
-0.036223 0.0226892
-0.0267352 0.0296978
0.0268143 0.0263384
-0.00894944 0.0325253
0.0867067 0.0400933
-0.0417792 0.0327654
0.0855722 0.0229102
0.0375382 0.0253353
-0.0707068 0.0245398
0.0234638 0.0296403
0.121243 0.0434678
-0.0385437 0.0267363
0.0387924 0.035345
0.0882822 0.0288188
0.0267781 0.034758
0.126086 0.0328946
-0.0631103 0.0289758
-2.23337e-05 0.0283786
-0.014485 0.0334417
0.0458115 0.0312434
0.0210209 0.0292829
0.130756 0.0373779
-0.0109489 0.0278601
0.0390895 0.0219157
0.062595 0.031319
0.129445 0.0280995
0.0237988 0.0344562
-0.00891927 0.0335048
-0.0449599 0.0279092
0.0305919 0.0309355
-0.129457 0.0288624
0.0117845 0.0251977
0.0997462 0.0316002
-0.0414014 0.0303124
0.0223972 0.0292297
0.0707072 0.0229599
0.106655 0.0278005
0.032902 0.0288384
0.0661222 0.0330162
0.0496682 0.0290931
0.0867119 0.0332983
0.00894003 0.0335437
-0.0416423 0.034192
0.0109517 0.0286044
0.0882822 0.0256984
0.0991789 0.0347477
0.0813269 0.0389897
-0.0417488 0.0325774
0.0223968 0.02576
0.024704 0.0294548
0.088729 0.0311024
0.0730676 0.0340633
-0.034636 0.0263917
0.0220537 0.034403
0.0907408 0.0362571
0.0375382 0.0305756
-0.0204122 0.0291536
0.0326469 0.0309169
0.0781223 0.02492
0.036794 0.0297915
0.0594397 0.0390116
-0.0417422 0.0256653
0.0477959 0.0311543
0.0424428 0.0260544
-1.32922e-07 0.030996
0.032158 0.0326484
0.0964305 0.0339424
-0.00699273 0.0320677
0.01684 0.0280672
0.130756 0.046912
0.0791961 0.0273331
-0.000793636 0.0285326
0.101441 0.0286213
0.0461385 0.028209
0.0691986 0.0336994
0.0859466 0.0403214
0.0337178 0.0451766
0.0267369 0.0295647
0.0122295 0.0245833
0.0318695 0.0287341
0.0310444 0.0307097
0.0582236 0.0288772
0.0871096 0.0384532
0.0867334 0.0378532
5.31452e-07 0.0258645
0.0197009 0.0343445
0.0462066 0.0276026
0.0417448 0.0248969
0.0236481 0.0370464
-0.0707062 0.0217136
0.0211182 0.0184706
0.0131855 0.0279405
-0.014471 0.0358079
0.0907409 0.0393194
0.0364958 0.0301102
-0.0529682 0.0299179
0.00931426 0.0329989
0.00596546 0.0373133
0.0390991 0.0273354
-0.00460347 0.0241594
0.130757 0.0377634
-0.00737805 0.0362592
0.0710838 0.0386227
-0.0417488 0.0262905
0.105506 0.0176922
0.0414051 0.0271516
0.0599031 0.0277148
0.0170834 0.0216569
0.0496682 0.0290083
0.0986862 0.0387623
-0.0346352 0.0272581
0.0594398 0.0315926
0.0989675 0.0340773
0.0626002 0.0306053
0.0594399 0.0300074
0.0734439 0.0246703
0.074953 0.0388644
0.0594562 0.0333363
0.0520969 0.0375543
0.0594399 0.034985
parameters: [ 8.384  1.218  3.858  1.577  3.524]. error: 5225183123.66.
----------------------------
epoch 0, loss 1.12027
epoch 128, loss 1.00516
epoch 256, loss 0.838088
epoch 384, loss 0.96111
epoch 512, loss 1.04072
epoch 640, loss 0.892426
epoch 768, loss 0.943614
epoch 896, loss 0.828659
epoch 1024, loss 0.8763
epoch 1152, loss 1.04289
epoch 1280, loss 1.01953
epoch 1408, loss 0.692459
epoch 1536, loss 0.82366
epoch 1664, loss 1.01176
epoch 1792, loss 0.924062
epoch 1920, loss 1.03652
epoch 2048, loss 0.977545
epoch 2176, loss 0.966007
epoch 2304, loss 0.916781
epoch 2432, loss 1.06709
epoch 2560, loss 0.997233
epoch 2688, loss 0.789939
epoch 2816, loss 0.964787
epoch 2944, loss 1.06889
epoch 3072, loss 0.909951
epoch 3200, loss 1.00522
epoch 3328, loss 1.25048
epoch 3456, loss 1.08912
epoch 3584, loss 0.981788
epoch 3712, loss 0.950334
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0593758 0.0345129
0.097887 0.0304699
-0.0532547 0.0273455
0.121243 0.055923
0.0117785 0.0234017
0.0866747 0.0383525
0.0867201 0.0442088
0.0330542 0.0281451
0.109566 0.0427988
-0.0131851 0.02367
0.0791969 0.0420425
-0.0117783 0.0272196
0.0247806 0.0408794
0.0243586 0.0363445
0.0321468 0.0411771
-0.00699273 0.0295789
0.0261392 0.0636859
0.0907411 0.0525359
0.038792 0.0384631
0.0594379 0.0393754
6.7959e-07 0.0257599
-0.0210969 0.0262716
0.130756 0.0567208
0.080392 0.0411535
0.0964305 0.0396659
0.0468012 0.0312985
0.0318695 0.0469033
0.0449618 0.0407688
0.129993 0.0365955
0.0482639 0.0483985
0.0901952 0.0354281
0.029557 0.0253275
0.0109586 0.0310594
-0.0494977 0.0273163
0.0964772 0.0383952
0.0800821 0.0383522
0.00931426 0.0361611
0.0435741 0.0347522
0.0594399 0.0464838
0.0659505 0.0473621
-0.0621274 0.0249905
0.0662934 0.0307544
0.0365698 0.0250234
0.0199309 0.0303702
-0.0204001 0.0352874
0.0859465 0.0564498
0.0859467 0.053866
0.0122194 0.0402664
0.046218 0.0427503
0.0661223 0.056507
0.0594398 0.0427704
0.090741 0.0467675
0.0286795 0.0361118
0.0594375 0.039217
0.0594466 0.0425662
0.109566 0.0442631
0.0464004 0.0469052
0.0286795 0.0307328
0.0247829 0.040481
0.028139 0.0510978
0.0390986 0.0257833
0.022444 0.0463863
-0.00628221 0.0406565
0.000802697 0.0257693
0.0335492 0.0389331
0.0527577 0.0562784
0.0236481 0.0481263
0.0764118 0.0279196
0.0495796 0.0201863
0.0210209 0.0312296
0.0594398 0.0492728
0.0764481 0.0257995
0.0594397 0.0467385
0.0468064 0.0362492
0.0642209 0.0293548
0.0237987 0.0383872
-3.17974e-05 0.0360753
0.0977799 0.0339412
-0.0734426 0.0436025
0.000213351 0.0299735
0.0482639 0.0480161
0.0385437 0.0182556
0.0133907 0.0418703
3.40452e-06 0.0251975
0.0662934 0.0248151
-2.99285e-06 0.0235894
0.0317698 0.0581022
0.0131848 0.0246679
0.0901949 0.0361956
0.0318693 0.0454174
0.0204045 0.0393046
0.0734439 0.032958
0.129916 0.0340279
0.0624794 0.0258551
0.0596272 0.0341691
0.0321468 0.041701
0.0691986 0.041064
-0.0267398 0.0288717
0.0237989 0.0326888
-0.00236347 0.0511864
-5.97989e-06 0.0222087
0.0271204 0.0195509
0.0243585 0.0461004
0.0261391 0.0641406
0.0706156 0.0483728
0.0201879 0.0315465
0.0611271 0.0215862
0.0492512 0.0279968
0.0385291 0.0291913
-0.0189864 0.0207684
0.0661222 0.06095
0.025123 0.0397813
0.0271204 0.026523
0.0966587 0.0455948
0.0868851 0.0582696
0.0207377 0.0256913
0.0911183 0.0502432
0.0989678 0.0312775
-0.000835063 0.0241461
0.0321586 0.0376245
-0.0204001 0.0352874
0.0093143 0.0421203
0.0594398 0.0452502
-0.0177216 0.0365412
0.0594452 0.0325449
0.0407513 0.0414704
0.0691072 0.036753
0.0373484 0.0322529
parameters: [ 8.467  0.996  3.929  1.462  3.825]. error: 2797319261.12.
----------------------------
epoch 0, loss 1.38039
epoch 128, loss 1.08404
epoch 256, loss 0.965835
epoch 384, loss 0.911618
epoch 512, loss 0.991207
epoch 640, loss 1.13787
epoch 768, loss 1.10628
epoch 896, loss 0.999517
epoch 1024, loss 0.883202
epoch 1152, loss 1.07682
epoch 1280, loss 1.04168
epoch 1408, loss 0.80754
epoch 1536, loss 0.971012
epoch 1664, loss 0.947479
epoch 1792, loss 1.06466
epoch 1920, loss 0.883987
epoch 2048, loss 1.08403
epoch 2176, loss 0.881134
epoch 2304, loss 0.775442
epoch 2432, loss 1.04189
epoch 2560, loss 1.15015
epoch 2688, loss 0.967492
epoch 2816, loss 1.05162
epoch 2944, loss 1.04571
epoch 3072, loss 0.925081
epoch 3200, loss 1.01849
epoch 3328, loss 0.975162
epoch 3456, loss 0.923129
epoch 3584, loss 1.06836
epoch 3712, loss 0.911842
epoch 3840, loss 0.887295
epoch 3968, loss 1.22622
epoch 4096, loss 1.09092
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0412752 0.0330089
0.0594398 0.0336539
-0.0529722 0.0353403
-0.0532643 0.0317228
0.0594325 0.0315328
0.0594494 0.0327716
-0.0631103 0.0315996
-0.000835048 0.032013
0.0261392 0.0403414
0.0800819 0.037487
0.129916 0.0344869
0.0764328 0.0370634
-2.95455e-07 0.0385198
0.0210212 0.0316037
0.0867335 0.0346039
0.0593312 0.0355769
0.0971043 0.0375596
0.0204028 0.0329441
0.134995 0.0366278
0.0477957 0.0381395
0.000834782 0.0347046
-0.0210896 0.0365764
-0.0295746 0.035409
-0.0385431 0.0344764
0.0370769 0.0319852
0.0412752 0.036366
-0.0495796 0.0374403
-3.17974e-05 0.0323934
0.0593133 0.0351255
0.0991788 0.0353897
0.0182077 0.0307127
-0.0234744 0.0369181
-0.0807974 0.0338267
0.121243 0.0338115
0.0373475 0.0374385
-0.065961 0.0353665
-3.11621e-07 0.0382979
0.0538177 0.0331126
-0.0323135 0.0322453
0.0375382 0.0364452
-0.0211014 0.0348876
-0.0707055 0.0341709
0.0910569 0.035428
-0.000805986 0.0334285
0.0764481 0.0358624
0.0593105 0.0385414
0.0966587 0.0352964
-0.00131656 0.0327326
-0.014471 0.0380638
-0.0662867 0.0300579
0.0871098 0.0311142
-0.0117817 0.0321234
0.10955 0.0345516
0.0594273 0.0351698
3.62028e-05 0.0337415
0.0152545 0.0330009
-0.00701261 0.0375973
0.134985 0.0358648
0.0952318 0.0329915
0.0375434 0.0384376
-0.00553123 0.0342092
0.0964772 0.0324084
-0.0385424 0.0339658
-0.0131854 0.0321476
0.0321468 0.0335644
0.0989675 0.0339486
0.0305923 0.0348842
0.0927396 0.0416884
0.0977802 0.0356278
0.027745 0.0343888
0.057604 0.0335145
0.0223972 0.0341419
0.0910569 0.033977
0.015257 0.0313832
0.028139 0.0324288
0.0897923 0.0325488
0.0215366 0.0291781
-0.0194973 0.0319393
-0.02083 0.0322796
-0.0707068 0.0366648
-0.0467928 0.0308714
0.0373484 0.0345264
0.0370775 0.0326915
0.0267779 0.0320377
0.0724734 0.0315929
0.0752955 0.0336091
-0.0152533 0.0311439
0.0477959 0.0331975
-0.0707062 0.034029
9.99101e-07 0.0351975
-0.0734312 0.030719
0.0701141 0.0331126
0.00931421 0.0327675
-0.0161248 0.033175
0.0582589 0.0346032
-0.0234677 0.0315045
0.0236479 0.0356229
0.12945 0.0311063
0.0594494 0.0381516
0.0407515 0.0327587
0.0237988 0.0363916
0.0538015 0.0341399
0.0541426 0.0366578
0.0223972 0.0343236
0.01684 0.0323583
0.0871097 0.035282
0.101441 0.0362729
-0.0734384 0.0352836
0.0477957 0.0319199
-0.0412711 0.0336944
0.0390991 0.0368891
3.62028e-05 0.0320833
0.0867333 0.0333679
0.0727409 0.0329094
0.0412725 0.0365434
0.0968255 0.0335221
0.000802697 0.0356048
0.0707066 0.0344149
-0.0707068 0.0323653
0.0220539 0.0335319
0.101441 0.035444
0.0271117 0.034479
0.0330949 0.0324832
-1.32922e-07 0.0345639
0.081797 0.0343912
0.0868848 0.0361002
0.0234681 0.0330946
0.0659615 0.0338795
parameters: [ 8.555  0.762  4.004  1.341  4.142]. error: 2329470256.67.
----------------------------
epoch 0, loss 1.33708
epoch 128, loss 1.22113
epoch 256, loss 1.03546
epoch 384, loss 0.890292
epoch 512, loss 0.97885
epoch 640, loss 1.09414
epoch 768, loss 1.09425
epoch 896, loss 1.07368
epoch 1024, loss 0.885071
epoch 1152, loss 0.849866
epoch 1280, loss 0.980325
epoch 1408, loss 0.854979
epoch 1536, loss 0.920169
epoch 1664, loss 1.15732
epoch 1792, loss 1.18019
epoch 1920, loss 0.752402
epoch 2048, loss 0.888056
epoch 2176, loss 1.09019
epoch 2304, loss 0.639072
epoch 2432, loss 0.978292
epoch 2560, loss 0.770967
epoch 2688, loss 0.782727
epoch 2816, loss 0.772327
epoch 2944, loss 0.871887
epoch 3072, loss 0.840317
epoch 3200, loss 0.843235
epoch 3328, loss 1.00733
epoch 3456, loss 0.812617
epoch 3584, loss 0.74448
epoch 3712, loss 0.718861
epoch 3840, loss 0.837197
epoch 3968, loss 0.761866
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0822309 0.0348605
0.0907411 0.0642258
0.132178 0.0599011
0.0527576 0.0328331
0.0417448 0.0137492
0.0594452 0.0431439
0.0414075 0.0224451
0.062595 0.0543532
0.0950802 0.0362172
0.0599612 0.0465995
0.121243 0.0466374
0.0868851 0.055214
0.072741 0.0403054
-0.0207316 0.0155284
0.0394803 0.0183188
0.088729 0.0424776
0.0273016 0.0137047
-0.0152583 0.0245797
-0.0734384 0.0304564
-0.0131845 0.0272289
-0.0857218 0.0213511
0.0857789 0.0400652
0.0599612 0.0394241
0.0109781 0.0281634
0.129925 0.0500939
0.0407513 0.0484172
0.0791268 0.0528017
0.0268195 0.0443428
0.0436381 0.0619302
-0.10012 0.0245398
-0.0188605 0.0170104
0.112932 0.0664093
-0.00699273 0.0400646
0.0236479 0.0588382
0.0337178 0.0664372
0.0966596 0.0400039
0.0986864 0.04006
0.0211182 0.0488408
-0.0250779 0.042969
0.0301504 0.0385965
0.0318695 0.0482323
0.121243 0.0481338
0.0278228 0.0522888
0.0131858 0.0178392
0.059181 0.0446485
0.0333262 0.0527247
0.0290868 0.0545052
0.0464004 0.0489886
0.10664 0.0491935
0.0753057 0.0524652
0.0375381 0.04778
-0.00628321 0.0715704
0.0317699 0.0525944
0.0290868 0.0520637
0.0424369 0.0395204
-0.063123 0.0264757
-0.0131854 0.0232415
-0.0144706 0.0173061
0.129925 0.0512108
0.0329332 0.0393763
0.0594398 0.0537159
0.0532677 0.012247
0.0144704 0.0257316
-0.0271153 0.0158681
0.10955 0.0521379
0.0927397 0.0381877
0.12945 0.0371904
-0.0340329 0.0126589
0.0625951 0.0497697
0.0589763 0.0375207
0.00863032 0.0508297
-0.00378297 0.0237
0.0237987 0.0337883
0.0734439 0.0198235
0.0449639 0.0298939
0.0495768 0.0165421
0.0621308 0.0156035
-0.0662843 0.0216981
0.0855721 0.0521442
0.0367961 0.0293934
0.0968257 0.0552026
0.0527576 0.0422474
0.0318787 0.0223395
0.132184 0.0527502
0.092056 0.0413655
0.0237987 0.0323994
0.0194981 0.0188877
0.0277564 0.0379544
0.036223 0.0193167
0.01684 0.0412523
0.126127 0.0432728
2.41162e-06 0.023438
0.0373484 0.0582648
0.0527576 0.0329773
0.0248097 0.0222397
0.0396776 0.0507209
-0.0385431 0.0167197
0.0326469 0.0153558
0.0594376 0.0500331
0.125883 0.0464221
0.0727412 0.0523632
0.0631207 0.0136001
-0.0131854 0.0274449
0.0497671 0.0468618
-0.00131656 0.0234017
-0.0417792 0.027124
0.0417448 0.0137492
-0.0271152 0.0180544
0.0247806 0.0213191
-0.0414059 0.0127912
-0.0462076 0.0143044
-0.0111821 0.0561426
-0.0117788 0.0219349
0.0661223 0.0506212
0.0527576 0.0505479
0.0335492 0.0449261
0.0861486 0.0405939
0.0321468 0.0492944
0.0385441 0.0161482
-0.000805986 0.0260379
-0.0211014 0.031725
0.0945038 0.0471512
-0.013282 0.0488145
0.0330953 0.0482238
0.0329331 0.0463428
0.0194981 0.0222589
0.0589763 0.0450333
0.0594482 0.0446483
parameters: [ 8.514  0.872  3.969  1.398  3.992]. error: 93117046.5514.
----------------------------
epoch 0, loss 1.2517
epoch 128, loss 1.12287
epoch 256, loss 1.28772
epoch 384, loss 1.12579
epoch 512, loss 0.889187
epoch 640, loss 1.03311
epoch 768, loss 1.11172
epoch 896, loss 1.03449
epoch 1024, loss 0.989999
epoch 1152, loss 1.14459
epoch 1280, loss 1.20022
epoch 1408, loss 1.19768
epoch 1536, loss 1.14886
epoch 1664, loss 0.998592
epoch 1792, loss 1.05165
epoch 1920, loss 0.94735
epoch 2048, loss 0.812684
epoch 2176, loss 1.06937
epoch 2304, loss 1.02025
epoch 2432, loss 1.04125
epoch 2560, loss 0.874503
epoch 2688, loss 0.64887
epoch 2816, loss 1.09167
epoch 2944, loss 0.97714
epoch 3072, loss 0.926182
epoch 3200, loss 0.879881
epoch 3328, loss 0.895746
epoch 3456, loss 1.03574
epoch 3584, loss 1.23359
epoch 3712, loss 1.0929
epoch 3840, loss 0.991369
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594398 0.0472026
0.0305919 0.0481719
0.0317697 0.0528022
-0.0117788 0.0101464
0.118684 0.0399491
-0.00735889 0.0314159
0.12945 0.0147821
0.036794 0.032216
0.046799 0.0310564
0.0593346 0.0230543
0.0691984 0.0454716
-0.0495796 0.0334523
0.09274 0.0484365
-0.0209708 0.0083265
0.0327314 0.0354141
0.125157 0.0526175
0.0764118 0.039375
0.019701 0.0326422
0.0870049 0.0341709
0.0509553 0.0276473
0.00250001 0.0183784
-0.0111692 0.0283333
0.00894003 0.0187481
0.059435 0.0419302
0.0764428 0.041228
0.0594397 0.0392513
-0.0346424 0.0220233
-0.0144856 0.0148327
-0.00628221 0.0398346
0.00932522 0.0427725
-0.0340213 0.0344518
0.0222152 0.0473417
0.0606162 0.0276294
0.019701 0.0345315
0.0865812 0.039022
0.0290868 0.0450356
-0.036223 0.0308229
0.0989677 0.0388197
0.0319945 0.046429
0.100116 0.0225466
0.0983621 0.0395555
0.0133878 0.0342266
0.0346452 0.0240969
0.109566 0.0438535
0.0582236 0.0343629
0.0991789 0.0324268
0.0964305 0.0465712
-0.0494977 0.0210749
0.0375434 0.0373766
0.062595 0.0501587
0.0364958 0.0219096
0.126171 0.0408405
-0.00735889 0.0321982
0.110244 0.0271456
0.0417702 0.0256761
0.00891968 0.0265512
0.0594494 0.0429903
0.0140549 0.0385289
-0.0194897 0.0252792
0.0907409 0.030472
0.0594426 0.036099
0.0861488 0.0352523
0.129458 0.0257671
0.0323166 0.0205517
-0.0247813 0.0326265
-0.041769 0.0348952
0.121243 0.0505345
-0.0023636 0.0572471
0.00863092 0.0275896
-0.0131851 0.0369856
0.0871095 0.0425953
0.0329331 0.0332033
-0.0113473 0.028983
0.0267419 0.0245958
-0.00700262 0.0448776
-0.0152556 0.0347184
0.0661225 0.0516398
-0.000805986 0.0271691
0.129445 0.0231446
0.0594398 0.0285025
0.0989675 0.0415294
0.0734322 0.0175917
0.0251231 0.0402794
0.0989677 0.0361817
0.0271204 0.0275945
-0.0204122 0.0172138
-0.0412757 0.0365971
0.0538177 0.0248348
0.0631173 0.0198642
0.0594251 0.0465702
0.0964305 0.0190403
0.0977799 0.0392185
0.0677309 0.0321529
-0.0234744 0.0341502
0.0290868 0.0341685
0.0385427 0.0348443
0.0626106 0.0392697
-0.00236373 0.0540727
0.0131849 0.0289424
0.0783558 0.0412594
0.0594395 0.0335826
0.130756 0.0564324
0.101814 0.0335515
-0.00735889 0.040762
0.0813268 0.0357447
-0.0210896 0.0168657
0.102035 0.0444006
0.0594396 0.0375648
-0.00894944 0.0270324
0.0210915 0.0208788
0.0199307 0.0396746
0.0283785 0.0291279
-0.0807974 0.0379001
0.057604 0.0421399
0.0496683 0.0299442
0.0860428 0.0288111
0.032933 0.0534891
0.088729 0.0497275
-0.0495799 0.0203513
0.080402 0.0311453
0.0691985 0.0518715
0.0541227 0.0309672
0.0494968 0.0153278
0.0188016 0.0313053
0.0462066 0.0456265
0.121243 0.0486725
-0.0394792 0.0177734
0.0210915 0.0290613
parameters: [ 8.496  0.92   3.953  1.422  3.928]. error: 3.73466967664.
----------------------------
epoch 0, loss 1.77562
epoch 128, loss 1.51733
epoch 256, loss 1.20521
epoch 384, loss 1.11968
epoch 512, loss 1.12743
epoch 640, loss 1.05666
epoch 768, loss 0.761127
epoch 896, loss 0.93747
epoch 1024, loss 0.945085
epoch 1152, loss 0.992558
epoch 1280, loss 1.07182
epoch 1408, loss 0.912972
epoch 1536, loss 0.932666
epoch 1664, loss 0.91535
epoch 1792, loss 0.825117
epoch 1920, loss 0.792426
epoch 2048, loss 0.695479
epoch 2176, loss 0.962388
epoch 2304, loss 0.694895
epoch 2432, loss 0.866785
epoch 2560, loss 0.867549
epoch 2688, loss 1.01236
epoch 2816, loss 0.805766
epoch 2944, loss 0.767092
epoch 3072, loss 0.880255
epoch 3200, loss 0.947525
epoch 3328, loss 0.805365
epoch 3456, loss 0.968631
epoch 3584, loss 0.902407
epoch 3712, loss 0.885408
epoch 3840, loss 0.816205
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.129449 0.00998849
0.0093143 0.0522927
0.000794682 0.0101326
-0.0394792 0.0187778
-0.0390898 0.000617348
0.0397474 0.0469571
-0.10011 0.00216921
-0.0385434 0.00858783
0.0449618 0.010243
0.0495768 0.00544386
0.0236479 0.0564888
0.0144707 0.00895435
-4.6184e-07 0.00553604
-0.0189848 0.0143544
0.0868848 0.0445388
0.0346348 0.00236078
-0.0248165 0.0131384
0.0267369 0.0136451
0.0424581 0.0586749
0.105506 0.0490503
0.0188023 0.00719461
0.09274 0.0450401
0.0495801 0.0138917
0.0859465 0.0410397
0.0237986 0.0421219
0.0384777 0.0406239
-0.032317 0.00487692
-0.049248 0.000207287
-0.00893962 0.0124709
-0.049248 0.018706
0.0527577 0.0405244
0.059181 0.045521
-0.00378297 0.0112883
-0.0412711 0.00778276
0.0189892 0.0112512
-6.30661e-07 0.0228316
0.0384777 0.0421908
0.0752422 0.0396187
0.0941706 0.0505452
0.0412725 0.00563783
0.0286789 0.0375772
0.0541227 0.0262617
0.0439209 0.0386641
-0.0248165 0.0015229
0.0189868 0.0157179
0.000796249 0.00230966
0.0497671 0.0480572
-0.0152533 0.00880304
0.028139 0.038822
0.0337182 0.0337735
5.31452e-07 0.0168464
0.126206 0.0370873
0.0321467 0.045459
-0.129445 0.00240551
0.0321466 0.043858
0.0321466 0.0476054
-0.0414033 0.00590105
0.0813321 0.0428834
-0.0209824 0.0109718
-0.0390966 0.0021871
0.0594399 0.0465205
0.0594301 0.0384881
-0.0188599 0.0199687
0.0851511 0.0341551
0.0783558 0.0613348
-0.0346352 0.00617725
0.0594424 0.0333236
0.0321897 0.0422892
0.0482638 0.0421268
0.0599612 0.0413224
0.0113418 0.0280755
-0.0117788 0.00643662
0.0860523 0.0212833
0.0131848 0.0093507
-0.0208232 0.0101513
0.0910569 0.0434661
-0.0662833 0.0127647
-0.0611267 0.0138184
0.0384676 0.0346127
0.0691068 0.0442238
-0.0110929 0.032443
0.121243 0.0411616
0.00856342 0.00938743
-0.00142797 0.0158536
0.0643397 0.0299692
-0.0109842 0.00676539
0.0813269 0.0416139
0.101441 0.0396493
0.0319943 0.0616014
0.0594376 0.0507837
0.0317696 0.040007
0.0327315 0.0443
0.0319943 0.0444804
0.00863092 0.0383941
-0.0716686 0.0135661
0.121243 0.0448864
0.0327314 0.0387407
0.0109586 0.00854795
0.0867119 0.0442972
0.0208321 0.0215862
0.0468012 0.00700277
0.0261391 0.0505046
0.0131849 0.0156563
0.0217931 0.0467527
-0.0462148 0.00605524
0.0594397 0.0420289
0.0532677 0.0175389
0.088729 0.045655
0.0594396 0.0455552
0.0662931 0.00736853
-0.0271153 0.00956623
-0.0215356 0.0113262
0.0594396 0.0504448
0.088729 0.0604711
-0.0707055 0.0187137
0.100115 0.00863943
0.0706156 0.0447677
-0.00893962 0.00781298
0.0335492 0.0329236
0.028139 0.0508292
-0.0417425 0.0120659
0.0776855 0.00482175
-0.000835377 0.00892173
0.0661224 0.0562524
-0.0662833 0.00791442
0.0283783 0.0470119
0.0326469 0.0051268
0.0764428 0.0467076
parameters: [ 8.502  0.904  3.958  1.414  3.949]. error: 1788995953.12.
----------------------------
epoch 0, loss 1.23088
epoch 128, loss 1.08993
epoch 256, loss 1.0765
epoch 384, loss 1.19177
epoch 512, loss 1.08386
epoch 640, loss 1.17724
epoch 768, loss 1.20289
epoch 896, loss 0.955469
epoch 1024, loss 1.21347
epoch 1152, loss 0.967668
epoch 1280, loss 1.25935
epoch 1408, loss 1.08097
epoch 1536, loss 1.08324
epoch 1664, loss 1.34803
epoch 1792, loss 0.849418
epoch 1920, loss 0.984299
epoch 2048, loss 0.985781
epoch 2176, loss 0.818474
epoch 2304, loss 1.09455
epoch 2432, loss 0.981237
epoch 2560, loss 1.19178
epoch 2688, loss 1.23731
epoch 2816, loss 1.0353
epoch 2944, loss 1.18039
epoch 3072, loss 1.0732
epoch 3200, loss 1.02191
epoch 3328, loss 1.00056
epoch 3456, loss 1.18589
epoch 3584, loss 0.964805
epoch 3712, loss 0.901395
epoch 3840, loss 1.04015
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0111692 0.0411763
-0.0621343 0.043099
0.0851511 0.0427874
0.0855722 0.0425467
0.0611271 0.0421037
0.0605534 0.0421877
0.0318693 0.0429572
0.076422 0.0431239
0.0594374 0.0423627
0.0727409 0.0428639
0.0236482 0.044337
0.0337178 0.0428661
0.0497674 0.0424674
0.0168394 0.0432096
2.41162e-06 0.0413094
0.0860523 0.0417083
0.0122504 0.0429801
-0.0707062 0.0416183
-0.00701261 0.0445855
0.0594325 0.0408844
0.0271187 0.0417687
-0.0272977 0.0423188
0.101814 0.0425401
0.110244 0.0430597
0.0117845 0.0415064
0.0236481 0.0440586
0.0122194 0.0425934
0.0691986 0.0426912
0.0417583 0.0401496
0.0710835 0.0438377
0.000213748 0.0431394
0.00931417 0.0410643
0.032158 0.0418258
-2.99285e-06 0.0415579
-0.0716787 0.0419949
0.026809 0.043662
0.0659615 0.0445097
-0.0412685 0.0417744
0.022215 0.0422504
0.109566 0.0410275
0.0182077 0.043436
0.0650734 0.0437201
0.0191272 0.0426082
0.0417583 0.0411715
3.82209e-05 0.0414031
0.0191276 0.0435961
0.00863029 0.0437587
-0.0267398 0.0431341
0.0272916 0.042988
0.0346348 0.0433412
-0.0385424 0.0414166
-0.0113495 0.0420636
0.0594449 0.0413386
0.105503 0.0425944
0.0511643 0.0418422
-0.0492483 0.0421178
0.0177241 0.0434423
0.0859467 0.0439116
0.0496681 0.044037
0.0706155 0.0437582
0.0133878 0.0411235
-0.0631103 0.0416749
-1.17339e-07 0.0405812
-0.00699273 0.0440086
0.0424428 0.0425193
0.125873 0.0438759
-0.0611177 0.0419979
-0.0295559 0.0437707
0.074953 0.043696
0.0286795 0.0429075
0.0497674 0.0433814
0.0594473 0.0415351
-0.10011 0.0417064
0.05627 0.041714
-7.68032e-07 0.0416454
-0.0210969 0.0424562
0.0599031 0.0410568
0.0626002 0.0432376
0.0237989 0.0409131
-0.0152533 0.0409374
-0.0462049 0.0424706
0.0724741 0.0413618
0.0436381 0.0422043
0.0945038 0.0441733
0.0346353 0.0413985
0.0329331 0.0422894
0.000796249 0.0417499
0.0267781 0.0437338
-0.0412711 0.0415091
0.032933 0.0446878
-0.0807986 0.0416936
0.0182076 0.0433789
0.0477957 0.0446854
0.0997468 0.0443582
0.0210958 0.0436198
0.0131845 0.0407012
0.0529686 0.0422422
-0.0188028 0.0410089
0.0977799 0.0418789
0.100097 0.042128
0.0201879 0.0440365
0.0997468 0.0449429
-0.00236343 0.0422782
0.00131949 0.0413358
0.0496682 0.0440408
0.0295687 0.0422611
-0.0111732 0.0429306
0.124311 0.0423664
0.129993 0.0411589
0.0582236 0.0411492
0.0424529 0.0437662
-0.034636 0.041993
0.0329331 0.0438432
0.0594398 0.042776
0.0497671 0.0425676
0.0170834 0.0417317
-0.0272904 0.0424441
0.000835126 0.0405024
0.0318787 0.0416327
-0.0494977 0.0432127
0.0412776 0.0410087
0.0594204 0.0429155
0.046799 0.0403868
0.0335492 0.0432005
0.034034 0.0414389
0.0594396 0.0435327
0.0950805 0.0424817
6.39156e-06 0.0419872
parameters: [ 8.485  0.949  3.944  1.437  3.889]. error: 384017766.249.
----------------------------
epoch 0, loss 1.00354
epoch 128, loss 0.895717
epoch 256, loss 1.3336
epoch 384, loss 0.9281
epoch 512, loss 1.06451
epoch 640, loss 1.14156
epoch 768, loss 1.2295
epoch 896, loss 1.18165
epoch 1024, loss 1.12572
epoch 1152, loss 1.12459
epoch 1280, loss 1.28859
epoch 1408, loss 1.25722
epoch 1536, loss 1.15823
epoch 1664, loss 0.988679
epoch 1792, loss 1.15615
epoch 1920, loss 0.913168
epoch 2048, loss 1.13622
epoch 2176, loss 1.00303
epoch 2304, loss 0.91385
epoch 2432, loss 1.05138
epoch 2560, loss 1.32318
epoch 2688, loss 1.15309
epoch 2816, loss 1.25529
epoch 2944, loss 1.0371
epoch 3072, loss 1.11293
epoch 3200, loss 1.12948
epoch 3328, loss 0.980305
epoch 3456, loss 0.939237
epoch 3584, loss 1.25346
epoch 3712, loss 1.10092
epoch 3840, loss 0.918233
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-7.99663e-06 0.0279382
0.0346353 0.02486
-0.0118767 0.0293185
0.0964305 0.0295581
-0.0532643 0.0255471
0.0131855 0.0271532
0.0662931 0.0281179
0.0911173 0.0272829
0.0920508 0.0277557
0.0673482 0.0285543
0.036223 0.0269698
0.0813269 0.0288596
0.0691987 0.0282551
0.0189852 0.0272694
-0.0131858 0.0263834
0.062595 0.0296387
0.080382 0.027101
-0.00250418 0.0269103
0.0144704 0.0269433
0.0594396 0.0319521
0.100097 0.0290329
0.0927397 0.0287697
0.0964766 0.0296918
-6.0681e-07 0.0264234
0.0462066 0.0265601
0.0621308 0.025995
0.0706156 0.0281629
0.0882823 0.0267869
0.104819 0.0288657
0.0776735 0.0250378
0.0461384 0.0304666
0.09274 0.0305644
-0.0621343 0.0253692
0.0920961 0.0280239
0.101441 0.0273406
0.0691986 0.0292957
0.0611204 0.0279126
0.0133907 0.0298736
0.0997468 0.029755
0.0950805 0.0277714
0.0477956 0.0291618
0.0594398 0.0290613
-0.0449623 0.0251084
0.000782366 0.0250054
0.0330542 0.0296647
0.0271204 0.0273226
0.0752321 0.0277247
-0.129457 0.0278404
0.0199307 0.0286947
0.0122504 0.0287137
0.0691068 0.029442
0.0189892 0.0279694
-0.0346416 0.0271228
-0.0056768 0.0283635
0.0964766 0.0269977
-0.0662843 0.0256681
0.0529729 0.0275419
0.0594562 0.0291836
-0.0117817 0.0261664
0.0594397 0.0315799
-0.00553123 0.0267307
0.0865811 0.0301158
-2.86114e-05 0.0267323
0.0327313 0.0298519
0.126127 0.0285991
0.0594398 0.0288673
-0.0207367 0.0312299
0.00459996 0.0262051
0.0764328 0.0297165
0.0964307 0.0291934
0.00596546 0.031325
0.0532677 0.0265859
0.0122504 0.0277557
0.000807032 0.0248833
0.0462156 0.028267
0.0329332 0.0261861
-0.0144856 0.0275721
-0.0468045 0.0267644
-0.0631103 0.0254231
0.0927399 0.0279295
0.105506 0.0306498
0.01684 0.0281691
0.112932 0.028917
0.132178 0.0277209
0.0989678 0.0292419
-0.00737805 0.0293931
-0.0161248 0.0312747
0.0764428 0.028679
0.126171 0.0311569
-1.32922e-07 0.0279498
0.0317699 0.0269924
-0.0117817 0.0257373
0.0467944 0.0276689
-0.0204001 0.0260259
-0.10011 0.0258123
0.0851613 0.0290235
-2.86114e-05 0.0255746
0.0482639 0.0282852
-0.00331175 0.0281857
0.0362237 0.0269761
-0.0113473 0.0282053
0.0261392 0.0275336
0.0317696 0.0284833
0.0300726 0.0275281
-0.0716782 0.0280247
-0.0417422 0.0263644
0.0594027 0.0268961
0.0910567 0.0275163
0.0594494 0.0291288
0.0968257 0.0286665
0.0330953 0.028792
-0.0161572 0.0293743
0.0800819 0.028702
0.00863092 0.0281483
0.0321467 0.0317021
0.0211183 0.0275631
-0.0109842 0.0256675
0.0827282 0.026583
-0.0611267 0.0252301
0.032933 0.0295448
0.0152591 0.0264722
-0.129445 0.0271013
0.0210212 0.0294835
0.080402 0.027484
-0.0247813 0.027305
-0.080798 0.0266707
0.0261392 0.0290168
0.0594449 0.0298456
parameters: [ 8.491  0.933  3.949  1.429  3.91 ]. error: 58.2474216219.
----------------------------
epoch 0, loss 1.29603
epoch 128, loss 1.00477
epoch 256, loss 1.21144
epoch 384, loss 1.29817
epoch 512, loss 1.01479
epoch 640, loss 1.11343
epoch 768, loss 1.35691
epoch 896, loss 1.00567
epoch 1024, loss 1.27331
epoch 1152, loss 1.33173
epoch 1280, loss 1.22588
epoch 1408, loss 1.03894
epoch 1536, loss 1.25211
epoch 1664, loss 1.1795
epoch 1792, loss 1.33991
epoch 1920, loss 1.17747
epoch 2048, loss 1.13745
epoch 2176, loss 1.40003
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
3.36628e-05 0.0182904
0.0271163 0.0168634
-0.0807986 0.0156889
-0.0362226 0.0170217
0.0243585 0.0180561
-0.034636 0.0167488
0.0278229 0.0149237
-0.0113401 0.0159303
-2.99285e-06 0.016174
-0.0194973 0.0167427
-0.0532643 0.0166648
0.0871097 0.0191135
0.0152591 0.0144055
0.0594397 0.0159968
0.028139 0.0197392
-0.0734312 0.0173365
-7.68032e-07 0.0178315
0.0368964 0.0182573
0.0346447 0.0181237
0.00894003 0.0174681
-0.00236347 0.0146517
0.0295843 0.0178863
-0.0318748 0.0189472
0.0594398 0.0179855
0.0482639 0.0191465
0.0267419 0.0162082
0.00459996 0.0164278
0.0201879 0.019072
0.0174339 0.0187868
-0.0529698 0.0187411
0.0267779 0.0190218
2.41162e-06 0.0144711
0.0467944 0.0175959
3.62028e-05 0.0188613
0.0631196 0.0178366
0.0417484 0.0164002
-0.0417624 0.0169214
0.135017 0.016908
-1.1614e-09 0.0181588
0.00131924 0.0158209
0.0268143 0.0178375
0.088729 0.0181188
-0.00236343 0.0172729
0.0268195 0.017019
-0.0323135 0.0173604
-0.0495796 0.0180194
0.0968257 0.0189582
0.093775 0.0172154
0.0945153 0.0168392
0.0997462 0.0166935
-0.00131656 0.0154511
-0.0207367 0.0168784
0.0327313 0.0178019
0.0562649 0.0163367
0.0247829 0.017462
0.0207193 0.0166494
-0.0188605 0.0183101
0.032933 0.0161723
-0.0394792 0.0165953
0.080402 0.0168686
0.0594397 0.0199285
0.100097 0.0148273
0.0662934 0.0172464
0.0594452 0.0174984
-0.129449 0.0163703
0.0662803 0.0161099
-0.0346352 0.0166553
0.0271185 0.0170781
0.085161 0.019525
0.0594204 0.0159208
2.05834e-07 0.0185661
0.0133878 0.0158531
0.0191276 0.0166521
0.00931421 0.0155939
0.0594397 0.0190242
0.0867335 0.0156187
0.0509553 0.0165459
-0.0417573 0.015868
-0.0234626 0.0185869
0.0189828 0.0171624
-0.0131854 0.0158577
0.0968257 0.01738
0.0424529 0.0165426
-0.0857218 0.018981
0.0996717 0.0180984
-0.0385424 0.0192603
0.0701141 0.0147817
0.0857183 0.0165178
-0.0611267 0.0193322
-7.99663e-06 0.0170088
8.40829e-06 0.0157842
0.0477956 0.0180283
-0.0023636 0.0161999
0.0330949 0.0191136
0.0237987 0.0161818
0.076422 0.0165508
0.134985 0.0151662
0.0106903 0.0182973
0.0494968 0.017486
-0.00728242 0.0141128
-0.0144836 0.0171775
-0.00894944 0.0198061
-0.0734384 0.0162929
0.0867333 0.0161934
0.0813268 0.01724
0.0781227 0.0182785
0.129925 0.0164027
0.0691986 0.0175901
0.0813268 0.0184506
-0.0111692 0.0143926
0.0208321 0.0182914
-0.0468045 0.0170146
0.124354 0.0164138
0.0368959 0.0171019
0.0594116 0.0189465
0.0405236 0.0157738
0.0594251 0.015735
0.0642209 0.0154297
0.0855722 0.0137688
0.109549 0.0182958
0.0691072 0.0180049
-0.014471 0.0161388
0.0707072 0.0150646
0.0394799 0.0165041
0.060558 0.0174309
0.0373475 0.0162209
0.110244 0.0170045
0.0189852 0.015774
parameters: [ 7.543  4.034  5.525  2.262  2.239]. error: 24434801805.8.
----------------------------
epoch 0, loss 1.17314
epoch 128, loss 1.23022
epoch 256, loss 0.977544
epoch 384, loss 0.97512
epoch 512, loss 1.12715
epoch 640, loss 0.882038
epoch 768, loss 0.885762
epoch 896, loss 1.07814
epoch 1024, loss 0.991683
epoch 1152, loss 0.995543
epoch 1280, loss 1.26948
epoch 1408, loss 1.21198
epoch 1536, loss 1.16842
epoch 1664, loss 0.976279
epoch 1792, loss 1.12645
epoch 1920, loss 0.904821
epoch 2048, loss 1.09972
epoch 2176, loss 0.926801
epoch 2304, loss 1.03625
epoch 2432, loss 1.03019
epoch 2560, loss 0.936176
epoch 2688, loss 1.0936
epoch 2816, loss 1.07673
epoch 2944, loss 1.21491
epoch 3072, loss 1.20803
epoch 3200, loss 1.30037
epoch 3328, loss 0.973639
epoch 3456, loss 0.913231
epoch 3584, loss 0.868409
epoch 3712, loss 1.03602
epoch 3840, loss 0.95296
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.049248 0.0289839
-0.00893962 0.0366799
0.0977799 0.0351248
-3.66166e-06 0.0346732
0.0739226 0.0363642
0.0594399 0.0426849
0.133575 0.0343232
0.00131924 0.0359109
0.0520969 0.0334532
0.0562597 0.0410027
0.080382 0.040195
0.0582236 0.036137
0.0594482 0.0400251
0.0322983 0.0336207
0.0901949 0.043232
0.0642209 0.0427587
0.0211185 0.0377513
0.0594399 0.0359574
0.100667 0.0459092
0.00142251 0.0303886
0.0593758 0.0496996
-0.0462076 0.0344379
0.0857284 0.0317404
-0.0807974 0.0436461
0.0188016 0.0407101
-0.0215356 0.0406256
0.0144707 0.04249
0.0131849 0.0384159
-0.0385437 0.0422743
0.0131858 0.0301664
0.027745 0.0297523
0.018802 0.0341552
-0.0211014 0.0430124
-0.0390898 0.0355594
0.0237988 0.0370651
0.0594375 0.0327847
-7.99663e-06 0.035041
0.0496681 0.0377604
0.0197009 0.0399448
-0.00331175 0.0377299
0.0439209 0.0379267
0.0436381 0.0345966
0.0594395 0.0370482
-0.10012 0.0360807
0.0996717 0.0437098
0.0661222 0.0391735
0.00931426 0.0461543
0.0710835 0.0414734
0.0527578 0.036814
0.000782366 0.0380749
0.0295774 0.0373884
0.0599031 0.0351436
0.0223968 0.0411674
0.0865811 0.0326202
0.0367961 0.0389503
-0.014485 0.0370095
0.0220537 0.0401742
0.0511643 0.0325296
-0.0326435 0.0357255
0.143948 0.0359855
-0.0860431 0.0342973
0.106629 0.0330558
-0.00735889 0.0354622
0.0997468 0.0434602
0.0781227 0.0394062
0.0234754 0.0370184
4.17498e-06 0.0444227
0.0188018 0.0376413
0.0594397 0.0332748
0.0220537 0.046585
-0.0117783 0.0342788
0.032933 0.047694
0.0330949 0.0376073
0.0327314 0.0379507
0.0495796 0.0301483
0.0626054 0.0463193
-0.0495796 0.0355461
-0.0631103 0.0403153
0.0248097 0.0323859
0.0283783 0.033049
0.0385291 0.0307786
-0.0118765 0.0498601
0.0594427 0.0322142
0.0851609 0.0559519
-0.0272977 0.0335814
1.74905e-07 0.0336427
0.0594396 0.043811
0.134985 0.0372822
0.0589133 0.0427682
-0.0449623 0.0341077
0.0594324 0.0428488
-0.0417422 0.0382096
0.0611263 0.0387817
0.109566 0.0459936
0.0857183 0.0347055
0.0482639 0.0345327
-0.00131656 0.0384544
0.0267781 0.046643
-0.0417425 0.0349652
0.0910569 0.0421822
0.0594427 0.0362404
-0.0495737 0.0370564
0.0281391 0.0412672
0.0966587 0.0361369
0.0594375 0.0304324
0.0394799 0.0431386
-0.0234677 0.0446634
-0.0295676 0.0334064
0.0199306 0.0377735
-0.0267352 0.030833
0.125873 0.0375083
0.0197009 0.0404489
-0.0188028 0.0337047
0.0340225 0.0402669
0.088282 0.0396668
0.0865811 0.0325967
0.0286789 0.0323958
0.0964307 0.0357069
0.0329332 0.0400072
-0.014471 0.032077
-0.0295676 0.0420742
0.0594398 0.0396026
0.0191276 0.0362534
0.057604 0.0375379
-0.0385437 0.0375638
-0.0707062 0.0366798
0.086885 0.0361754
0.0819546 0.0414042
parameters: [ 8.496  0.92   3.953  1.422  3.928]. error: 162263377050.0.
----------------------------
epoch 0, loss 1.90147
epoch 128, loss 1.34888
epoch 256, loss 1.53748
epoch 384, loss 1.20314
epoch 512, loss 1.10224
epoch 640, loss 1.19408
epoch 768, loss 1.08404
epoch 896, loss 0.998223
epoch 1024, loss 1.0618
epoch 1152, loss 0.876538
epoch 1280, loss 0.842494
epoch 1408, loss 0.783897
epoch 1536, loss 0.690069
epoch 1664, loss 0.790758
epoch 1792, loss 0.800163
epoch 1920, loss 0.726042
epoch 2048, loss 0.751367
epoch 2176, loss 0.708618
epoch 2304, loss 0.813725
epoch 2432, loss 0.678871
epoch 2560, loss 0.884143
epoch 2688, loss 0.978277
epoch 2816, loss 0.777152
epoch 2944, loss 0.839152
epoch 3072, loss 0.886307
epoch 3200, loss 0.831219
epoch 3328, loss 0.776016
epoch 3456, loss 0.810084
epoch 3584, loss 0.648156
epoch 3712, loss 0.698818
epoch 3840, loss 0.946751
epoch 3968, loss 0.764615
epoch 4096, loss 0.83884
epoch 4224, loss 0.703561
epoch 4352, loss 0.640428
epoch 4480, loss 0.773714
epoch 4608, loss 0.760514
epoch 4736, loss 0.875836
epoch 4864, loss 0.730582
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
6.7959e-07 0.0220653
0.0387924 0.0532468
0.0319943 0.0609661
0.0449639 0.0208013
0.0562851 0.0589798
0.0865811 0.0590451
3.62028e-05 0.0139991
0.0417702 0.0167397
-0.00131656 0.0156137
0.0272916 0.0402317
0.0920961 0.0598717
0.0527575 0.0566032
-0.129457 0.0161478
0.0295687 0.0151662
0.121243 0.0531938
0.0267779 0.0426025
0.0449751 0.0601672
0.0950804 0.0585349
-0.0532643 0.0250806
0.0662931 0.000206725
0.0511643 0.0474991
0.0267779 0.0451944
-0.00249422 0.00420851
0.00863088 0.0544205
0.0871098 0.0579634
0.0764118 0.0516629
0.0416434 0.0136234
0.0346353 0.00558936
0.121243 0.0489709
0.000213748 0.0498245
0.022215 0.0421511
-0.02083 0.0231699
0.0390889 -0.0100786
0.034034 0.0124819
-0.0394792 0.0129873
0.0487559 0.0592423
0.081797 0.0460109
0.074953 0.0469606
0.125883 0.0512451
0.0492474 0.00659414
0.132184 0.0492913
-0.00250418 0.0142816
-0.0416423 0.0138844
0.0599031 0.0479593
-0.0131858 0.0206113
0.0882823 0.0589237
0.034034 0.0288709
0.0594397 0.0549031
-0.0209824 0.0271901
0.0871096 0.0553293
0.0594397 0.0554091
0.0385441 0.0282969
-0.027288 0.0202294
0.000796249 0.0170792
0.0417484 -0.00490891
0.0188015 0.0507092
0.0364958 0.0556454
0.0477957 0.0589786
0.0468012 0.0151941
-0.0807974 0.000244104
0.0390991 0.00851647
0.0281392 0.0649982
0.0752955 0.0546854
0.00863092 0.05112
0.05937 0.0522037
0.0710835 0.0583852
0.0859465 0.0592671
0.0907408 0.0619028
-0.0271126 0.000107434
-0.0529722 0.0149437
0.0710837 0.0526533
0.0964772 0.0499092
0.0776855 0.0340497
0.0752422 0.051285
0.0329021 0.050045
0.0248076 0.013045
0.0346452 0.0182181
0.0867333 0.0632821
0.0271204 0.00645449
0.0131855 0.0288939
-0.0468045 0.015456
0.0691068 0.0603287
0.019701 0.0502677
0.00080867 0.0107985
-2.64683e-07 0.0211782
0.0496682 0.0551771
0.0191272 0.0564653
0.0626054 0.060148
0.0234754 0.0115637
0.130005 0.056104
-0.0271157 0.0124457
-0.032317 0.00010397
-0.0529698 0.000324656
0.0800819 0.0595345
0.0385427 0.0170838
-0.00378521 0.0164926
0.0461385 0.0636691
0.0321468 0.0623477
0.0968256 0.0562197
0.130756 0.0586301
-0.053266 0.014076
0.0859465 0.0592671
0.0859465 0.0618351
0.0458115 0.0607154
0.124354 0.0546713
-0.0416401 0.0231065
0.0752955 0.0500488
-0.0662867 -0.00475973
0.0199307 0.0598947
0.0117782 -0.00553306
0.0853255 0.052186
0.00932522 0.0584044
-0.036223 0.0222639
-0.0734384 0.0116787
0.0594397 0.0491298
0.0594398 0.0538981
0.0861486 0.0655165
-0.0716782 0.00140578
3.40452e-06 0.0111301
0.0941706 0.0608329
-0.080798 0.0179967
-0.0295746 0.0186114
0.0337179 0.0364932
0.0724741 0.0566603
0.134985 0.0420679
0.0730675 0.0555514
0.0477956 0.0565548
0.022215 0.0605133
parameters: [ 8.496  0.92   3.953  1.422  4.928]. error: 1899594699.78.
----------------------------
epoch 0, loss 1.624
epoch 128, loss 1.58219
epoch 256, loss 1.17893
epoch 384, loss 1.27622
epoch 512, loss 1.04491
epoch 640, loss 0.969756
epoch 768, loss 1.23018
epoch 896, loss 1.18038
epoch 1024, loss 1.20535
epoch 1152, loss 1.23832
epoch 1280, loss 1.06931
epoch 1408, loss 1.16098
epoch 1536, loss 1.00362
epoch 1664, loss 1.02579
epoch 1792, loss 1.23902
epoch 1920, loss 0.952735
epoch 2048, loss 0.984023
epoch 2176, loss 1.02775
epoch 2304, loss 1.00327
epoch 2432, loss 1.03149
epoch 2560, loss 1.19046
epoch 2688, loss 1.24124
epoch 2816, loss 0.933811
epoch 2944, loss 1.02118
epoch 3072, loss 0.695234
epoch 3200, loss 1.0357
epoch 3328, loss 0.97632
epoch 3456, loss 0.787119
epoch 3584, loss 0.840118
epoch 3712, loss 0.883658
epoch 3840, loss 0.668214
epoch 3968, loss 0.890457
epoch 4096, loss 0.812807
epoch 4224, loss 0.728077
epoch 4352, loss 0.813204
epoch 4480, loss 0.882338
epoch 4608, loss 0.808713
epoch 4736, loss 0.962252
epoch 4864, loss 0.836295
epoch 4992, loss 0.946235
epoch 5120, loss 0.669178
epoch 5248, loss 0.784889
epoch 5376, loss 0.920232
epoch 5504, loss 0.701376
epoch 5632, loss 0.801475
epoch 5760, loss 0.790674
epoch 5888, loss 0.858078
epoch 6016, loss 0.683767
epoch 6144, loss 0.852744
epoch 6272, loss 0.839262
epoch 6400, loss 0.830403
epoch 6528, loss 0.839173
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.135027 0.041754
0.0582589 0.0383748
0.0621302 0.0150692
0.0461384 0.0537081
0.0937752 0.0480526
8.39808e-07 -0.00101254
0.0267443 0.0192022
0.0209836 0.0171686
0.0904958 0.0466228
0.00894166 0.0143957
0.0662934 0.00688605
0.0362237 0.0213868
0.0267393 0.0118018
0.0317696 0.0484976
0.0271117 0.00618731
0.100097 0.0501707
-0.0295823 0.0059528
0.0271187 0.00463771
0.0599029 0.0472879
0.0217933 0.0469428
-0.0271278 0.0204851
-0.00628221 0.0509693
0.0329021 0.0492088
0.0734439 0.00954352
0.0174335 0.0423976
0.0752321 0.0438292
7.08095e-06 0.0187295
0.0937752 0.0478746
-0.0807974 0.0188865
-0.0412731 0.016531
0.0168394 0.0437465
0.0237988 0.0481525
0.0330949 0.0468978
0.0594397 0.0446217
-0.0267352 0.0143196
0.0851613 0.0602234
-0.000835392 0.0206211
0.101441 0.038186
-0.0365701 0.0224824
0.0204045 0.0142978
0.026809 0.0403953
0.0594396 0.0534066
0.0424369 0.0411892
0.0405236 0.0512126
0.0538009 0.0446449
0.0237986 0.0481288
0.0594398 0.0437028
0.0177241 0.0185351
0.0861487 0.0525037
0.0937752 0.0487347
7.08095e-06 0.0223036
0.0978866 0.0500272
-0.0631232 0.015459
0.0267781 0.0397794
0.0594397 0.0455958
-0.0394796 0.021955
0.0144707 0.0215781
0.01134 0.00547292
-0.00236368 0.0553028
0.104819 0.0408907
-0.0131848 0.0194268
0.046799 0.00604863
0.0281392 0.0470098
0.0131858 0.0278944
0.0871095 0.0521547
0.00596546 0.0403852
0.0966595 0.0398762
0.0691072 0.033518
0.0384776 0.0407416
0.0424316 0.0419056
0.0594398 0.0514332
-0.0662843 0.0138103
0.126206 0.0380313
0.0461385 0.0501676
0.0236481 0.0457389
0.0188023 0.0172275
0.0977801 0.0451263
4.07609e-05 0.0145842
0.0346353 0.0116006
0.0752955 0.0388276
0.0133907 0.0533274
-0.0152606 0.0274146
0.0396776 0.0436492
0.0582389 0.0421916
0.0204143 0.0190631
-0.0529722 0.0168144
0.10665 0.0423284
0.0807987 0.0227551
0.0170933 0.0473396
0.0594397 0.055943
0.0131851 0.0118627
0.0907408 0.0509381
0.0562649 0.0443666
0.080392 0.0399021
0.0897923 0.0439613
0.059435 0.0448018
0.0594397 0.0563671
0.0368959 0.0395046
0.026809 0.0432824
0.0857183 0.0181797
0.0277564 0.0405683
0.0482639 0.0455156
0.0781223 0.0410697
-0.00484737 0.0372306
-0.0532643 0.0334107
0.0197008 0.0528114
-0.0385433 0.0176126
0.000807032 0.0161497
0.0562649 0.0387106
0.0281391 0.0535334
0.0730674 0.056881
-0.0611267 0.012992
0.020735 0.020282
0.0477957 0.0539968
0.0322983 0.0515159
-0.0807974 0.0214895
0.0384676 0.0383781
0.0243594 0.0410789
0.0234681 0.0146352
0.0290868 0.0501409
0.0734322 0.0145484
-0.0118767 0.0580978
0.0594376 0.0372245
0.0134334 0.0529463
0.0174335 0.0389738
-0.0267425 0.0175307
0.0966595 0.0398762
0.0594759 0.0409273
parameters: [ 8.496  0.92   3.953  1.422  6.546]. error: 5568116.03282.
----------------------------
epoch 0, loss 1.16169
epoch 128, loss 0.956567
epoch 256, loss 1.05233
epoch 384, loss 1.15209
epoch 512, loss 0.881654
epoch 640, loss 1.14675
epoch 768, loss 1.03989
epoch 896, loss 1.02007
epoch 1024, loss 0.87786
epoch 1152, loss 1.12178
epoch 1280, loss 1.28757
epoch 1408, loss 0.944003
epoch 1536, loss 0.90644
epoch 1664, loss 1.0081
epoch 1792, loss 0.817834
epoch 1920, loss 1.0232
epoch 2048, loss 0.909665
epoch 2176, loss 1.11677
epoch 2304, loss 1.05287
epoch 2432, loss 0.848612
epoch 2560, loss 0.888273
epoch 2688, loss 0.874981
epoch 2816, loss 0.756135
epoch 2944, loss 0.794555
epoch 3072, loss 0.829447
epoch 3200, loss 0.971784
epoch 3328, loss 0.789909
epoch 3456, loss 0.722749
epoch 3584, loss 0.984048
epoch 3712, loss 1.01083
epoch 3840, loss 0.836538
epoch 3968, loss 0.727413
epoch 4096, loss 0.774613
epoch 4224, loss 0.780727
epoch 4352, loss 1.01771
epoch 4480, loss 1.02393
epoch 4608, loss 0.734104
epoch 4736, loss 0.784133
epoch 4864, loss 0.856168
epoch 4992, loss 0.98634
epoch 5120, loss 0.747749
epoch 5248, loss 0.740359
epoch 5376, loss 0.967618
epoch 5504, loss 0.891194
epoch 5632, loss 0.857799
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0385421 0.0324394
0.0300757 0.0232816
0.094504 0.0546049
0.00894166 0.0204087
0.0594396 0.0535428
0.0594397 0.063673
-0.0494977 0.0177355
0.0593758 0.0507261
-0.0271193 0.0209203
1.01848e-06 0.0196087
0.0594482 0.0602728
0.0594349 0.0507264
0.0397476 0.0591942
-0.039089 0.0163428
-0.0188028 0.0154138
0.0385291 0.0169927
0.0819546 0.0488041
0.0911173 0.0545532
-0.00249422 0.0226965
0.0807983 0.0189884
0.0978866 0.0485232
-0.0209708 0.0227727
0.0691985 0.0636576
-0.00700272 0.0390116
-0.0707055 0.0226188
-0.00378297 0.0321481
0.0677309 0.0460868
0.0300757 0.0137546
0.0223972 0.0595415
0.0897923 0.0641984
0.0477957 0.0617501
0.0199306 0.0500585
0.014471 0.0319298
0.0362234 0.0249729
0.0133878 0.0525081
-0.0188605 0.0267362
3.36628e-05 0.0336252
0.0373484 0.055139
0.0385427 0.0267952
-0.0295746 0.0303942
0.0966596 0.0440639
-0.049248 0.0184376
0.0857789 0.0534956
0.0482638 0.0648493
-0.00735889 0.0412708
-0.0529659 0.0181785
0.0199307 0.0516831
0.081797 0.0556972
0.0813321 0.0504728
0.059435 0.0500009
4.57767e-08 0.0190152
-0.0144856 0.0322318
0.100097 0.0494496
0.0968255 0.0649035
0.0278229 0.0504656
0.0243594 0.0550665
0.0220537 0.0600492
0.0867333 0.0574835
0.0710837 0.0677726
0.00378707 0.0341907
-0.0529659 0.0228455
0.0223972 0.0535059
0.0532657 0.036323
0.0424215 0.0500088
-0.0662877 0.0170367
-1.48505e-07 0.0234385
-0.053257 0.030273
-0.00700262 0.0496168
0.0223968 0.0548893
0.0243594 0.0605424
0.0667826 0.0623889
0.0271109 0.0200041
0.059435 0.0500009
-0.0204101 0.0244008
0.0367961 0.0224179
0.0710836 0.0705855
-0.0716782 0.0233402
0.130005 0.0465732
-0.041635 0.023805
0.0662931 0.0226766
-0.0412731 0.0300016
0.0188018 0.0518504
-0.0416423 0.0302525
0.0414051 0.0374141
0.0611271 0.0175407
0.000835111 0.0209947
0.0593346 0.0445898
-0.0207292 0.0259109
0.0495763 0.0131212
-0.00728242 0.048122
0.0497671 0.0551084
0.0133907 0.0517501
-0.0207367 0.0286818
0.0901949 0.0512725
0.0904958 0.0523085
0.0532578 0.0333315
0.0321897 0.0451063
-0.0111732 0.0429307
0.032933 0.062274
0.0396777 0.0441463
0.0594396 0.0627275
0.0611197 0.0134505
0.0813321 0.0473964
-0.000801651 0.0230235
-0.041635 0.0391932
0.0209836 0.0231993
0.0964305 0.062241
0.0424316 0.0568146
0.0283785 0.053665
0.0370775 0.0589462
-0.0300731 0.0141905
-0.0210969 0.0304283
-0.0362226 0.0266262
0.0859466 0.0661993
0.0248076 0.0166861
0.00596587 0.0522294
0.0594396 0.0559631
0.101441 0.0548431
-0.0194973 0.0282973
0.0268195 0.0573182
0.0152616 0.0353821
0.109566 0.048562
0.020735 0.0277164
0.0677309 0.0483341
0.0449639 0.0328573
0.0594116 0.0452845
0.106629 0.0456257
0.0541227 0.0466443
parameters: [ 8.496  0.92   3.953  1.422  5.747]. error: 1.11208977584e+12.
----------------------------
epoch 0, loss 1.25439
epoch 128, loss 1.07618
epoch 256, loss 0.947794
epoch 384, loss 1.10158
epoch 512, loss 1.01861
epoch 640, loss 1.06195
epoch 768, loss 0.881966
epoch 896, loss 0.968938
epoch 1024, loss 1.04304
epoch 1152, loss 1.06821
epoch 1280, loss 0.939992
epoch 1408, loss 0.987939
epoch 1536, loss 1.21968
epoch 1664, loss 1.23161
epoch 1792, loss 1.19062
epoch 1920, loss 0.991543
epoch 2048, loss 0.98451
epoch 2176, loss 1.20217
epoch 2304, loss 0.962927
epoch 2432, loss 1.09047
epoch 2560, loss 0.930393
epoch 2688, loss 0.734879
epoch 2816, loss 1.05624
epoch 2944, loss 0.797958
epoch 3072, loss 0.862161
epoch 3200, loss 0.811426
epoch 3328, loss 0.87709
epoch 3456, loss 1.24169
epoch 3584, loss 0.941637
epoch 3712, loss 1.06102
epoch 3840, loss 1.1123
epoch 3968, loss 0.909878
epoch 4096, loss 0.841841
epoch 4224, loss 0.835293
epoch 4352, loss 0.780879
epoch 4480, loss 0.938794
epoch 4608, loss 0.820571
epoch 4736, loss 0.905044
epoch 4864, loss 1.01062
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.044975 0.0337987
0.0594297 0.0334669
0.0271163 0.02572
0.0662801 0.0245395
0.0867333 0.0434893
-0.00968441 0.0300592
-0.0144843 0.0335305
-0.0211014 0.0335266
-0.00628321 0.0344568
0.0594396 0.0339898
-0.0204122 0.0305921
0.0977799 0.0346739
0.0424316 0.0333782
0.129993 0.038488
0.0860428 0.0297779
0.0911173 0.0350173
-0.013282 0.0375174
-0.00142113 0.0366363
0.0424268 0.0381973
0.0901952 0.0400653
0.0496683 0.0358968
0.0859467 0.0370007
0.0594494 0.0362654
-1.48505e-07 0.0320022
0.0416434 0.0288924
0.0335492 0.0320385
0.0318693 0.0325058
0.0496683 0.0325716
0.0109586 0.0327709
0.0867333 0.0433428
0.0594396 0.0323412
-0.0495799 0.0314122
-0.00331175 0.0319701
0.0857794 0.0282923
-2.99285e-06 0.035394
0.0340225 0.0282891
0.0290868 0.0307504
-0.0207342 0.0321448
0.0321467 0.0421089
0.0594399 0.0297651
0.0589127 0.0333556
0.088729 0.0422984
0.0593665 0.0385778
-0.0385424 0.0359047
0.0675205 0.0312662
0.0131849 0.0277666
0.0318695 0.0333028
0.000834782 0.0373563
0.134985 0.0385752
0.0594399 0.0379185
0.0468064 0.0325318
-0.0295823 0.0332336
0.0487559 0.0367405
0.0131851 0.0302042
0.0594325 0.0378826
0.081527 0.0321589
-0.0414014 0.0281682
0.0594919 0.0384605
0.105471 0.0338812
-0.0394796 0.029944
0.0599031 0.0414509
0.0205116 0.0342851
0.0562649 0.0384845
0.121243 0.0373934
-0.0272977 0.0340109
-0.053266 0.0300866
-0.016115 0.0354348
-0.0417573 0.0374371
0.100667 0.0377959
0.0318766 0.025048
0.0968257 0.0362101
-0.0467928 0.0284337
0.0971047 0.038806
-0.00628221 0.0345725
-0.0204122 0.0283642
0.0851612 0.034932
0.0385434 0.0315171
0.0168394 0.0398431
0.0986862 0.0368797
-0.0023636 0.0300896
0.0496681 0.0333167
-0.0272977 0.0337093
-0.129449 0.0356308
0.0247829 0.0364272
0.0910569 0.0326777
0.0659615 0.0295923
0.0174335 0.0365685
0.0941706 0.0434161
-0.0860447 0.0336291
0.0272988 0.0320761
0.0390895 0.0297723
0.0272894 0.0336384
0.0538297 0.0388916
-0.0271157 0.0244469
0.0716781 0.029249
0.00459996 0.0310625
-0.0462049 0.0340132
0.0594397 0.0326578
0.0407513 0.041564
0.0220539 0.0358918
0.0859465 0.0272045
0.0384777 0.0378423
-0.0346416 0.0314522
0.0977801 0.0379104
0.121243 0.0374386
-0.0417491 0.0295199
0.0234704 0.0243823
-0.0271193 0.0291565
-0.0300731 0.0279826
0.0920561 0.0321865
-0.041769 0.0332094
0.0267419 0.0283169
0.0582651 0.0322823
0.109566 0.0434804
0.121243 0.0346564
-0.053266 0.0337498
-0.00699273 0.0296129
0.0594759 0.0332312
-0.0271193 0.0260624
0.0749536 0.0363098
-0.0144843 0.0335219
0.0496682 0.0372837
0.0191272 0.0355901
0.0188015 0.03774
-1.63011e-07 0.0326087
-0.0248069 0.0296863
0.0594349 0.0329418
0.101441 0.0371364
parameters: [ 8.496  0.92   3.953  1.422  4.928]. error: 134.18984554.
----------------------------
epoch 0, loss 1.16011
epoch 128, loss 1.16265
epoch 256, loss 1.15689
epoch 384, loss 1.27011
epoch 512, loss 1.11624
epoch 640, loss 1.24154
epoch 768, loss 1.04387
epoch 896, loss 1.25704
epoch 1024, loss 0.805944
epoch 1152, loss 1.05089
epoch 1280, loss 1.01442
epoch 1408, loss 1.01803
epoch 1536, loss 0.84551
epoch 1664, loss 1.02337
epoch 1792, loss 1.07331
epoch 1920, loss 1.1948
epoch 2048, loss 0.962643
epoch 2176, loss 0.94534
epoch 2304, loss 1.2495
epoch 2432, loss 0.799069
epoch 2560, loss 0.942409
epoch 2688, loss 0.957943
epoch 2816, loss 0.852946
epoch 2944, loss 1.11015
epoch 3072, loss 0.940804
epoch 3200, loss 0.961827
epoch 3328, loss 0.982939
epoch 3456, loss 1.04424
epoch 3584, loss 1.14124
epoch 3712, loss 0.909683
epoch 3840, loss 1.21079
epoch 3968, loss 1.07177
epoch 4096, loss 1.05168
epoch 4224, loss 0.858531
epoch 4352, loss 0.884613
epoch 4480, loss 0.972646
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0271152 0.0181439
-0.0271157 0.0174216
0.0538009 0.0316286
0.0290868 0.0451926
0.0337182 0.0254446
-0.00628221 0.0339548
-1.99995e-06 0.0256724
-0.0462076 0.0316282
-0.0462076 0.0291621
0.130756 0.0431063
0.0983621 0.0450787
0.0807993 0.0208368
-0.0385431 0.027768
0.129993 0.0211323
-0.000835377 0.0149676
0.0989678 0.0299452
0.0851609 0.0503164
0.0170834 0.0258052
0.0322983 0.0383334
0.0907411 0.0460822
0.0764328 0.0258273
0.0594398 0.0473854
0.101441 0.0378554
0.106655 0.0403113
-0.049248 0.0141142
0.000213748 0.0231648
0.056285 0.0442721
0.0920613 0.0201733
0.0977801 0.0237764
0.0261395 0.0498129
0.0394799 0.0179255
0.0650734 0.0381368
0.028139 0.0326772
0.0595095 0.0166471
0.0188603 0.0329609
-2.15513e-06 0.0167268
0.0464005 0.0329479
0.0611204 0.0189298
0.0734439 0.02709
0.0950805 0.0400122
0.0691985 0.0330218
0.0691985 0.0276388
0.0865811 0.0374115
0.0594204 0.0338647
0.0322982 0.0382368
0.0859465 0.0291973
0.0267443 0.0284109
0.0117845 0.021515
0.0191276 0.0208443
0.0589764 0.0360745
0.090741 0.0405381
0.0117785 0.0206685
0.0594398 0.0355737
0.0861488 0.0383726
0.0407513 0.0394544
0.0594397 0.0380296
0.059435 0.0209862
0.0730675 0.0389634
0.0177241 0.0246884
-0.0234744 0.0373755
-0.000835048 0.0113489
0.102035 0.0237164
-0.0385433 0.0243951
0.0237987 0.0311325
0.081797 0.0284306
-0.0189887 0.0184132
0.0373475 0.0396278
-0.0188605 0.0171427
0.0791961 0.0313137
-0.0529722 0.0190431
-0.053266 0.0281113
0.0867335 0.035258
-0.00728242 0.0190926
0.100097 0.0303107
0.0390889 0.0197001
0.0594397 0.0402609
0.0424316 0.0246966
0.0861486 0.0325891
0.0327314 0.0410155
0.074953 0.0394274
-0.0113373 0.0284899
0.0907408 0.0412396
0.059435 0.028084
0.0989675 0.0225609
0.0217931 0.0406749
0.0321466 0.0383836
0.088282 0.0272461
0.046209 0.0308852
0.0131845 0.0353374
-0.0318771 0.0174536
0.0989675 0.0275283
0.000835126 0.0310954
0.0117845 0.0157406
0.0194912 0.0352653
0.0727412 0.0309273
0.0594396 0.0296841
0.0724741 0.0399459
0.0594395 0.0459018
-0.0132965 0.0328377
0.0927398 0.0451198
0.126086 0.0301886
0.014484 0.0126562
0.135027 0.0311665
0.00143008 0.0179343
0.0211183 0.0258199
0.0424316 0.0379192
0.0329332 0.0419938
0.0857284 0.0119146
0.0927396 0.0252266
0.0223968 0.0381915
-0.00701261 0.0437058
-0.0385424 0.0107136
0.0851613 0.037107
0.0582589 0.0304644
-0.00735889 0.0244779
0.000782366 0.0266776
0.0867335 0.0415148
-0.00700262 0.0278773
0.0631207 0.0320569
0.100097 0.0289383
3.62028e-05 0.022797
-0.0177216 0.0336655
-0.00331175 0.02357
0.038792 0.0304213
0.109549 0.046863
0.0281392 0.0421276
0.0594396 0.0405307
0.0199306 0.0340075
parameters: [ 8.496  0.92   3.953  1.422  4.546]. error: 99504.4068219.
----------------------------
epoch 0, loss 1.20245
epoch 128, loss 1.10414
epoch 256, loss 1.1436
epoch 384, loss 1.27018
epoch 512, loss 1.0523
epoch 640, loss 0.990303
epoch 768, loss 0.950428
epoch 896, loss 1.00159
epoch 1024, loss 1.10713
epoch 1152, loss 1.0087
epoch 1280, loss 0.804803
epoch 1408, loss 0.683962
epoch 1536, loss 0.964684
epoch 1664, loss 0.943216
epoch 1792, loss 0.868669
epoch 1920, loss 1.00882
epoch 2048, loss 0.988364
epoch 2176, loss 0.913158
epoch 2304, loss 0.923447
epoch 2432, loss 1.00833
epoch 2560, loss 0.839832
epoch 2688, loss 1.02813
epoch 2816, loss 0.883499
epoch 2944, loss 1.05688
epoch 3072, loss 0.863906
epoch 3200, loss 0.847868
epoch 3328, loss 0.834621
epoch 3456, loss 0.966654
epoch 3584, loss 0.86252
epoch 3712, loss 0.88788
epoch 3840, loss 0.69522
epoch 3968, loss 0.694997
epoch 4096, loss 0.749046
epoch 4224, loss 0.677724
epoch 4352, loss 0.751385
epoch 4480, loss 0.865982
epoch 4608, loss 1.00174
epoch 4736, loss 0.991073
epoch 4864, loss 0.944714
epoch 4992, loss 0.906212
epoch 5120, loss 0.821919
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-2.95455e-07 0.0313208
0.0477957 0.0557784
-1.17339e-07 0.0262811
0.10665 0.0484753
0.0462156 0.0218586
-0.0716686 0.00656141
0.0855722 0.047556
0.0997468 0.0479267
-0.0117817 0.0147706
-0.00131975 0.0163265
-0.0318748 0.0240294
0.0691068 0.0540248
-0.0250779 0.0409657
-0.0414059 0.0185908
0.0234638 0.0141978
0.046799 0.0254219
0.0592247 0.046947
0.105471 0.0544314
0.057604 0.0578047
0.0594396 0.0533235
0.0661223 0.0496513
0.0223968 0.0550453
0.0983621 0.0514116
-0.00701261 0.0424603
0.0710837 0.0629675
0.0910567 0.0526012
0.0662803 0.0321342
0.0594273 0.0497932
0.0340225 0.0159477
0.0482639 0.0497808
0.0210981 0.0187321
0.00459996 0.0170409
0.0251234 0.0481725
0.0168394 0.0477304
0.090741 0.0614634
0.0594397 0.056113
0.0871097 0.0583351
0.0286789 0.0486792
0.0594396 0.0511014
0.0907409 0.062315
0.0594397 0.0570773
0.0868849 0.0564767
-0.0417422 0.00797532
-0.0111692 0.0466229
0.0626054 0.0474262
0.0867201 0.0575984
0.0305923 0.0492454
-0.00628221 0.0521864
0.0593665 0.0430322
0.0329332 0.057361
0.0208245 0.0374779
0.0631207 0.00265493
0.0326469 0.0140908
-0.00236357 0.0594659
0.00142251 0.0206674
0.125157 0.0562529
0.101441 0.0533611
0.0305923 0.0550312
5.31452e-07 0.0351419
0.0791961 0.049682
0.0122142 0.0458485
0.121243 0.0581011
0.0188019 0.0512429
0.0278228 0.05532
0.0594397 0.0596139
-2.86114e-05 0.011656
0.0375434 0.0506699
0.0337182 0.0540813
-0.0204101 0.0159329
0.0131845 0.0201898
-0.0111732 0.0481014
0.0594466 0.0522236
0.0182076 0.0640125
0.0594396 0.0524699
-0.0208232 0.0109912
0.0594397 0.0534005
0.00856342 0.0111982
0.0861486 0.0569642
0.110244 0.0505279
0.0496681 0.0496702
0.0594398 0.0582686
0.0412725 0.0270866
0.0461385 0.0511809
0.0322983 0.0571713
0.0464005 0.0474619
0.0144846 0.0382165
0.0300726 0.0186738
0.0927397 0.0585397
-0.0234699 0.00986014
0.093775 0.0556648
0.0317697 0.0526894
0.0599029 0.0571986
0.0424268 0.053586
0.0691985 0.047848
-0.0215356 0.0323098
0.0461385 0.0499493
0.143948 0.0508264
0.0327314 0.0589246
-0.0326435 0.014557
-0.10011 0.0187242
0.0300757 0.00765354
-0.0716787 0.0184344
-0.00891927 0.0175353
0.000793148 0.0330391
0.0631163 0.0193868
0.0594397 0.0493538
0.0188086 0.0187654
-0.0113473 0.00960858
0.0210915 0.0272896
0.0594324 0.0451356
0.0468064 0.0364048
0.0621339 0.0152724
0.00142251 0.0310462
0.0412752 0.0228445
0.124277 0.0410234
0.0631173 0.0255318
-0.000801651 0.0199077
0.0330542 0.0430256
0.0248097 0.0391125
0.0594402 0.0486395
-0.0131858 0.022702
-0.0109842 0.0207832
2.41162e-06 0.0138188
-0.129449 0.00616063
0.0370775 0.0445968
0.0199307 0.0473402
0.0887289 0.0553067
0.0920969 0.0522833
parameters: [ 8.496  0.92   3.953  1.422  5.241]. error: 11243582765.4.
----------------------------
epoch 0, loss 1.06768
epoch 128, loss 1.04237
epoch 256, loss 1.01133
epoch 384, loss 0.957547
epoch 512, loss 1.13361
epoch 640, loss 0.96602
epoch 768, loss 0.852877
epoch 896, loss 1.01815
epoch 1024, loss 0.963947
epoch 1152, loss 1.10628
epoch 1280, loss 1.00568
epoch 1408, loss 0.868959
epoch 1536, loss 0.978936
epoch 1664, loss 0.88028
epoch 1792, loss 0.847176
epoch 1920, loss 1.06143
epoch 2048, loss 1.00871
epoch 2176, loss 0.947553
epoch 2304, loss 1.07586
epoch 2432, loss 0.881138
epoch 2560, loss 1.06969
epoch 2688, loss 0.798975
epoch 2816, loss 0.911872
epoch 2944, loss 0.909812
epoch 3072, loss 0.839633
epoch 3200, loss 0.706441
epoch 3328, loss 0.752866
epoch 3456, loss 0.782044
epoch 3584, loss 0.925313
epoch 3712, loss 0.878163
epoch 3840, loss 0.86527
epoch 3968, loss 0.797075
epoch 4096, loss 0.825925
epoch 4224, loss 0.785175
epoch 4352, loss 0.707869
epoch 4480, loss 0.736985
epoch 4608, loss 0.802783
epoch 4736, loss 0.844663
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.00460075 0.0078514
0.0417484 0.0162783
0.0261395 0.0584066
0.0527576 0.0399059
0.0482639 0.0545222
0.0594397 0.05205
0.0122243 0.0426961
-0.053266 0.011093
0.0271289 0.0226222
-0.00459356 0.00980387
0.0174339 0.0531984
0.0865811 0.048087
0.0594273 0.0435361
0.0117847 0.00850288
0.0237989 0.0341731
0.0907411 0.0472746
-0.0215238 0.0157543
-0.0385437 0.0150441
0.0188609 0.0365064
0.0710838 0.0470935
0.0375434 0.0478412
0.126206 0.0443979
0.076422 0.0576636
0.026809 0.0530831
0.0594396 0.0453895
-0.0707068 0.0276718
0.0416434 0.0119066
0.0791961 0.0504057
0.0412702 0.0224085
0.01134 0.0143188
-2.95455e-07 0.0173533
0.028139 0.0383727
0.0904958 0.0419244
0.00931417 0.0404193
-3.11149e-07 0.0202932
0.0131861 0.020725
0.0593346 0.0499314
0.0461382 0.0435652
0.0290868 0.049175
0.0122352 0.0510578
0.118684 0.0455636
0.0819546 0.0532972
0.0407515 0.0504061
0.100667 0.0520856
0.0734322 0.021035
0.0791969 0.0370889
0.0941706 0.0641161
0.0594427 0.0573109
0.0197008 0.056392
-3.11621e-07 0.0167705
0.0538009 0.0369062
0.0495763 0.0210599
0.0384776 0.0465312
-0.041769 0.0161799
0.0594396 0.0460167
0.0764328 0.058695
-0.0118764 0.0320672
0.0594162 0.0415373
-0.0188599 0.00909528
0.0220537 0.0471865
0.000786701 0.0117538
0.0414025 0.0194186
0.0189892 0.0265195
-0.0118765 0.0342538
0.0677309 0.0415846
0.0791268 0.0514794
0.0707072 0.0184824
-0.036223 0.0178364
0.0222151 0.044102
-0.0189848 0.0130293
0.0394799 0.0135407
0.0927399 0.0508541
-0.0659495 0.0175435
0.0631173 0.0127932
0.0337182 0.0431411
0.109549 0.050236
0.0326469 0.0212198
-0.0532643 0.00877527
0.0300726 0.00450842
0.0467944 0.0218734
0.100667 0.0520856
0.056285 0.0430938
-0.10011 0.0146951
0.0222152 0.0310875
0.0168394 0.039718
0.0855721 0.063573
-0.0131854 0.0204286
8.39808e-07 0.0262389
0.0599029 0.0489457
0.0730675 0.0394986
0.0676335 0.041961
0.093775 0.0643982
0.0594395 0.0577843
0.106645 0.0488058
-0.0109774 0.0113514
0.0337178 0.0421524
0.0791268 0.0498896
0.0897923 0.05761
0.0562648 0.0448049
-0.00891927 0.0149764
0.0594397 0.0455408
0.0966596 0.0456562
0.0964766 0.0495033
-0.0111018 0.0490242
0.0882823 0.0512758
0.0991788 0.0438382
0.0807993 0.0236549
0.0691985 0.0481266
-0.0417573 0.0199006
0.0911173 0.0416663
-0.0271157 0.00768787
0.00378707 0.0268811
-0.00142797 0.0144427
0.0373475 0.0568146
0.000796249 0.0128085
0.0776855 0.0173783
-0.00699273 0.059165
-0.0248182 0.0131118
-0.014471 0.028904
0.0952317 0.0497096
-0.00484737 0.0400309
-2.23337e-05 0.012641
0.0927397 0.0475134
0.0327313 0.0379508
0.0901952 0.0431767
0.0144707 0.0227603
0.0791268 0.0497926
0.0301505 0.0575764
parameters: [ 8.496  0.92   3.953  1.422  4.737]. error: 1725501001.48.
----------------------------
epoch 0, loss 1.63584
epoch 128, loss 1.38296
epoch 256, loss 1.28065
epoch 384, loss 1.44908
epoch 512, loss 1.29149
epoch 640, loss 1.20435
epoch 768, loss 1.05698
epoch 896, loss 1.09059
epoch 1024, loss 0.901052
epoch 1152, loss 1.17251
epoch 1280, loss 0.972407
epoch 1408, loss 1.05029
epoch 1536, loss 0.985673
epoch 1664, loss 1.02657
epoch 1792, loss 1.09047
epoch 1920, loss 0.894287
epoch 2048, loss 0.792064
epoch 2176, loss 1.0777
epoch 2304, loss 1.09473
epoch 2432, loss 0.865349
epoch 2560, loss 1.13218
epoch 2688, loss 0.922136
epoch 2816, loss 0.77066
epoch 2944, loss 0.893446
epoch 3072, loss 0.954015
epoch 3200, loss 0.8816
epoch 3328, loss 0.712112
epoch 3456, loss 0.888465
epoch 3584, loss 1.07127
epoch 3712, loss 1.09953
epoch 3840, loss 0.93039
epoch 3968, loss 0.87673
epoch 4096, loss 0.915617
epoch 4224, loss 0.919572
epoch 4352, loss 1.10696
epoch 4480, loss 0.943681
epoch 4608, loss 1.07184
epoch 4736, loss 0.899571
epoch 4864, loss 0.928257
epoch 4992, loss 1.03412
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-4.71368e-07 0.0256871
0.126206 0.027186
-0.000807624 0.0161247
0.0594426 0.0302302
0.0599029 0.0516367
-0.0412711 0.0263732
-7.68032e-07 0.00652835
-0.0210947 0.0159154
0.0281391 0.0371915
-3.11149e-07 0.0209562
0.0321466 0.0580349
0.00596587 0.0394389
0.0319947 0.0259801
0.0827282 0.00971001
0.0416434 0.0129913
0.0496681 0.0482201
-0.0295559 0.0279675
0.00931426 0.0477458
0.0204119 0.0127572
-2.95455e-07 0.0148484
-0.0385431 0.0275791
0.0234638 0.0310851
0.0594398 0.0446852
2.41162e-06 0.00891413
0.0396778 0.0354175
2.5668e-06 0.0271216
-0.0234744 0.0194534
0.0605534 0.0383329
0.0593665 0.0342597
-0.00726943 0.0355584
-0.129457 0.0121842
0.00378707 0.00748413
0.049499 0.013379
0.0870049 0.0414292
-0.00700272 0.0276459
-0.0209708 0.0258407
-0.129457 0.00677331
0.0611271 0.011127
0.032902 0.027303
-0.0234626 0.0131755
0.0851612 0.0566956
0.043628 0.0268093
0.0764272 0.0281483
0.0417583 0.0271999
0.0594397 0.0538448
0.0236481 0.0403616
0.0706155 0.0489956
1.74905e-07 0.0225355
0.0461384 0.0528984
0.0710836 0.0505371
0.0592247 0.0369839
0.0281393 0.0321999
-0.00378297 0.0266717
-0.0412711 0.0325624
-0.0414033 0.0258647
0.0477956 0.0596155
0.0977799 0.0395559
0.0650733 0.0408172
0.0435639 0.0366293
0.0407515 0.04336
1.74905e-07 0.0255786
0.0997462 0.0258223
0.0468064 0.0292161
0.0861488 0.0526591
0.0592543 0.0338509
0.0776855 0.00920473
0.0174335 0.0408301
0.101441 0.0443102
0.109566 0.0553446
0.0144704 0.0129928
0.0261395 0.0585681
0.121243 0.0533589
0.0207193 0.0396064
0.0189868 0.0136942
-0.0385421 0.00937799
0.0496682 0.0397581
0.0283785 0.0383398
0.0752422 0.038906
2.9023e-05 0.0184365
0.0375434 0.0348879
0.0407515 0.0481678
0.0594399 0.0443124
0.0495763 0.0277821
0.0300726 0.0181543
0.0416418 0.0100777
-0.0272977 0.0100206
0.0220537 0.0544987
0.0734366 0.0160402
0.0261392 0.0603772
0.0477956 0.0553006
0.0290868 0.0550462
0.0267779 0.0493795
0.0267393 0.0247719
0.0217933 0.0299122
0.0901949 0.0338006
0.0152591 0.0138842
0.130005 0.0436071
0.076422 0.027463
-0.0132848 0.0290738
0.0278228 0.0459644
-0.0807974 0.02465
0.000213748 0.041548
0.125883 0.0404938
0.0538009 0.026577
0.0594027 0.0333326
0.0144859 0.00798138
-0.0413988 0.0179939
-0.00459356 0.0241746
0.0978866 0.0407465
0.0739227 0.0307033
0.0237987 0.0404341
0.0436381 0.0363717
0.081797 0.0418098
0.0131845 0.0296953
0.143948 0.0303552
0.0594919 0.0433114
0.05937 0.0317615
0.0594399 0.0456335
0.0691068 0.0384578
0.0396777 0.0268261
0.0300726 0.0109527
0.0611197 0.00861877
0.0594325 0.036016
0.0865811 0.0565276
0.0594396 0.0463203
0.0329331 0.032415
0.0234754 0.0152232
0.0234638 0.02214
parameters: [ 8.496  0.92   3.953  1.422  5.048]. error: 81787495002.7.
----------------------------
epoch 0, loss 1.1365
epoch 128, loss 1.47944
epoch 256, loss 1.16541
epoch 384, loss 1.14246
epoch 512, loss 1.2654
epoch 640, loss 1.42799
epoch 768, loss 1.12091
epoch 896, loss 0.991836
epoch 1024, loss 0.960746
epoch 1152, loss 1.06777
epoch 1280, loss 1.00818
epoch 1408, loss 1.28728
epoch 1536, loss 1.08563
epoch 1664, loss 1.01691
epoch 1792, loss 1.22202
epoch 1920, loss 1.00356
epoch 2048, loss 1.1302
epoch 2176, loss 1.06813
epoch 2304, loss 0.999409
epoch 2432, loss 1.0275
epoch 2560, loss 1.05034
epoch 2688, loss 1.12463
epoch 2816, loss 1.04427
epoch 2944, loss 0.966926
epoch 3072, loss 0.933099
epoch 3200, loss 1.06836
epoch 3328, loss 0.832381
epoch 3456, loss 0.843844
epoch 3584, loss 1.03885
epoch 3712, loss 1.04707
epoch 3840, loss 0.992593
epoch 3968, loss 0.89419
epoch 4096, loss 1.08625
epoch 4224, loss 1.0757
epoch 4352, loss 0.973182
epoch 4480, loss 0.837643
epoch 4608, loss 1.0778
epoch 4736, loss 1.09111
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0267425 0.0329111
-0.0271111 0.0230741
0.0109517 0.0273419
0.0589763 0.0317541
0.0247806 0.0370717
0.0295687 0.0197744
0.0390889 0.0375903
0.0643397 0.0305051
0.0405236 0.0307393
-0.0189848 0.0313661
2.96104e-08 0.027861
-6.0681e-07 0.0229778
0.0861488 0.0316768
0.0204045 0.0313331
0.0177216 0.0336074
0.0122194 0.0348685
-0.036223 0.0371342
0.0497674 0.037538
0.0197008 0.0403355
0.05943 0.0293507
0.000213351 0.0263103
0.0594399 0.0390852
0.0594301 0.0294168
-0.00549668 0.0314803
0.0594396 0.0414985
0.0910569 0.0473659
0.0300757 0.0277945
-0.0417792 0.0359948
-0.000801651 0.0360486
0.102035 0.0329459
-0.0857218 0.0306218
0.0727409 0.0278744
-1.92327e-05 0.0202004
0.0204143 0.0330175
0.0541227 0.0304449
0.100097 0.0293768
0.0461384 0.0395273
0.0201879 0.0358671
0.0659615 0.0240218
0.0621308 0.0275991
-0.0662833 0.0276092
-0.00142113 0.0280894
0.0707062 0.0322447
0.100097 0.0230258
0.0261395 0.0216965
0.135017 0.0258762
0.110244 0.0331075
0.0867332 0.0374711
-0.080798 0.0377802
0.0937752 0.0349937
-0.0188057 0.0311863
-0.0716782 0.0276267
0.0538297 0.0321563
0.018802 0.0326797
0.0977799 0.0368077
4.57767e-08 0.0227636
0.0661224 0.0353987
-4.4075e-07 0.0353587
0.0815268 0.0390009
0.0301505 0.0355883
0.018802 0.0282502
0.0594301 0.0294168
0.0882823 0.0380627
-0.00236343 0.0313315
0.0977801 0.0369842
0.0594402 0.0282829
0.135017 0.0379574
0.00080867 0.0347748
0.0323163 0.0298689
-0.0417488 0.0310298
-2.86114e-05 0.0167588
0.00131924 0.0311161
0.059435 0.0302834
0.125883 0.0310427
-0.0271157 0.0307928
0.0261394 0.0224635
0.0631196 0.0267996
0.062595 0.0308965
-0.0631103 0.0348138
-0.0271278 0.0230444
-0.0631101 0.0272647
-0.00236348 0.0317086
0.05943 0.0293259
0.0281393 0.0327551
-0.0272977 0.0272371
-0.0776845 0.0304893
0.0594116 0.0299366
0.022444 0.0383374
0.0907408 0.0403715
0.00863029 0.030807
0.0122142 0.0389566
-0.0204101 0.0362977
0.0594297 0.0328728
-0.0111821 0.039324
0.0117782 0.0331082
0.121243 0.040966
0.0966587 0.0352486
0.081527 0.0363983
0.110244 0.0327012
0.0321632 0.0411556
-0.0412685 0.0199053
0.110244 0.0327542
0.0326439 0.0270357
-0.0132848 0.0289288
0.0807983 0.0303002
0.0188597 0.0279106
0.0868848 0.0369346
-0.0346352 0.0354356
-0.0659495 0.0270056
0.0867335 0.0408071
0.0217931 0.0251567
0.0626002 0.029438
0.0594399 0.0238429
0.126086 0.030101
0.0482639 0.0294865
0.0439209 0.0347867
-0.0707068 0.0356779
-0.0414059 0.0379872
-3.11621e-07 0.0291744
0.0317697 0.0317049
-0.00484737 0.030607
0.0317697 0.0287526
0.100116 0.0306014
0.0901952 0.0403236
0.109566 0.0327552
0.0261394 0.0401313
0.0621339 0.023882
0.0305923 0.0292487
parameters: [ 8.496  0.92   3.953  1.422  4.855]. error: 8.587785192e+12.
----------------------------
epoch 0, loss 0.986488
epoch 128, loss 1.23709
epoch 256, loss 0.992851
epoch 384, loss 0.994386
epoch 512, loss 0.990065
epoch 640, loss 1.18773
epoch 768, loss 0.943583
epoch 896, loss 1.00431
epoch 1024, loss 0.977317
epoch 1152, loss 1.18725
epoch 1280, loss 0.986083
epoch 1408, loss 1.03976
epoch 1536, loss 1.01572
epoch 1664, loss 0.985626
epoch 1792, loss 1.00851
epoch 1920, loss 0.984097
epoch 2048, loss 0.933502
epoch 2176, loss 1.08394
epoch 2304, loss 0.909447
epoch 2432, loss 1.16771
epoch 2560, loss 0.928063
epoch 2688, loss 1.14767
epoch 2816, loss 0.912893
epoch 2944, loss 1.09973
epoch 3072, loss 0.992671
epoch 3200, loss 0.984079
epoch 3328, loss 0.843406
epoch 3456, loss 0.913492
epoch 3584, loss 0.934446
epoch 3712, loss 0.804564
epoch 3840, loss 0.879881
epoch 3968, loss 0.774698
epoch 4096, loss 0.872209
epoch 4224, loss 0.972659
epoch 4352, loss 0.955199
epoch 4480, loss 0.845864
epoch 4608, loss 0.818822
epoch 4736, loss 0.739141
epoch 4864, loss 0.978464
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0791961 0.0394834
0.0764118 0.038486
0.0267419 0.024817
0.0594398 0.0396288
0.0871097 0.036545
0.0211183 0.0455693
0.0243586 0.0428612
0.0390895 0.0208385
0.0582651 0.044863
0.0204028 0.0138396
-0.00700262 0.0406947
0.0822309 0.044766
0.09274 0.0405208
0.0199306 0.0480102
0.0910567 0.0449547
0.0247038 0.0364645
-0.00236343 0.0441332
0.0286795 0.0441482
0.0201879 0.0460457
-0.02083 0.0189586
-0.0416468 0.0315057
0.0482638 0.0404461
0.129445 0.0213292
0.0286789 0.0443096
-0.0365701 0.0197504
0.0122404 0.0370485
-0.0271126 0.014339
0.0878297 0.0400039
0.0594399 0.0473285
0.036223 0.02249
0.0648671 0.0428423
-0.0416468 0.0197877
-6.66928e-06 0.0268226
-0.0152533 0.0185967
2.27454e-05 0.0296026
0.109549 0.0337352
0.0800819 0.0383267
0.0904958 0.0388746
0.028139 0.0419929
0.0122504 0.0406565
0.0739226 0.0494079
0.0764328 0.0358755
0.0594398 0.0460033
0.0416434 0.0385141
-0.0611267 0.023245
0.057604 0.0498842
0.0981549 0.0366086
-0.0234677 0.0311503
0.0322983 0.047392
0.0370769 0.0375172
-0.0495734 0.0167841
0.0592543 0.0409594
0.0867119 0.0386445
0.0449639 0.0274338
0.0191272 0.0367235
0.0691068 0.0388148
0.026809 0.0420253
-0.0734426 0.0251615
0.0122194 0.0319453
0.0861486 0.0466367
0.0594397 0.0430915
-0.0117817 0.0143122
0.0368964 0.042764
0.0300757 0.0115945
-0.0211014 0.0318798
-0.053257 0.0341311
-4.71368e-07 0.0192226
-0.0109774 0.0246555
0.00894003 0.0248465
0.032195 0.044927
0.0997468 0.0413638
-0.0113401 0.0226362
-0.0234626 0.0318784
0.0907409 0.046635
0.026809 0.0397967
0.0716719 0.0138323
0.124311 0.0427597
0.0867067 0.0367875
0.0968257 0.0435874
-0.0131848 0.0238262
3.62028e-05 0.0183301
-0.00549668 0.0428066
0.0882823 0.0353924
0.0989675 0.0433713
-2.99285e-06 0.0233779
0.023648 0.0420175
-0.0495737 0.0208966
0.0373475 0.0458758
-0.0390898 0.0165099
0.0497673 0.0415265
0.0290868 0.0440734
0.0477959 0.0479202
0.12945 0.0174571
0.0189828 0.0196482
0.0594919 0.0468168
0.0527575 0.0420543
-0.053257 0.0238871
0.0594399 0.0473285
-0.0023636 0.040405
-0.0492483 0.0236868
0.118684 0.0498467
0.0174339 0.0375914
-0.0417792 0.0197534
0.0605534 0.0450629
0.0981543 0.0403681
-0.0468045 0.0204801
0.0857794 0.0438356
0.0468064 0.0185307
-0.00131975 0.0165175
0.0594396 0.0393179
0.0764328 0.0358755
0.0189892 0.0243791
0.10664 0.036376
0.0781227 0.0369024
0.0109517 0.026625
-0.0271152 0.018263
0.0871095 0.0338756
-0.0210896 0.0187836
0.110244 0.0439026
0.0335493 0.0456293
0.0966595 0.0286988
0.0859466 0.0436006
-0.0611177 0.0227431
0.112932 0.0469802
0.0497673 0.0382533
0.0819546 0.0423646
-0.0362233 0.00776488
0.0283783 0.0353369
parameters: [ 8.496  0.92   3.953  1.422  4.974]. error: 611072.095619.
----------------------------
epoch 0, loss 1.23702
epoch 128, loss 1.13485
epoch 256, loss 1.18671
epoch 384, loss 1.14075
epoch 512, loss 1.3254
epoch 640, loss 1.29414
epoch 768, loss 1.10164
epoch 896, loss 1.27416
epoch 1024, loss 1.24823
epoch 1152, loss 0.964202
epoch 1280, loss 0.859044
epoch 1408, loss 1.15721
epoch 1536, loss 1.19906
epoch 1664, loss 0.965073
epoch 1792, loss 1.1825
epoch 1920, loss 1.05811
epoch 2048, loss 0.956534
epoch 2176, loss 1.24826
epoch 2304, loss 1.18283
epoch 2432, loss 1.08717
epoch 2560, loss 0.968158
epoch 2688, loss 0.889757
epoch 2816, loss 1.23837
epoch 2944, loss 0.956812
epoch 3072, loss 1.01801
epoch 3200, loss 1.27761
epoch 3328, loss 1.23697
epoch 3456, loss 1.11952
epoch 3584, loss 1.07567
epoch 3712, loss 0.932028
epoch 3840, loss 0.854527
epoch 3968, loss 1.04675
epoch 4096, loss 1.18144
epoch 4224, loss 1.01345
epoch 4352, loss 0.933011
epoch 4480, loss 0.987598
epoch 4608, loss 0.869394
epoch 4736, loss 1.05495
epoch 4864, loss 0.920134
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0945151 0.0433995
0.0592247 0.0403037
0.0594398 0.043282
0.0272894 0.0440482
0.0691987 0.0438319
-0.0234744 0.0401492
-0.0631103 0.0400474
0.0594396 0.0380997
-0.0385424 0.0424649
-0.0385431 0.0443313
0.0594399 0.0414829
-0.0734361 0.0441911
2.27454e-05 0.0403483
0.0594397 0.0422744
0.0188019 0.0421139
0.020735 0.0433236
0.126086 0.0408127
0.10955 0.0453294
0.0870049 0.0428074
-0.129457 0.0412151
-0.0177216 0.0413851
0.0594427 0.0421799
-0.0362233 0.0394201
0.0322983 0.0421498
0.0621339 0.0403416
0.0174339 0.0431393
-0.039089 0.0394625
-0.0318771 0.0402343
0.0981543 0.0412734
0.0897923 0.0431238
-0.0412711 0.0449366
0.0204143 0.0423823
0.0859865 0.0412235
-0.0109489 0.0414368
0.0626054 0.0407447
0.0271117 0.0409381
0.0950802 0.0433394
0.059181 0.0402197
0.0599031 0.0432356
0.0449751 0.0390572
0.0131845 0.0448198
0.019701 0.0439926
-0.0267398 0.0391844
0.0468012 0.0437455
0.0208245 0.0431598
0.0122452 0.0397642
0.0594398 0.0416372
0.0621339 0.0424019
0.0144707 0.0415525
0.079197 0.0434971
0.0661222 0.0401876
-0.0207292 0.0428789
0.022444 0.0413296
0.0223972 0.0411208
0.0237988 0.0415994
0.0945153 0.0433916
0.0857795 0.0380536
0.0820225 0.0398002
0.0205126 0.0405831
0.0375434 0.0424068
0.0497671 0.0433023
0.059435 0.0407174
-0.0023636 0.040165
0.0710837 0.0417716
0.0989677 0.039729
0.0859467 0.0397878
0.0416484 0.0424609
0.0237986 0.0446654
0.0211182 0.0412073
0.0368959 0.0393456
0.121243 0.0408059
0.0621339 0.0400367
-1.48505e-07 0.0389988
0.0327313 0.0430443
-0.0462049 0.0423346
0.000786701 0.040563
0.0611197 0.0434211
-0.0271157 0.0405164
0.0594396 0.0380997
-0.0390959 0.041544
0.0462066 0.0429235
0.088282 0.0397958
0.0594402 0.0432389
0.0764272 0.0399345
0.0983621 0.0433716
0.0538009 0.0396965
-0.0144856 0.0404269
0.0532657 0.0399138
0.0871098 0.0401577
-0.00236348 0.042237
0.0414001 0.0406256
-0.0414033 0.0418712
-0.0204028 0.0441786
0.0937752 0.0403476
0.0373475 0.0416036
0.0983621 0.0451786
0.102035 0.0421592
-0.00459356 0.0428591
0.0920613 0.0409902
0.0625951 0.0437162
-0.0529698 0.0426862
0.0267779 0.0423948
-0.0417624 0.0399264
0.0449639 0.0389108
0.038544 0.0410773
0.0384776 0.0421238
0.121243 0.0428156
3.82209e-05 0.0434457
0.0621302 0.0413651
0.0243586 0.0434049
0.0109857 0.0443732
0.0538015 0.0392592
0.0318766 0.0400266
0.0330949 0.0392557
0.0966595 0.0417162
0.059181 0.0398063
0.0407513 0.0394961
0.079197 0.0386884
0.0631173 0.0402803
-0.0611171 0.0423873
0.059435 0.0383057
0.0373484 0.0394545
0.0599029 0.0427257
0.130756 0.048106
0.0367961 0.0431237
0.0710837 0.0409733
0.081527 0.0407779
0.0593665 0.0407244
parameters: [ 8.496  0.92   3.953  1.422  4.901]. error: 1325349.59697.
----------------------------
epoch 0, loss 0.943902
epoch 128, loss 1.16505
epoch 256, loss 0.967049
epoch 384, loss 0.783842
epoch 512, loss 1.06203
epoch 640, loss 1.3981
epoch 768, loss 1.08547
epoch 896, loss 0.950301
epoch 1024, loss 0.985798
epoch 1152, loss 0.999206
epoch 1280, loss 0.964692
epoch 1408, loss 1.03256
epoch 1536, loss 0.93382
epoch 1664, loss 0.911096
epoch 1792, loss 1.00627
epoch 1920, loss 1.15622
epoch 2048, loss 1.07147
epoch 2176, loss 1.01102
epoch 2304, loss 0.922777
epoch 2432, loss 0.948602
epoch 2560, loss 0.887874
epoch 2688, loss 1.11147
epoch 2816, loss 0.958199
epoch 2944, loss 1.20886
epoch 3072, loss 1.02639
epoch 3200, loss 0.904557
epoch 3328, loss 0.985785
epoch 3456, loss 0.933727
epoch 3584, loss 1.01687
epoch 3712, loss 0.783483
epoch 3840, loss 1.0611
epoch 3968, loss 0.913052
epoch 4096, loss 0.915383
epoch 4224, loss 1.0023
epoch 4352, loss 0.929727
epoch 4480, loss 1.04376
epoch 4608, loss 0.968345
epoch 4736, loss 0.902002
epoch 4864, loss 0.846842
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0807993 0.0362381
-0.0144706 0.0303337
2.27454e-05 0.0442246
0.0562649 0.0367937
0.05937 0.0433065
-0.000793636 0.0428328
-0.000835048 0.0438886
0.0853255 0.049256
-6.15975e-07 0.0404959
-0.0611272 0.0427967
0.100116 0.0437357
0.0764428 0.0478431
0.0117785 0.0364948
0.0529729 0.0410055
0.0966587 0.039584
0.0394799 0.0451214
0.0625949 0.041121
0.0532535 0.0482279
0.032902 0.0487305
0.0857788 0.0458234
-0.129449 0.0446773
0.0593105 0.0461713
0.0648671 0.0497478
-0.00731719 0.0332748
0.0662931 0.041569
0.0281391 0.0538
0.0271204 0.0386762
0.124277 0.0330768
0.0247806 0.0381664
0.0594395 0.047271
0.0800821 0.0345786
0.0122295 0.0444812
-1.63011e-07 0.0438833
0.104819 0.0368607
0.0904958 0.0369588
0.0582236 0.046763
-0.0204122 0.0413169
0.0815268 0.048279
-0.000786289 0.0317101
0.0407513 0.0433006
0.0594396 0.0445758
-0.053266 0.04744
0.0593665 0.0426878
-0.0462148 0.0397137
0.0871097 0.0441975
0.0317697 0.0469782
-0.0144856 0.0464327
0.0199306 0.0412216
-3.91322e-05 0.0282787
0.0194981 0.0337771
0.0594396 0.0466987
0.0661224 0.0433509
0.0340225 0.0442884
0.106619 0.0478621
0.0295843 0.0299464
0.0710838 0.0388848
0.0131849 0.0389856
0.0691984 0.0354936
0.022215 0.0267923
0.0606162 0.0397472
0.0706155 0.0385685
-0.0113401 0.0311076
0.081797 0.042229
0.0482639 0.0385885
0.0594424 0.0467627
-0.0734361 0.0242092
0.0131849 0.0370514
0.0189828 0.0349254
0.0152545 0.0379932
0.0764328 0.0484736
-0.0417488 0.0380281
0.0594398 0.0308865
0.0529729 0.037438
0.0594398 0.035809
0.0511643 0.0450345
0.074953 0.0428904
0.0210209 0.0499417
-0.0394792 0.0356544
0.0866747 0.0449754
0.0318693 0.0421977
0.0706156 0.0417539
0.0527576 0.0453542
-0.0188023 0.0290424
0.0318766 0.0344485
0.0267781 0.0310624
-0.0860431 0.0443117
-0.0385427 0.0369038
4.07333e-06 0.0414459
0.0593105 0.0390465
0.0365698 0.0402567
-0.0362233 0.028203
-0.000835048 0.0299054
-0.0367964 0.0325199
-0.0204101 0.0377326
0.0286789 0.0315768
-0.0271278 0.0341949
0.0800819 0.0430099
0.0237987 0.0519821
0.0482639 0.0349635
0.0593105 0.049117
-0.0807986 0.0486447
0.0272916 0.052414
0.0594297 0.0437073
0.0594374 0.0427274
0.0330948 0.0432558
-0.0734361 0.0448252
-0.0385437 0.0464247
0.0594398 0.0432626
0.094504 0.038632
-0.00236347 0.0603856
0.0310444 0.0423202
-0.00700262 0.0366362
0.0860587 0.0378015
0.0594398 0.034122
0.0724734 0.0390866
0.0375434 0.0492153
0.0191272 0.0452
0.0329021 0.0435957
0.0594297 0.0520194
0.05943 0.0400372
0.0626106 0.0350132
-0.0188057 0.0379337
0.0449751 0.041412
-0.00893962 0.0352409
0.0199307 0.0379532
0.0204028 0.0345009
0.0267443 0.0300395
0.0968255 0.0332804
parameters: [ 8.496  0.92   3.953  1.422  4.946]. error: 104431311.325.
----------------------------
epoch 0, loss 1.12414
epoch 128, loss 0.956815
epoch 256, loss 1.08625
epoch 384, loss 0.95958
epoch 512, loss 1.07527
epoch 640, loss 1.0107
epoch 768, loss 1.20124
epoch 896, loss 1.14872
epoch 1024, loss 1.22305
epoch 1152, loss 0.941081
epoch 1280, loss 1.17883
epoch 1408, loss 1.03901
epoch 1536, loss 1.0502
epoch 1664, loss 1.15995
epoch 1792, loss 0.790915
epoch 1920, loss 0.9731
epoch 2048, loss 1.09783
epoch 2176, loss 1.10318
epoch 2304, loss 0.97314
epoch 2432, loss 0.734156
epoch 2560, loss 1.01556
epoch 2688, loss 0.781341
epoch 2816, loss 0.971508
epoch 2944, loss 1.05559
epoch 3072, loss 0.877418
epoch 3200, loss 1.05911
epoch 3328, loss 0.933677
epoch 3456, loss 0.962801
epoch 3584, loss 1.02421
epoch 3712, loss 0.993748
epoch 3840, loss 0.891039
epoch 3968, loss 0.799947
epoch 4096, loss 1.08079
epoch 4224, loss 1.34916
epoch 4352, loss 0.81573
epoch 4480, loss 0.896429
epoch 4608, loss 0.925557
epoch 4736, loss 0.891832
epoch 4864, loss 1.00782
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0865812 0.0482512
-0.0295823 0.0295916
0.0327314 0.0555303
0.0991788 0.0442287
0.0945153 0.0409814
0.102034 0.0471815
0.0208245 0.0346291
-0.0248092 0.0386631
-0.0716787 0.0256154
-0.000801651 0.0372216
0.0964766 0.0415856
0.014484 0.0267646
0.0375382 0.0356006
0.0168394 0.0428202
0.0667826 0.0441846
0.0594473 0.0470169
0.0305923 0.0389825
0.0707072 0.0336461
0.0497674 0.0347944
0.0495801 0.0274995
0.0691985 0.0414616
0.0414001 0.0344749
0.0868849 0.0397502
0.0594273 0.0366666
-0.0529659 0.0236788
0.0910569 0.0467927
0.0582236 0.034668
0.0861488 0.0352199
0.105503 0.0455211
0.106629 0.0375922
0.0416434 0.038292
0.0131851 0.0223619
-0.0215238 0.0375762
-0.0857226 0.0283347
-0.00378297 0.025188
0.0319947 0.047848
-0.0385434 0.034607
0.0611271 0.0299135
0.059435 0.0476016
0.0966587 0.0279399
0.0791969 0.0429326
0.0791268 0.0397276
-0.0414059 0.0426908
0.0237986 0.0407777
0.0642209 0.0332876
0.0776735 0.022279
0.0417702 0.0321887
-0.0271111 0.024639
0.101814 0.0366583
-0.0631232 0.0284563
0.0594759 0.0286878
0.104819 0.0398738
0.0791268 0.0415692
0.0691986 0.0591387
-0.00142113 0.0328502
0.0710838 0.0382423
0.126169 0.0269435
0.100097 0.0279794
0.0582236 0.0416443
0.00863029 0.0418242
0.0882822 0.0398972
-0.0131848 0.0368331
0.0594324 0.0403604
0.088729 0.0482673
0.0207304 0.0375506
0.0593105 0.0284788
0.0251231 0.0282211
0.0197008 0.0395322
0.0977801 0.0346089
0.0496683 0.0589628
0.0920961 0.0423606
0.0277564 0.0397886
0.0730674 0.0417723
0.088282 0.0425168
0.0945151 0.0388747
0.0730675 0.0404077
0.0706156 0.0479731
0.10546 0.0428477
0.0859466 0.0565197
0.0594399 0.0552196
0.0710835 0.0583401
0.0594396 0.0456322
0.0734322 0.0472386
0.0211183 0.0293534
0.0271117 0.0279658
0.0527575 0.0579978
0.0594395 0.0561023
0.0594395 0.0407331
-2.64683e-07 0.0300369
0.135017 0.0376109
0.0385437 0.0285334
0.00863029 0.0374114
0.00894922 0.0386243
0.0989677 0.0357091
0.0661223 0.0522307
-0.0188599 0.0414515
-0.0492483 0.0276011
-0.0385431 0.0355138
0.046218 0.0404329
-0.0390966 0.0252231
0.0594398 0.0509421
0.0691068 0.0394681
0.0340225 0.0426572
-0.0529659 0.0205704
0.0673482 0.0316098
0.0749536 0.0423564
-0.00378297 0.0257947
0.088282 0.0448767
0.0624794 0.0304185
0.0813268 0.0319011
-0.0023637 0.0498518
0.0599029 0.0441916
0.0752955 0.0449171
0.0321468 0.0507163
0.0477959 0.0582965
0.0487559 0.039393
0.0237988 0.0407647
0.0594398 0.0402556
0.0945038 0.0278518
-4.6184e-07 0.0271952
0.0261394 0.0518673
0.0594374 0.032696
0.0920969 0.0337768
0.0870049 0.0462989
0.0468064 0.0262184
-0.0529698 0.0357676
0.0131851 0.0266206
0.0449751 0.0379325
parameters: [ 8.496  0.92   3.953  1.422  4.918]. error: 0.991526740614.
----------------------------
epoch 0, loss 1.04234
epoch 128, loss 0.864155
epoch 256, loss 0.908904
epoch 384, loss 0.915483
epoch 512, loss 0.977402
epoch 640, loss 1.22715
epoch 768, loss 1.09552
epoch 896, loss 0.989366
epoch 1024, loss 0.797956
epoch 1152, loss 1.05198
epoch 1280, loss 1.17606
epoch 1408, loss 0.973862
epoch 1536, loss 0.836903
epoch 1664, loss 1.03557
epoch 1792, loss 1.0182
epoch 1920, loss 0.837036
epoch 2048, loss 0.903464
epoch 2176, loss 1.07997
epoch 2304, loss 0.881001
epoch 2432, loss 1.21062
epoch 2560, loss 1.06107
epoch 2688, loss 0.92454
epoch 2816, loss 0.84894
epoch 2944, loss 0.871854
epoch 3072, loss 0.625326
epoch 3200, loss 0.906026
epoch 3328, loss 1.03851
epoch 3456, loss 0.956812
epoch 3584, loss 0.881716
epoch 3712, loss 0.755594
epoch 3840, loss 0.963998
epoch 3968, loss 0.83879
epoch 4096, loss 0.728264
epoch 4224, loss 0.895875
epoch 4352, loss 0.875257
epoch 4480, loss 0.907274
epoch 4608, loss 0.915813
epoch 4736, loss 0.659477
epoch 4864, loss 0.8357
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594399 0.0402594
0.0818743 0.0375144
0.0730674 0.043321
0.093775 0.0374344
0.132184 0.0451736
0.0861486 0.0460367
0.0945153 0.0285977
0.0594397 0.0468034
-0.0346416 0.0194168
-0.0188606 0.0185698
0.0595095 0.034708
0.00931426 0.046295
0.0412776 0.0295206
0.0435741 0.0358164
0.0318695 0.0421579
0.0707072 0.0189562
0.0764428 0.0415506
-3.76332e-06 0.0238559
0.0251233 0.0341812
0.0310444 0.0448563
0.0860364 0.0141682
0.0271163 0.0186498
0.0676335 0.0300405
0.0859465 0.0426393
0.0204045 0.0149598
0.0122295 0.0331365
0.0599612 0.0413703
-0.0208232 0.032984
0.0281392 0.043352
0.105506 0.0372511
0.025123 0.0357618
0.0594398 0.0349084
0.0621334 0.0236887
0.0927399 0.0402279
-0.0807986 0.0318787
0.0261392 0.0461626
0.046799 0.0235096
0.0362237 0.0231737
0.0594396 0.0444262
0.0310444 0.0444687
0.0594396 0.0466016
-0.0807993 0.018129
0.046209 0.0461728
0.0977799 0.0426583
0.0901953 0.0360306
0.0977801 0.0426529
-0.0248182 0.0316612
0.110244 0.0407282
0.081527 0.0431289
0.0621334 0.0256683
-0.0207292 0.0209334
-0.0144836 0.0145615
0.0911183 0.0445618
-0.129453 0.0163636
0.0509553 0.0359282
0.0321468 0.037903
0.0131861 0.0401072
0.0589763 0.0418286
-0.02083 0.0313268
0.0857794 0.0327128
0.0174339 0.0464896
0.038792 0.0340052
0.0414051 0.0132683
0.0407515 0.0468779
0.125157 0.0485626
0.0414025 0.0174141
0.0188019 0.0334699
0.129916 0.0312392
0.0194912 0.0440728
0.0791268 0.0394051
0.0397476 0.0404889
0.0208245 0.0365444
0.0188023 0.0189884
0.0827282 0.0258751
-0.034636 0.0254666
4.17498e-06 0.022192
0.0867335 0.0383309
0.0626002 0.0388357
-0.0495737 0.0136203
-0.0467978 0.0316588
0.0174339 0.0431611
0.121243 0.0535941
0.0594396 0.0427233
0.0272894 0.0112482
0.0319947 0.0494608
-0.0248069 0.027047
-0.0234677 0.0294049
0.0964307 0.0483139
-0.0144836 0.0107237
0.0321467 0.0431662
0.0594116 0.0293277
0.0188088 0.0158579
0.01134 0.0452237
0.0621302 0.0192233
0.0323163 0.0245378
0.0271289 0.019248
0.0707062 0.0237612
0.0461384 0.0490375
0.0234754 0.0307973
0.00856342 0.0219887
-0.00331175 0.0428416
0.112932 0.033877
0.0215366 0.041918
0.0964305 0.0413705
0.0562851 0.0358576
-0.0394796 0.0269282
0.0594396 0.0460898
-0.00894125 0.00677546
0.101814 0.0337697
0.0977801 0.0425778
0.129458 0.0170679
0.000213748 0.041359
0.134995 0.0434556
0.0273016 0.0188556
0.081797 0.043811
-0.0106908 0.020337
0.0927397 0.0462746
-0.0417792 0.031348
0.0295687 0.0245648
0.021525 0.0157131
-3.66166e-06 0.0241239
0.0968257 0.0498663
0.0261391 0.0388825
-0.0152533 0.0270254
0.0424428 0.0408299
-0.053257 0.0189603
0.0594473 0.0336209
-0.0495734 0.0101932
parameters: [ 8.496  0.92   3.953  1.422  4.918]. error: 20599370.8622.
----------------------------
epoch 0, loss 1.20363
epoch 128, loss 0.906396
epoch 256, loss 0.903573
epoch 384, loss 1.02166
epoch 512, loss 0.871291
epoch 640, loss 0.956799
epoch 768, loss 0.850005
epoch 896, loss 0.796829
epoch 1024, loss 0.8786
epoch 1152, loss 0.830488
epoch 1280, loss 0.811119
epoch 1408, loss 1.04329
epoch 1536, loss 0.711452
epoch 1664, loss 0.989579
epoch 1792, loss 0.980046
epoch 1920, loss 0.821186
epoch 2048, loss 0.901628
epoch 2176, loss 0.651208
epoch 2304, loss 0.801204
epoch 2432, loss 0.96525
epoch 2560, loss 0.67982
epoch 2688, loss 0.710372
epoch 2816, loss 0.755666
epoch 2944, loss 0.76505
epoch 3072, loss 0.780913
epoch 3200, loss 0.824726
epoch 3328, loss 0.662154
epoch 3456, loss 0.856894
epoch 3584, loss 0.650612
epoch 3712, loss 1.02003
epoch 3840, loss 0.708906
epoch 3968, loss 0.744119
epoch 4096, loss 0.670202
epoch 4224, loss 0.600611
epoch 4352, loss 0.739819
epoch 4480, loss 0.797883
epoch 4608, loss 0.756988
epoch 4736, loss 0.804856
epoch 4864, loss 0.757135
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0710836 0.0516428
0.0662801 0.00772116
0.0305919 0.0546071
0.0594301 0.0371916
-0.0295559 0.0230121
-0.0385437 0.0121362
0.0594398 0.0579831
0.0224432 0.04962
0.019701 0.0511233
0.0861487 0.0565494
0.0326469 0.011646
-0.0194973 0.0180257
0.0390991 0.00898631
0.0910569 0.0671344
0.0867333 0.0649004
0.036223 0.00827018
0.081527 0.0565771
0.0188596 0.0129871
0.0131845 0.0202742
0.121243 0.0679646
0.0199307 0.0467085
0.0594467 0.0525614
6.39156e-06 0.0159348
0.0211185 0.0489251
0.023648 0.0728586
0.0691987 0.0479485
0.0595095 0.0406406
0.0878297 0.0507433
0.125146 0.0704105
0.0861486 0.080735
0.0362234 0.0117965
-0.036223 0.0156139
4.17498e-06 0.00446577
0.0594467 0.0525614
0.0927397 0.0374818
0.0384877 0.0765899
0.0301504 0.0729068
0.0541227 0.05321
0.022215 0.0455523
-0.0467928 0.0109687
0.0385291 0.024721
0.0594398 0.0725586
0.00596546 0.071701
0.0243585 0.0494205
-0.0131851 0.0255202
0.0776855 0.0300079
3.40452e-06 0.00572045
0.0594162 0.0362114
0.10664 0.0612212
0.0417484 0.00778393
0.0734366 0.0267933
0.0385437 0.0162065
0.0271289 0.00911262
0.0691068 0.0496712
0.0594349 0.0405522
0.0194912 0.0483017
0.0272916 0.0136081
-0.00731719 0.0610046
0.0496681 0.0458463
0.0621334 0.0123467
-0.0385421 0.0390199
-0.0248092 0.0121073
0.0691986 0.0459875
0.086885 0.0769444
-0.000795203 0.0103324
0.132184 0.0744267
0.0966587 0.054814
0.0301505 0.0508511
0.0631196 0.0128123
0.0594399 0.0814095
0.0439209 0.0598549
-0.0318771 0.010644
0.0764118 0.0516317
0.0305923 0.051122
0.024704 0.0442603
0.0662934 0.0099829
0.0318766 0.0185218
-0.00131975 0.0196657
0.0871097 0.0410036
0.088729 0.0494635
0.0497672 0.0498292
0.0301504 0.0564675
0.042448 0.0682862
1.01848e-06 0.00717026
-0.027288 0.0170394
0.13219 0.0540064
-0.0529659 0.00882638
0.0385427 0.00772772
0.0605534 0.0270822
0.081797 0.0616044
0.0710836 0.0500619
0.028139 0.0777401
0.106629 0.0783099
0.0594398 0.0828284
0.000834782 0.00886151
0.0983621 0.0366465
0.0734394 0.0194846
6.7959e-07 0.0190465
0.101441 0.0445697
-0.0776729 0.0141659
0.0477959 0.0487695
0.0529666 0.0137114
-0.0346352 0.0183747
0.0594452 0.0833522
0.0373475 0.0493143
0.081527 0.0534654
0.074953 0.045333
0.126171 0.0339495
0.12614 0.0510408
-0.0234744 0.0118163
0.0188023 0.0109896
0.00143008 0.0245414
0.0594297 0.0492667
0.0562851 0.0488306
0.0319947 0.0524343
0.125157 0.0531748
0.0867067 0.035991
0.0897923 0.0437244
0.102034 0.0429931
-0.0023636 0.0408627
0.0281391 0.0509819
0.0210209 0.0551493
2.5668e-06 0.0169783
0.0594399 0.0379207
0.0261392 0.0489313
4.17498e-06 0.0122893
0.062595 0.0562216
-0.0211014 0.0133674
parameters: [ 8.496  0.08   3.953  1.422  4.918]. error: 1314991197.99.
----------------------------
epoch 0, loss 1.31773
epoch 128, loss 1.24789
epoch 256, loss 1.18899
epoch 384, loss 1.38039
epoch 512, loss 1.32313
epoch 640, loss 1.2926
epoch 768, loss 1.02746
epoch 896, loss 1.32815
epoch 1024, loss 1.07785
epoch 1152, loss 1.22628
epoch 1280, loss 1.21538
epoch 1408, loss 1.02526
epoch 1536, loss 1.13729
epoch 1664, loss 0.967972
epoch 1792, loss 1.03266
epoch 1920, loss 1.17681
epoch 2048, loss 1.25028
epoch 2176, loss 1.12861
epoch 2304, loss 1.03421
epoch 2432, loss 1.08588
epoch 2560, loss 1.12377
epoch 2688, loss 1.23183
epoch 2816, loss 1.07152
epoch 2944, loss 0.957993
epoch 3072, loss 1.34656
epoch 3200, loss 1.07269
epoch 3328, loss 1.10526
epoch 3456, loss 0.939581
epoch 3584, loss 1.02154
epoch 3712, loss 0.940422
epoch 3840, loss 0.835413
epoch 3968, loss 1.06926
epoch 4096, loss 1.16377
epoch 4224, loss 1.0969
epoch 4352, loss 1.19881
epoch 4480, loss 0.949023
epoch 4608, loss 0.76603
epoch 4736, loss 0.910343
epoch 4864, loss 0.951182
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594396 0.0256165
0.00596546 0.0261935
0.0317699 0.0332573
0.110244 0.0326654
-0.0390959 0.0301187
0.0594399 0.0344933
-0.0109842 0.0237896
0.0851609 0.0302505
0.0950802 0.0361595
0.0247829 0.032294
0.109566 0.0299902
0.00142251 0.0269682
0.0188597 0.0362415
0.00894003 0.0249917
-0.0414059 0.0252734
0.0449618 0.0267689
0.0414025 0.026924
0.0333261 0.0278334
0.090741 0.025095
-0.0621343 0.0297355
0.0861486 0.0265664
0.0217933 0.0326324
0.0594426 0.0275762
0.0739227 0.0278589
0.0384676 0.0280408
0.0468064 0.0291045
0.0394803 0.0314823
0.0494968 0.0289313
0.0300726 0.0352567
-0.00699273 0.0269756
0.0911173 0.0349674
6.7959e-07 0.0307974
0.0781223 0.0333451
0.0317697 0.0339686
0.0435741 0.0264861
-0.0807993 0.0257959
0.057604 0.0365332
-0.00728242 0.0308872
0.0907408 0.0344943
0.000786701 0.0296513
0.0417454 0.0266658
-0.0494953 0.0365162
0.00894166 0.0310687
0.0867333 0.028686
0.109566 0.0342033
-0.0117788 0.0251338
0.126206 0.0249602
-0.00893962 0.033991
0.0707072 0.0252
0.0867332 0.0305718
0.0283783 0.0315352
-0.0106908 0.0239011
-0.0659495 0.0225073
0.0317699 0.0325009
-0.00731719 0.02662
0.0236482 0.0345937
0.000835111 0.0289994
-0.0807993 0.0249534
-0.0267425 0.0321178
-0.0467978 0.0250706
-0.0208232 0.025925
0.088282 0.0342412
-0.0346352 0.0312161
0.025123 0.028839
-0.00331175 0.0251482
0.0887289 0.0288956
-0.0271278 0.0324495
0.0596272 0.0328348
0.110244 0.0265761
0.0273016 0.0288805
-0.0662877 0.0273017
0.0278231 0.031112
0.0337182 0.0397216
0.036223 0.0259749
0.0327313 0.0241731
0.0122504 0.0307181
0.0385441 0.0297191
0.0734394 0.0307089
0.0989677 0.0268466
0.028139 0.0263548
-0.0118767 0.0303139
0.0210981 0.0359235
0.041765 0.0274784
-0.0621343 0.0312584
0.0642209 0.0334964
0.0966587 0.0309337
0.0117845 0.0272259
-0.0707062 0.0290303
0.0477957 0.0284841
0.0910569 0.0322907
0.0144707 0.0269955
-4.6184e-07 0.0252701
0.102035 0.0331586
0.0989677 0.0268466
0.0375381 0.0270899
-0.000807624 0.0241692
0.0322982 0.0305086
0.074953 0.0281499
0.0592543 0.0278208
-0.0161248 0.0241133
0.130756 0.0315102
0.00856342 0.0346189
0.0277564 0.0299605
0.13219 0.0341575
0.0340225 0.0262945
0.0301504 0.0269615
0.129925 0.0335381
0.0477958 0.0243366
0.0272894 0.0330166
0.0144707 0.0294829
0.0373484 0.0329965
0.0675205 0.0285362
0.0650733 0.0331002
0.0865811 0.0304979
-0.0412685 0.0332245
-2.15513e-06 0.0247956
0.0215366 0.0263324
0.0673482 0.0319803
-0.0271121 0.0244966
0.0177216 0.0254287
0.0496683 0.0374493
0.0321468 0.0336076
0.0248197 0.037254
0.0781223 0.0331153
0.0152591 0.0269753
0.0417454 0.0351698
0.0691985 0.037427
0.0321467 0.0348165
parameters: [ 8.496  2.538  3.953  1.422  4.918]. error: 4371878064.4.
----------------------------
epoch 0, loss 0.936947
epoch 128, loss 0.947872
epoch 256, loss 1.0847
epoch 384, loss 0.963826
epoch 512, loss 1.07313
epoch 640, loss 1.19564
epoch 768, loss 0.981513
epoch 896, loss 0.995229
epoch 1024, loss 1.10994
epoch 1152, loss 0.817729
epoch 1280, loss 0.92811
epoch 1408, loss 0.884724
epoch 1536, loss 1.10607
epoch 1664, loss 0.782242
epoch 1792, loss 0.909844
epoch 1920, loss 0.952157
epoch 2048, loss 0.923205
epoch 2176, loss 1.06543
epoch 2304, loss 0.935467
epoch 2432, loss 0.90304
epoch 2560, loss 0.767421
epoch 2688, loss 1.06355
epoch 2816, loss 0.754025
epoch 2944, loss 0.824066
epoch 3072, loss 0.792464
epoch 3200, loss 0.969784
epoch 3328, loss 0.721647
epoch 3456, loss 0.901314
epoch 3584, loss 0.856015
epoch 3712, loss 0.952342
epoch 3840, loss 0.877948
epoch 3968, loss 0.658092
epoch 4096, loss 0.790013
epoch 4224, loss 0.714056
epoch 4352, loss 0.828869
epoch 4480, loss 0.851725
epoch 4608, loss 0.893505
epoch 4736, loss 0.819402
epoch 4864, loss 0.884709
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0122295 0.0378696
0.088282 0.0466311
0.0626054 0.0416462
0.129925 0.0350194
-0.0326437 0.0199022
8.40829e-06 0.0138962
0.0752422 0.0432938
0.09274 0.0479548
0.0261394 0.0479349
0.088729 0.0480657
0.0691985 0.0472234
0.0605996 0.0390546
0.0174339 0.0387196
0.0621339 0.0277882
-2.64683e-07 0.00897634
0.0532557 0.0157181
0.0346447 0.012454
-0.0412685 0.0173386
0.0435741 0.0451854
0.0859467 0.051249
0.0964772 0.0427729
-0.0412757 0.0216021
0.0594398 0.0427559
0.0461385 0.0438121
0.059435 0.0489227
0.0271117 0.0214659
2.9023e-05 0.020544
0.0497673 0.0425177
0.0594397 0.0401842
-4.71368e-07 0.0283352
0.0321586 0.0488007
-2.64683e-07 0.0242771
0.0710835 0.0548405
0.0247038 0.0440115
-0.0390959 0.0184556
0.0878299 0.039008
0.0529729 0.01178
0.0594374 0.0430602
0.0417583 0.0232955
0.0327313 0.051564
0.129445 -0.000579684
0.000782366 0.0203098
-0.0529682 0.0191715
-0.10012 0.0290188
0.0897923 0.0451643
0.0594397 0.047877
0.0375382 0.0388759
0.0222152 0.0431747
0.100097 0.0393694
0.0594426 0.0434101
0.0920961 0.0439785
0.00142251 0.0152857
0.088729 0.0469914
0.0878297 0.038823
-0.00968441 0.037259
0.0272988 0.0198174
0.038792 0.0392296
0.0330948 0.0421495
-0.0189824 0.0238987
0.0281391 0.0543313
0.059435 0.0489227
0.0749536 0.0479086
0.0205126 0.0455681
0.062595 0.0485682
-0.049248 0.0100931
-0.0272977 0.00833814
0.101441 0.0394462
-0.0631103 0.0142084
0.135017 0.0425792
0.0594374 0.0421708
0.0305923 0.0478826
0.0752955 0.0385469
0.0467944 0.00769825
0.0764118 0.0450503
0.043921 0.0412863
0.0897923 0.0471479
0.101814 0.037821
2.96104e-08 0.0281656
0.0800819 0.0414793
0.0248076 0.0219301
0.0594297 0.0341799
0.0267781 0.040748
0.0189828 0.00414639
0.0261393 0.0456934
0.0966587 0.0406629
0.0691986 0.0427518
0.0691072 0.0419124
-0.0234677 0.0363074
0.0174335 0.0392978
0.0594426 0.0435423
0.0133907 0.0484636
0.088729 0.0510794
0.126206 0.0422108
0.0237989 0.0478202
0.124277 0.0431784
0.080382 0.0428465
0.121243 0.0486209
0.0871095 0.0509659
0.0286789 0.0451958
0.0691068 0.0425861
-0.0248092 0.0267475
-0.0529659 0.00330271
0.057604 0.0411113
-0.0807974 0.00567376
0.088729 0.048074
-0.0247813 0.0218386
0.0461382 0.053843
-0.000805986 0.0253033
1.96444e-05 0.0166561
-0.0417425 0.012668
0.112932 0.0477743
-0.0131851 0.0356594
-0.0144843 0.0188453
0.00596546 0.0434878
0.0271115 0.0127673
0.0396776 0.044119
0.0599031 0.0540929
0.0390889 0.00303651
-2.95455e-07 0.0161696
0.0631163 0.0294056
0.0621302 0.0113313
0.0243585 0.0466396
0.0920961 0.0373718
0.0390895 0.0150966
-0.00249422 0.0249946
0.0907409 0.0488286
0.0594466 0.0490716
0.106624 0.0437595
parameters: [ 8.496  0.92   3.953  1.422  4.918]. error: 8.8244228386e+12.
----------------------------
epoch 0, loss 0.899251
epoch 128, loss 1.08318
epoch 256, loss 1.12051
epoch 384, loss 1.13271
epoch 512, loss 1.0572
epoch 640, loss 1.00437
epoch 768, loss 0.942266
epoch 896, loss 1.05674
epoch 1024, loss 1.00199
epoch 1152, loss 1.07572
epoch 1280, loss 0.887023
epoch 1408, loss 1.01656
epoch 1536, loss 1.06251
epoch 1664, loss 0.815558
epoch 1792, loss 0.731337
epoch 1920, loss 0.803839
epoch 2048, loss 0.816034
epoch 2176, loss 1.2429
epoch 2304, loss 0.952634
epoch 2432, loss 0.810179
epoch 2560, loss 1.02665
epoch 2688, loss 1.01619
epoch 2816, loss 0.911269
epoch 2944, loss 0.973437
epoch 3072, loss 0.757644
epoch 3200, loss 0.845524
epoch 3328, loss 1.19979
epoch 3456, loss 0.903878
epoch 3584, loss 0.932018
epoch 3712, loss 0.854532
epoch 3840, loss 0.958892
epoch 3968, loss 0.856371
epoch 4096, loss 0.752728
epoch 4224, loss 0.767767
epoch 4352, loss 0.817182
epoch 4480, loss 0.747369
epoch 4608, loss 0.975426
epoch 4736, loss 0.880364
epoch 4864, loss 1.01368
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0192074 0.0448563
0.0989675 0.0356806
-0.0734384 0.0483359
0.0346353 0.0137704
-0.0495737 0.0329409
0.0594452 0.046517
0.0375434 0.0407861
-0.0144706 0.0309588
-0.000835392 0.038453
-0.0295676 0.0383188
0.0541227 0.0463724
0.0201879 0.0397162
0.0964305 0.0500868
0.0509553 0.0432015
0.0968257 0.0488543
0.0807993 0.0190509
0.129458 0.0214789
0.0396776 0.0473039
0.0527578 0.0390598
-0.0662877 0.0213001
0.0323163 0.0263795
0.0927397 0.0405685
0.0662801 0.016563
0.0527577 0.0491812
-0.0323135 0.0339991
-0.0776729 0.0380482
-0.0412685 0.0271874
0.0594325 0.0464365
0.0220537 0.0487823
0.0385437 0.0316145
0.00932522 0.0492612
-0.0271157 0.0256186
-3.11149e-07 0.0285286
0.0966587 0.0273845
-0.0621274 0.0229584
0.0220539 0.049734
0.0496681 0.0497823
0.129445 0.00553278
0.000782366 0.00850241
0.0113515 0.0276094
0.00931417 0.0411209
0.0589127 0.0504284
0.0385431 0.0282701
0.0868848 0.0532335
0.036794 0.0374345
0.0605534 0.039063
0.129458 0.0184798
0.0337178 0.0442371
0.0497671 0.0500302
0.0706156 0.0482809
0.0529729 0.0235849
0.0964307 0.042014
-0.0194973 0.0182214
0.0589133 0.0466705
0.0461382 0.0475058
0.0968255 0.053424
0.0727409 0.0522502
-0.0385431 0.0238091
0.0340225 0.0448108
0.0541426 0.0463584
0.0594398 0.0467537
0.0113418 0.0277155
0.062595 0.0504319
0.109566 0.0544606
0.0188086 0.0299777
0.0920961 0.0406086
0.0113491 0.0275039
0.0981543 0.0473949
0.0857183 0.0188668
0.0106903 0.0267874
0.0330542 0.0389715
0.0122295 0.0417061
0.0981543 0.0494891
0.0243594 0.0410781
0.0589763 0.0492713
0.0818743 0.0402504
0.0897923 0.050332
-0.053257 0.0412123
-0.041769 0.0186514
0.0710835 0.0453822
-0.014471 0.0268288
-0.0118768 0.0552296
0.0189892 0.0279257
0.000834782 0.0310278
-0.0716787 0.0269529
0.0373484 0.042825
-3.66166e-06 0.0208926
0.0870053 0.0438194
0.0562648 0.0418977
-0.0860382 0.0179786
0.000835126 0.0159488
0.080382 0.034185
0.0764428 0.0381742
0.0321468 0.0536906
0.0739227 0.0350923
0.027823 0.0491269
0.0594396 0.0548807
0.129445 0.0182912
0.101814 0.0441141
0.0807993 0.0356264
0.0217933 0.0387655
0.05937 0.0395646
-0.0707068 0.0188252
-0.00735889 0.0319634
0.0322982 0.0529999
-0.00737805 0.0345894
-0.0807974 0.0140268
0.0424428 0.0356563
-0.00628321 0.0460402
0.106629 0.0468601
0.0122352 0.0426742
0.0807983 0.0257802
0.059181 0.040527
0.0188018 0.0392973
-0.036223 0.0274255
0.0776855 0.0367817
0.0910567 0.0429777
0.0897923 0.0562424
0.0278228 0.0433702
0.0330948 0.0402993
0.0764428 0.0494371
0.057604 0.045934
0.0122504 0.0415861
0.0335492 0.0446116
0.0968257 0.054274
0.10665 0.0471701
0.0867335 0.0437362
-1.92327e-05 0.0237402
parameters: [ 8.496  1.538  3.953  1.422  4.918]. error: 110.701839218.
----------------------------
epoch 0, loss 1.23677
epoch 128, loss 1.21835
epoch 256, loss 1.03747
epoch 384, loss 1.18813
epoch 512, loss 1.28963
epoch 640, loss 0.99369
epoch 768, loss 1.30414
epoch 896, loss 0.975622
epoch 1024, loss 1.02976
epoch 1152, loss 1.10581
epoch 1280, loss 0.970958
epoch 1408, loss 1.00635
epoch 1536, loss 1.00388
epoch 1664, loss 1.09123
epoch 1792, loss 1.01365
epoch 1920, loss 1.0044
epoch 2048, loss 0.805867
epoch 2176, loss 1.05075
epoch 2304, loss 0.885336
epoch 2432, loss 1.13548
epoch 2560, loss 1.19449
epoch 2688, loss 0.909348
epoch 2816, loss 1.08578
epoch 2944, loss 1.10054
epoch 3072, loss 0.971081
epoch 3200, loss 1.07789
epoch 3328, loss 0.913435
epoch 3456, loss 1.13392
epoch 3584, loss 0.745782
epoch 3712, loss 1.22658
epoch 3840, loss 1.01528
epoch 3968, loss 0.850316
epoch 4096, loss 1.1324
epoch 4224, loss 1.04034
epoch 4352, loss 1.1697
epoch 4480, loss 0.918238
epoch 4608, loss 1.00824
epoch 4736, loss 0.879782
epoch 4864, loss 1.0913
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0497672 0.0377819
0.0261394 0.0469063
-0.0385433 0.0386678
0.0800819 0.042463
0.129458 0.0367558
0.0211183 0.0391475
0.0594399 0.0414979
-6.66928e-06 0.0372443
-4.6184e-07 0.0385664
0.101441 0.0399188
0.0594398 0.0420222
-0.0131854 0.0403942
0.0648671 0.0411151
0.0385291 0.0377121
-0.0234677 0.0370293
0.0390889 0.0355994
0.0964772 0.0386353
0.0461382 0.0495613
0.0791268 0.03742
0.00131924 0.0392391
0.0482638 0.0415593
0.0927398 0.0444617
0.0215366 0.0383093
0.056285 0.0450933
-5.97989e-06 0.038041
0.124277 0.0421269
0.100668 0.050336
0.094504 0.040698
0.0496681 0.0381617
0.130757 0.04391
0.0800819 0.0388273
0.0317698 0.0415545
0.097887 0.0404465
0.0945151 0.0436048
0.0477956 0.0479461
-6.66928e-06 0.0417172
0.0783558 0.0415036
0.0177241 0.0386034
-0.0611267 0.039054
0.0727411 0.0498369
-0.0323135 0.0377948
0.0267781 0.0366098
0.0424529 0.0375628
0.0724734 0.0381186
-1.1614e-09 0.0384735
-0.0267379 0.0377513
-0.10011 0.0402071
0.0529709 0.0387551
0.0278229 0.0419383
-0.0113401 0.0371673
0.0271163 0.0378779
-0.0177194 0.0371284
0.0301505 0.0452881
0.0631207 0.0402883
0.0168394 0.0380676
0.033718 0.0445487
0.088282 0.0406541
0.0857795 0.0403975
0.0385291 0.0353899
0.0234754 0.0396417
0.0189868 0.0390344
0.0131848 0.0399423
0.0295687 0.0404009
0.0964307 0.0367778
0.129458 0.0367558
0.05943 0.0378246
0.0599029 0.0417033
0.0144846 0.040789
0.0330542 0.0421216
0.0989677 0.0391635
-1.32922e-07 0.0379853
-2.23337e-05 0.0396522
-0.00968441 0.0378663
0.0277564 0.040582
0.0319947 0.0405054
0.0594398 0.0451879
0.0368964 0.0386633
0.0730674 0.038245
0.0764272 0.036933
-0.00250418 0.0380629
0.0174339 0.0397737
-0.0495799 0.0368324
-0.0385424 0.0378112
0.0477959 0.0458396
0.0927397 0.0452049
0.0373484 0.0415182
0.0199307 0.0395975
0.0321468 0.0390079
0.0133907 0.0398138
0.0589763 0.0451622
0.0174335 0.0374149
-0.0118764 0.0454897
-0.0106876 0.0370028
-2.15513e-06 0.0406974
0.0897923 0.0459772
0.0562849 0.0421512
0.0901952 0.0390914
0.0093143 0.0395299
0.0691985 0.0416085
-0.00236368 0.0390455
-0.0860431 0.0388498
0.0734366 0.037932
0.130757 0.0512277
0.029557 0.0416592
0.0815268 0.0396104
0.0373475 0.0371637
0.0290868 0.0495663
0.0878297 0.0397788
-0.0267425 0.0382953
0.0945153 0.0396098
0.0414075 0.0388265
-0.0532547 0.0397418
0.135017 0.0395124
2.05834e-07 0.0392898
0.0346353 0.0395688
0.0468012 0.0387362
0.0449751 0.0407901
0.0417454 0.0383708
-0.0662877 0.0362942
0.0267419 0.0400897
0.0594396 0.038862
-0.0323135 0.0350202
-0.0857226 0.0364673
-0.0131848 0.0388106
0.0462156 0.04219
-0.0131841 0.0400852
0.0436381 0.0361224
0.126086 0.0394287
parameters: [ 8.496  1.92   3.953  1.422  4.918]. error: 159187635230.0.
----------------------------
epoch 0, loss 1.55684
epoch 128, loss 1.2039
epoch 256, loss 1.26977
epoch 384, loss 1.44948
epoch 512, loss 1.3293
epoch 640, loss 1.19769
epoch 768, loss 0.978287
epoch 896, loss 1.33777
epoch 1024, loss 1.19715
epoch 1152, loss 1.24305
epoch 1280, loss 1.08138
epoch 1408, loss 1.05077
epoch 1536, loss 0.995883
epoch 1664, loss 1.03842
epoch 1792, loss 1.20461
epoch 1920, loss 1.08173
epoch 2048, loss 1.15639
epoch 2176, loss 0.989968
epoch 2304, loss 1.00718
epoch 2432, loss 1.25315
epoch 2560, loss 1.19379
epoch 2688, loss 1.03658
epoch 2816, loss 1.31192
epoch 2944, loss 1.11423
epoch 3072, loss 1.02817
epoch 3200, loss 1.00749
epoch 3328, loss 0.94756
epoch 3456, loss 1.03115
epoch 3584, loss 1.04178
epoch 3712, loss 1.06832
epoch 3840, loss 0.883554
epoch 3968, loss 0.880478
epoch 4096, loss 0.994653
epoch 4224, loss 1.05425
epoch 4352, loss 1.03038
epoch 4480, loss 0.891943
epoch 4608, loss 0.95944
epoch 4736, loss 0.941637
epoch 4864, loss 1.07134
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0327313 0.0353536
-0.0611267 0.0321278
-0.0210947 0.0454176
0.0964307 0.0420885
0.100097 0.0427913
0.0234638 0.0385704
0.0495801 0.0340708
0.0927396 0.0311015
0.0710837 0.0310612
0.0861488 0.0393944
-0.0807974 0.0326154
0.0209719 0.0361028
0.0290868 0.0387194
0.0594466 0.0478398
0.0621334 0.0365724
0.0417454 0.0387202
-1.48505e-07 0.0330724
0.0562597 0.0374727
0.105503 0.0497448
0.0859465 0.0336169
-0.0152606 0.0394445
-0.0385427 0.0434426
-0.0631232 0.0472879
3.62028e-05 0.0293276
0.0109857 0.0318279
0.0983621 0.0481992
0.0860428 0.0341837
0.0710836 0.0386589
0.0290868 0.0413108
0.0599029 0.0443607
0.0495768 0.0369665
0.0329331 0.0448041
0.0109586 0.0306861
0.0468012 0.0293818
0.0691068 0.0352537
0.076417 0.0454692
-0.00735889 0.0448463
0.0424581 0.0440166
0.0152591 0.0340275
0.027823 0.0470487
0.0730676 0.0362468
0.0594116 0.0460055
0.0907409 0.0390386
0.0321468 0.0401198
0.081797 0.0398076
-0.0267352 0.0293261
0.0277564 0.0550012
-0.0611171 0.0295027
0.0800821 0.048027
-6.66928e-06 0.0408271
0.0113418 0.0267142
0.0966587 0.0337024
0.0594424 0.0397911
0.0122352 0.0507903
0.0424369 0.0403214
-0.0113401 0.0411354
0.0204045 0.0429177
-0.0152606 0.0320227
0.0860428 0.0279818
0.0532677 0.0451386
-0.00459356 0.0414969
0.041765 0.0378139
-0.039089 0.0287931
-0.0416401 0.0404762
0.0435639 0.0431134
7.08095e-06 0.0486464
0.0800821 0.039351
0.0199309 0.0386922
0.0271289 0.0414931
0.021525 0.0214804
-0.0807993 0.0277725
-0.0860447 0.023567
0.0625951 0.0339291
0.0532557 0.0309632
-0.0462169 0.0235795
-0.053257 0.0326778
-0.0362233 0.0435172
0.0800821 0.0442092
0.0384776 0.0423086
0.032933 0.0386564
-0.000835392 0.0358841
0.0271289 0.0418807
0.0677309 0.0425498
-0.0734312 0.0317482
0.0140549 0.0397926
0.0691068 0.0450035
-1.1614e-09 0.0281563
0.0594473 0.0330592
-2.99285e-06 0.0374508
-0.0662867 0.028299
0.0861488 0.0377544
0.0477957 0.0286557
0.0870053 0.0408942
0.0870049 0.0348368
0.0857284 0.0274249
0.118684 0.0357354
-0.00131975 0.0255474
0.0621302 0.0460191
0.085161 0.0405577
0.0329332 0.0309093
0.0966587 0.0342893
0.0952315 0.031838
-0.0113401 0.0333942
0.0730674 0.028329
0.0594397 0.0328244
6.7959e-07 0.0414332
-0.0611267 0.0239066
0.0243586 0.0370362
0.0631173 0.0284638
-0.0132965 0.0543136
0.0594399 0.0457344
-0.0211014 0.0294955
0.0594398 0.0427968
0.0373484 0.0483411
0.0800819 0.0446069
-0.0414033 0.034472
0.0764481 0.0455154
-0.0267398 0.0419068
-7.68032e-07 0.041711
-0.0318771 0.0370354
0.032158 0.0444662
-0.0132965 0.0430964
0.0986862 0.0514668
0.0205126 0.0285968
0.0910567 0.041595
0.0594324 0.0385438
0.0346353 0.0343472
6.7959e-07 0.0327678
parameters: [ 8.496  1.714  3.953  1.422  4.918]. error: 9178135270.23.
----------------------------
epoch 0, loss 1.27431
epoch 128, loss 1.12796
epoch 256, loss 1.09305
epoch 384, loss 1.20339
epoch 512, loss 1.113
epoch 640, loss 1.19686
epoch 768, loss 1.14732
epoch 896, loss 1.06927
epoch 1024, loss 1.18937
epoch 1152, loss 0.959468
epoch 1280, loss 0.988706
epoch 1408, loss 1.00118
epoch 1536, loss 1.15373
epoch 1664, loss 1.14253
epoch 1792, loss 0.997803
epoch 1920, loss 1.05668
epoch 2048, loss 1.01145
epoch 2176, loss 0.795518
epoch 2304, loss 1.00865
epoch 2432, loss 1.07733
epoch 2560, loss 0.928916
epoch 2688, loss 1.09776
epoch 2816, loss 0.92998
epoch 2944, loss 0.926041
epoch 3072, loss 0.971978
epoch 3200, loss 1.09186
epoch 3328, loss 0.98424
epoch 3456, loss 0.96527
epoch 3584, loss 1.04713
epoch 3712, loss 0.805281
epoch 3840, loss 0.88957
epoch 3968, loss 1.08244
epoch 4096, loss 0.916256
epoch 4224, loss 1.03843
epoch 4352, loss 1.035
epoch 4480, loss 0.956296
epoch 4608, loss 1.05713
epoch 4736, loss 0.825041
epoch 4864, loss 0.900951
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0370775 0.0392862
0.0911183 0.0375423
0.0168394 0.0280634
-0.0385424 0.0334421
0.088729 0.0381832
-0.0188599 0.0356568
2.05834e-07 0.0286865
0.000834782 0.0230456
0.0851511 0.035584
-0.0109489 0.0329785
0.0243594 0.0380324
-0.0161473 0.032188
0.088729 0.040661
0.0859962 0.031811
-0.00460347 0.0339632
0.0424369 0.0374771
-0.0234626 0.0357755
-0.0161248 0.036128
0.0964305 0.032671
0.0851609 0.0501099
-0.000781954 0.0297216
0.0207331 0.0329991
0.0511643 0.0346578
-0.0111018 0.0406761
0.0286795 0.0474982
0.0594325 0.0368931
-0.0716686 0.0351978
0.0594398 0.0375925
0.0283785 0.0440893
0.062595 0.0388906
0.0594297 0.0351574
0.0174335 0.0401084
-0.0131851 0.0273461
-0.0188593 0.0360937
0.0222151 0.0423251
-0.0204101 0.0395931
0.0461383 0.0437137
0.0904958 0.0440593
-0.0188593 0.0268624
0.0966595 0.027987
0.056285 0.0358332
0.0222151 0.03836
0.132178 0.0383324
0.0871098 0.039882
-0.0188593 0.0295678
0.123121 0.0331864
-0.00378521 0.0332671
0.015257 0.0312048
0.0464004 0.0350339
0.0599612 0.0367686
0.0321586 0.0351781
0.0037846 0.0293976
0.0468012 0.0324457
0.0855722 0.0356398
0.0468012 0.0331757
0.0511643 0.0306392
0.0813321 0.0327236
0.0271117 0.0345713
0.124354 0.0311865
0.0461383 0.0433235
0.110244 0.0308596
0.0594397 0.0349485
-0.0340213 0.032735
0.0435741 0.0261464
-0.000801651 0.0291799
0.12432 0.032413
0.0594398 0.0411806
0.0321468 0.0358262
4.17498e-06 0.0373832
-0.0056768 0.0267445
0.0271115 0.0283609
0.0267779 0.034366
-0.0111018 0.0377482
0.0144859 0.0288989
0.0209719 0.032852
0.0243594 0.0388938
-0.0807974 0.0285428
0.0329332 0.046927
0.0281392 0.0395356
0.0538297 0.03895
0.0710838 0.0460937
0.0595095 0.0341908
-0.0131845 0.0261367
0.123121 0.0255612
0.0477956 0.0423128
0.062595 0.0355229
0.0983621 0.0402373
0.0286795 0.037997
-0.0631101 0.0284008
0.0417489 0.0269299
-0.0194897 0.0329057
0.0706156 0.0391886
0.0752422 0.0398891
0.112932 0.0343222
0.0781227 0.0355132
-0.00893962 0.0298991
-0.0621274 0.0320293
0.0210209 0.0345647
-0.0462076 0.0304668
0.049499 0.0383969
0.0861488 0.0296212
0.0907408 0.0381244
0.0907408 0.0422689
0.0122452 0.0367319
0.0271187 0.0259982
0.0220539 0.0338894
-0.0662877 0.0296886
0.0867199 0.0372983
-0.0248165 0.0265914
-0.0417488 0.0331929
0.0414001 0.03474
0.0867119 0.0319275
0.121243 0.0388973
0.0117782 0.0277127
0.0706155 0.0372455
0.0210958 0.0383005
0.0384776 0.0398479
-0.0106908 0.0257487
-0.041635 0.0345982
-0.0267379 0.0335654
0.0594398 0.0401607
0.0364958 0.0295875
-0.0267352 0.0305034
0.0818743 0.0363478
0.0716773 0.0343737
0.0131858 0.0314722
0.090741 0.0431543
0.0529709 0.0272832
parameters: [ 8.496  1.611  3.953  1.422  4.918]. error: 72510695369.3.
----------------------------
epoch 0, loss 1.60114
epoch 128, loss 1.68512
epoch 256, loss 1.25094
epoch 384, loss 1.05595
epoch 512, loss 1.3911
epoch 640, loss 1.30921
epoch 768, loss 1.08877
epoch 896, loss 1.06618
epoch 1024, loss 1.18308
epoch 1152, loss 0.976422
epoch 1280, loss 1.17934
epoch 1408, loss 1.09213
epoch 1536, loss 1.24523
epoch 1664, loss 0.996331
epoch 1792, loss 1.04546
epoch 1920, loss 1.06695
epoch 2048, loss 1.17654
epoch 2176, loss 1.06229
epoch 2304, loss 1.0168
epoch 2432, loss 1.14508
epoch 2560, loss 1.08973
epoch 2688, loss 1.04786
epoch 2816, loss 1.00653
epoch 2944, loss 1.0379
epoch 3072, loss 0.887533
epoch 3200, loss 1.05949
epoch 3328, loss 0.853399
epoch 3456, loss 1.26195
epoch 3584, loss 1.03292
epoch 3712, loss 1.10333
epoch 3840, loss 1.14943
epoch 3968, loss 1.05134
epoch 4096, loss 1.12259
epoch 4224, loss 1.2618
epoch 4352, loss 1.04062
epoch 4480, loss 1.02656
epoch 4608, loss 1.24701
epoch 4736, loss 1.13243
epoch 4864, loss 1.13115
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0593665 0.0361454
0.0867332 0.0469261
0.121243 0.0526384
-0.0707068 0.025381
0.0594452 0.0327249
0.0117845 0.0334468
0.0857788 0.036879
0.0318695 0.0318891
-0.0177216 0.0327568
0.0594473 0.0277696
0.0373484 0.0368037
-0.0707062 0.0310343
-0.0326437 0.0345077
0.0730675 0.0400789
0.0209719 0.0393047
-0.0417488 0.032506
0.0813321 0.0325099
-0.053266 0.0386936
-0.0109774 0.0310155
0.0496682 0.0407983
0.0373484 0.0416182
-0.0621272 0.0364943
-0.0417491 0.0329017
0.0927399 0.0495523
0.0589764 0.0464792
-0.0194973 0.0337163
-0.000781954 0.0291409
0.12945 0.0292016
0.0394799 0.0341092
0.0268143 0.0348468
0.0562849 0.0437171
0.0251234 0.0322681
0.0317697 0.0562753
0.0626054 0.0289384
0.0964772 0.0384257
0.106655 0.0274312
0.0477959 0.0399508
0.0248169 0.0372637
0.0701141 0.0356827
0.0468064 0.042212
0.0527576 0.0574155
0.0907409 0.0408922
0.0594325 0.0354768
-0.000786289 0.0321457
0.0435639 0.0361117
0.0907408 0.0521949
0.0261392 0.041759
0.0867067 0.0316618
-0.00236357 0.0494341
-0.0417624 0.0311904
0.102034 0.0358174
-0.0734312 0.0413931
0.0594449 0.0360871
2.96104e-08 0.0320348
0.0611204 0.0294804
0.0144859 0.0386329
0.0594116 0.0312089
-0.0267352 0.0367594
0.0867119 0.0262316
0.102035 0.0308004
0.056285 0.0508946
-0.0188023 0.0333223
0.133575 0.0267683
0.110244 0.0334369
0.0290868 0.0503753
0.0776855 0.0298294
0.0966595 0.0354071
0.0318695 0.0278689
-0.053257 0.0441362
0.0800819 0.0334333
0.0920613 0.0365034
0.015257 0.0342146
0.0871095 0.0408771
0.0677309 0.0323958
0.0424215 0.0348537
-0.000835392 0.0303206
0.0764481 0.0337842
0.0716773 0.0333029
0.118684 0.0259461
0.018802 0.0348515
0.0676335 0.0299634
0.0375382 0.0318834
0.0871098 0.040925
-0.00331175 0.0317022
0.0878297 0.0354113
0.0594396 0.0403242
0.0594397 0.0388929
0.0813321 0.0336511
0.0594375 0.0287625
-0.00700272 0.0461225
-0.0271153 0.0274224
-3.11621e-07 0.0309923
0.126127 0.0361316
0.0317697 0.0327246
0.0594397 0.0441232
0.088729 0.0471879
0.0532657 0.0386028
0.0594426 0.0276149
0.0458117 0.0427071
-0.0532547 0.0368687
0.0140549 0.0348481
0.0327313 0.0464705
0.0594251 0.0424851
0.000786701 0.0271189
-0.0272995 0.031009
0.0174335 0.0310552
0.0322983 0.0412515
0.0234754 0.0395554
0.0385291 0.0347554
-0.0807974 0.0307654
0.0295687 0.04631
0.0122243 0.0368703
0.0964772 0.0434817
-0.0631103 0.0312645
0.0397476 0.0411053
-0.0131845 0.0367274
0.0730676 0.0374211
-0.0323135 0.0308577
0.0594398 0.0455265
0.0813321 0.0339906
0.0113418 0.0466905
0.0734366 0.039966
0.126086 0.0351556
0.0594466 0.0248852
0.0791961 0.0308154
0.105506 0.0276992
0.0964305 0.046885
0.0977802 0.0268218
parameters: [ 8.496  1.301  3.953  1.422  4.918]. error: 1.21742604222e+13.
----------------------------
epoch 0, loss 1.53393
epoch 128, loss 1.57789
epoch 256, loss 1.72809
epoch 384, loss 1.46577
epoch 512, loss 1.24382
epoch 640, loss 1.45825
epoch 768, loss 1.29547
epoch 896, loss 1.33616
epoch 1024, loss 1.29383
epoch 1152, loss 1.32031
epoch 1280, loss 1.01634
epoch 1408, loss 0.977773
epoch 1536, loss 1.04203
epoch 1664, loss 1.00021
epoch 1792, loss 1.08231
epoch 1920, loss 0.97108
epoch 2048, loss 0.944287
epoch 2176, loss 1.24836
epoch 2304, loss 0.968558
epoch 2432, loss 0.929209
epoch 2560, loss 1.00539
epoch 2688, loss 0.809874
epoch 2816, loss 0.95881
epoch 2944, loss 0.787608
epoch 3072, loss 0.941229
epoch 3200, loss 0.893854
epoch 3328, loss 0.899949
epoch 3456, loss 1.02463
epoch 3584, loss 0.704661
epoch 3712, loss 0.793824
epoch 3840, loss 1.01733
epoch 3968, loss 0.955025
epoch 4096, loss 0.87024
epoch 4224, loss 0.838552
epoch 4352, loss 0.974369
epoch 4480, loss 0.773032
epoch 4608, loss 0.941953
epoch 4736, loss 1.15179
epoch 4864, loss 0.806691
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0247806 0.035566
0.0593346 0.0540406
0.133575 0.0391944
0.0594398 0.0502324
-0.0417491 0.0266642
0.0611271 0.0191412
0.0594204 0.0422561
0.0439209 0.0441347
0.0199307 0.0483906
0.121243 0.049047
-0.00735889 0.040627
0.0594397 0.0412184
-0.041769 0.0364886
0.0261391 0.0571364
0.0170933 0.0519695
-2.23337e-05 0.0231505
0.0290868 0.0506774
-0.000805986 0.0196294
0.0594398 0.0441683
-6.0681e-07 0.0383728
0.0851612 0.056769
0.0321468 0.0356944
0.081527 0.0514271
0.0424428 0.0558188
-0.00700272 0.0491404
0.0461385 0.0499553
0.0204028 0.0329917
0.0562851 0.0500733
0.0594398 0.0484705
0.0648671 0.0495714
-3.11149e-07 0.0129657
0.0117785 0.013921
0.0286789 0.0451289
-0.0194973 0.0207249
0.0319947 0.0422225
0.0861488 0.0477178
0.0286789 0.0410575
0.029557 0.0217549
-0.0271193 0.0286798
-0.0662867 0.0227739
2.96104e-08 0.035029
-0.0532547 0.0253947
0.0367961 0.0370302
0.102034 0.0463881
-0.00728242 0.0487031
0.0691986 0.0481914
-0.0118767 0.0317809
0.0996717 0.0524887
0.0387924 0.0434063
-0.0621272 0.0255438
0.0871095 0.0614322
0.132184 0.0603018
0.0191272 0.0487856
0.0207331 0.0126311
0.0272988 0.0277475
0.0599031 0.0473927
-0.0659495 0.0154303
-0.0716787 0.0137095
0.0449618 0.022588
0.0189828 0.0223245
0.0397474 0.0498659
-0.00737805 0.0486574
0.0424316 0.0425426
0.101441 0.0430468
0.0861486 0.0500245
-0.0462076 0.0347216
-0.0295559 0.0170158
0.0595095 0.0530753
0.0295687 0.0283461
0.0390895 0.0154061
0.0859467 0.0473495
-0.0177216 0.0216508
0.0594398 0.0502885
0.0467944 0.0213898
-0.0204101 0.0271535
0.126127 0.0470605
-0.0247793 0.0340158
0.0301505 0.0444968
0.0317699 0.0498311
0.01134 0.0345824
0.0867332 0.0469692
0.0662931 0.023448
0.0599029 0.0485938
0.0593758 0.0434364
0.0594324 0.0488025
-0.0144843 0.0224671
0.0370769 0.0439904
0.0952317 0.052891
0.0991788 0.0458574
0.0594399 0.0525803
0.0927399 0.0574127
0.059435 0.0475952
0.0857795 0.0453895
0.0234754 0.0256496
0.0594349 0.0532375
-0.0417422 0.0224656
0.0109781 0.0342216
0.0910567 0.0368029
0.0397474 0.0496255
0.0168394 0.0462948
-0.000792737 0.0336669
-0.0271278 0.0243698
0.0248076 0.0368426
0.0333261 0.0448687
0.0140549 0.0418928
0.0281392 0.0557888
0.0827282 0.0320978
0.0859865 0.0609575
0.109566 0.0512105
0.000793148 0.021104
-0.00733491 0.0486912
-0.0362233 0.0287875
0.0384777 0.0504761
-0.0494977 0.0162115
3.36628e-05 0.0297918
0.0859466 0.0384189
0.0492512 0.0200056
0.060558 0.041067
0.0997462 0.0432192
-0.0132965 0.0525619
8.39808e-07 0.0357366
0.0859467 0.0400786
-0.0111821 0.0405365
0.12432 0.0510494
0.0920961 0.0513965
0.0867202 0.0532072
-0.0188028 0.0266162
-0.0188057 0.0238209
parameters: [ 8.496  1.447  3.953  1.422  4.918]. error: 1.52215500062e+13.
----------------------------
epoch 0, loss 1.12593
epoch 128, loss 1.08766
epoch 256, loss 1.31113
epoch 384, loss 1.1303
epoch 512, loss 1.17654
epoch 640, loss 1.11168
epoch 768, loss 1.37838
epoch 896, loss 1.07586
epoch 1024, loss 1.03759
epoch 1152, loss 0.976179
epoch 1280, loss 1.234
epoch 1408, loss 0.965234
epoch 1536, loss 0.990591
epoch 1664, loss 0.885359
epoch 1792, loss 1.17171
epoch 1920, loss 0.921277
epoch 2048, loss 0.985934
epoch 2176, loss 1.09048
epoch 2304, loss 0.932595
epoch 2432, loss 1.05098
epoch 2560, loss 1.06343
epoch 2688, loss 1.028
epoch 2816, loss 1.15862
epoch 2944, loss 1.30577
epoch 3072, loss 0.931078
epoch 3200, loss 0.912895
epoch 3328, loss 1.03122
epoch 3456, loss 0.995478
epoch 3584, loss 0.887682
epoch 3712, loss 1.17199
epoch 3840, loss 0.937704
epoch 3968, loss 1.06243
epoch 4096, loss 0.815492
epoch 4224, loss 1.04394
epoch 4352, loss 1.08462
epoch 4480, loss 0.89521
epoch 4608, loss 1.10538
epoch 4736, loss 0.93446
epoch 4864, loss 1.19109
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.088282 0.0440579
0.0301505 0.0463594
0.0152591 0.0434503
0.0813269 0.0439393
0.0710838 0.0440611
0.0122194 0.0357088
-0.016115 0.0422056
0.0752321 0.0414283
0.100097 0.0357401
0.0594297 0.0382666
0.124354 0.041136
-0.0295823 0.0343883
0.0594273 0.0434429
0.0387924 0.0426359
0.0416418 0.0387484
-0.0152606 0.0343565
0.0706155 0.0501778
0.00931417 0.0484243
-0.0417425 0.0333738
0.0385437 0.031875
-0.0161473 0.036826
0.0907408 0.0454921
-0.0449599 0.0440885
0.0594398 0.0510012
-0.00628221 0.0527516
-0.0188605 0.0399833
0.029557 0.0343214
0.0234754 0.0394636
-0.0189824 0.0320334
0.0267779 0.0367856
0.0113418 0.0338724
0.12432 0.0349768
-0.0529698 0.0384165
-0.0189824 0.0339084
0.0582651 0.037751
0.0496683 0.0504095
0.0989675 0.0432497
0.0977799 0.037243
0.0449639 0.0416209
0.0827282 0.0351438
-2.15513e-06 0.0318055
0.0217933 0.035258
0.0290868 0.0502929
0.0322983 0.0418179
0.0611197 0.0326553
0.0867202 0.0482807
0.0405236 0.0433184
0.000786701 0.0384706
-0.0529659 0.0325964
-0.0495734 0.0343886
0.121243 0.0430727
0.036794 0.038665
0.0813269 0.0382684
0.0464005 0.0422517
0.0594398 0.0469312
-0.041769 0.0378815
-2.64683e-07 0.0392479
0.027745 0.0392624
0.0661224 0.040236
-4.6184e-07 0.0392308
0.0397476 0.0421212
0.0871097 0.0409934
-0.0860366 0.0429495
0.0497671 0.0430388
0.125883 0.0483609
0.0373484 0.0454056
0.0594452 0.035215
0.0407515 0.0419322
0.0867067 0.0404652
0.0594398 0.0496211
0.081527 0.0387865
-0.0152556 0.0333414
0.0901952 0.040987
0.0964307 0.0426912
0.125157 0.0468213
0.0594273 0.0442485
0.0286795 0.0355615
-0.0807974 0.0440916
0.0477956 0.0534324
0.0424369 0.0412216
0.0800819 0.0419631
0.0321586 0.0432866
-0.10012 0.0376717
-0.0611171 0.0363416
0.0496683 0.049425
0.0317696 0.0409865
0.0860428 0.0346019
-0.014471 0.037837
-0.0857218 0.0322726
0.0527575 0.0401976
0.0594397 0.0463549
7.08095e-06 0.0371278
0.0764428 0.0379688
0.121243 0.0433862
0.0207304 0.0368347
0.0706155 0.0518042
0.100097 0.0349112
0.0416368 0.0328871
0.0301505 0.0494817
0.0261392 0.0561515
0.0853245 0.0408743
0.0497671 0.0418307
0.0716781 0.0352147
0.0396776 0.0380259
1.74905e-07 0.037212
0.0859466 0.0500598
0.0964772 0.0440821
0.0594396 0.049822
-1.1614e-09 0.0324858
0.0224432 0.0355003
0.0385441 0.0309425
0.0133907 0.0408781
0.0813321 0.0437737
0.0594396 0.0433231
0.0243594 0.0422345
0.109566 0.0431141
-0.0295559 0.0357538
0.0582589 0.0439291
0.0764328 0.0357016
0.0373475 0.0403559
0.0385427 0.0389775
-0.0106876 0.036792
-0.036223 0.0409884
0.0562849 0.049475
0.0170933 0.0341078
0.0394799 0.0404316
0.0813268 0.0382524
0.0247038 0.0430315
parameters: [ 8.496  1.503  3.953  1.422  4.918]. error: 208785912767.0.
----------------------------
epoch 0, loss 1.21129
epoch 128, loss 1.02797
epoch 256, loss 0.996742
epoch 384, loss 1.02168
epoch 512, loss 1.11145
epoch 640, loss 1.09629
epoch 768, loss 0.960158
epoch 896, loss 0.964452
epoch 1024, loss 1.07401
epoch 1152, loss 1.00185
epoch 1280, loss 0.951481
epoch 1408, loss 1.03978
epoch 1536, loss 0.894056
epoch 1664, loss 1.04545
epoch 1792, loss 0.967394
epoch 1920, loss 1.05451
epoch 2048, loss 0.803854
epoch 2176, loss 1.29911
epoch 2304, loss 1.00868
epoch 2432, loss 1.00027
epoch 2560, loss 0.888112
epoch 2688, loss 0.870427
epoch 2816, loss 0.746387
epoch 2944, loss 0.977054
epoch 3072, loss 0.931928
epoch 3200, loss 0.824478
epoch 3328, loss 0.84525
epoch 3456, loss 1.17806
epoch 3584, loss 0.9024
epoch 3712, loss 1.22349
epoch 3840, loss 1.0095
epoch 3968, loss 1.2419
epoch 4096, loss 0.990685
epoch 4224, loss 1.01023
epoch 4352, loss 1.21752
epoch 4480, loss 0.959904
epoch 4608, loss 0.954199
epoch 4736, loss 0.856394
epoch 4864, loss 0.994304
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0152533 0.031296
4.07609e-05 0.0296577
-0.0416423 0.0364286
-0.0659495 0.0345104
-0.0113401 0.0332379
0.0594397 0.0342443
0.0860523 0.0305436
0.0201879 0.0333741
-0.00236343 0.0341807
0.0273016 0.0352492
0.0989678 0.0379683
0.0424529 0.0349495
-0.0109842 0.0313596
0.0594398 0.0335395
0.0317699 0.0357611
0.0286795 0.0353765
0.0865811 0.0341813
0.0482638 0.0339586
0.043921 0.0358618
0.0593133 0.0357864
0.0208321 0.0322336
0.0271185 0.0307301
0.0109857 0.0323646
0.0594397 0.0332372
0.0271163 0.0262973
-0.0271126 0.0290172
-0.10012 0.0319339
0.0131861 0.0340546
0.000793148 0.0396953
0.0487559 0.0350983
0.0991788 0.0256561
0.0594396 0.0334522
0.0593758 0.0403664
-2.15513e-06 0.0393927
0.0251234 0.0336516
-0.0631232 0.0323486
0.0375381 0.0373345
-0.0621272 0.0288431
0.0594396 0.0338506
0.0920613 0.0391914
0.0800821 0.0324962
0.0582389 0.0402521
0.0333261 0.0403158
0.0317699 0.0347093
-4.71368e-07 0.0316305
0.0589127 0.0369146
0.0807993 0.0405659
0.0753057 0.037311
0.0594397 0.0342558
0.0497673 0.0344802
-0.0318748 0.0286143
0.0859465 0.0312831
-0.0362226 0.035288
0.032902 0.0367911
0.0594396 0.0290025
-0.0234677 0.0277973
0.0414075 0.033487
0.0458117 0.0318636
0.0594398 0.0287048
0.0211185 0.0399595
0.0753057 0.0385297
-3.91322e-05 0.0294814
0.0204119 0.0311973
0.0594427 0.0360004
0.0626002 0.0421628
0.0596272 0.0370997
0.0462156 0.0334744
0.028139 0.0346649
0.0594396 0.0336376
0.133575 0.0413387
0.0859466 0.0335196
0.0710838 0.0356561
0.0866799 0.039747
0.00250001 0.0352275
0.130005 0.0443534
-0.0271193 0.0328001
0.0937753 0.0344824
-0.0367964 0.0303644
0.0197009 0.0287938
0.0857284 0.0309178
-0.0295676 0.0364123
0.0305919 0.0375393
-0.00249422 0.0316683
6.39156e-06 0.0358375
0.100115 0.033189
0.0599612 0.0413002
-0.000792737 0.0260365
0.0407513 0.0362881
0.0208321 0.0330702
-0.0210896 0.0337488
0.0319945 0.0345578
0.0950802 0.0282217
0.0261391 0.0276268
0.032902 0.0358734
0.0870053 0.0374229
-0.00236357 0.0346752
0.0753057 0.037311
0.105506 0.039853
-0.0394796 0.0328101
-0.0323135 0.0297729
0.025123 0.032598
0.0234681 0.0308211
0.0188015 0.0383124
0.0594398 0.0300185
0.106629 0.0382186
0.025123 0.0387253
-0.00891927 0.0336461
0.135027 0.040067
0.0281391 0.031321
0.0950803 0.0280639
0.0989675 0.0378381
0.0907408 0.0388059
0.0594482 0.0406709
-0.0734426 0.0311874
-0.0417573 0.0315911
0.0989675 0.0367852
0.0416368 0.0360449
-1.63011e-07 0.0392372
0.0322982 0.0334858
0.0859465 0.0361577
0.0384877 0.0338009
0.059435 0.039294
0.0594396 0.0326618
0.100668 0.039015
-0.000793636 0.0315524
0.0131845 0.0312939
0.104819 0.0323441
0.026809 0.038841
parameters: [ 8.496  1.566  3.953  1.422  4.918]. error: 23541219.0548.
----------------------------
epoch 0, loss 1.20442
epoch 128, loss 1.11667
epoch 256, loss 1.21468
epoch 384, loss 0.962784
epoch 512, loss 0.911237
epoch 640, loss 0.976292
epoch 768, loss 1.04728
epoch 896, loss 0.851497
epoch 1024, loss 0.818384
epoch 1152, loss 0.968193
epoch 1280, loss 0.990395
epoch 1408, loss 1.08883
epoch 1536, loss 0.952169
epoch 1664, loss 0.748448
epoch 1792, loss 0.773415
epoch 1920, loss 0.845157
epoch 2048, loss 0.832726
epoch 2176, loss 1.05957
epoch 2304, loss 0.989466
epoch 2432, loss 0.896484
epoch 2560, loss 0.798877
epoch 2688, loss 0.878349
epoch 2816, loss 0.79244
epoch 2944, loss 0.809306
epoch 3072, loss 0.670041
epoch 3200, loss 0.944586
epoch 3328, loss 0.815681
epoch 3456, loss 0.963838
epoch 3584, loss 0.687966
epoch 3712, loss 1.08013
epoch 3840, loss 0.806791
epoch 3968, loss 0.794618
epoch 4096, loss 0.792621
epoch 4224, loss 0.806572
epoch 4352, loss 0.952142
epoch 4480, loss 0.77732
epoch 4608, loss 0.848692
epoch 4736, loss 0.893981
epoch 4864, loss 0.808383
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0144843 0.0198584
0.0113491 0.0271538
-0.0707062 0.0198625
-7.68032e-07 0.018417
0.0861488 0.0498202
-0.0367964 0.028562
0.0317696 0.0476793
0.100097 0.0447643
0.081527 0.0445084
0.0997462 0.0464523
0.0859466 0.045537
-0.0468045 0.0166198
-0.041769 0.0236293
0.0188018 0.0465739
0.0964305 0.0405101
-0.0631232 0.0126284
-0.0390966 0.0208633
0.0204119 0.0251104
-0.0412685 0.0256903
-0.036223 0.0136138
0.072741 0.0428308
-0.0621274 0.0158285
-3.11621e-07 0.0217737
-0.0318771 0.0316723
0.0220537 0.0496204
0.0594204 0.0428175
0.0458117 0.0469882
0.0267781 0.0425343
0.0417454 0.0108156
-0.0532643 0.0178566
0.143948 0.0552628
0.0237988 0.0511082
-0.0417488 0.011947
0.125863 0.0470388
0.0594397 0.048661
3.40452e-06 0.0162359
-0.0367964 0.0174619
0.0624794 0.0413579
-0.0495796 0.0220889
0.0237988 0.0518998
-0.0131845 0.0155734
0.0529666 0.0214379
0.0199307 0.0455574
0.0278228 0.0469473
0.036794 0.0468845
-0.0494977 0.0190625
0.0405236 0.0460293
0.0272988 0.0154175
-0.0414059 0.0208706
0.0991789 0.0567673
-0.0621274 0.0163482
0.01134 0.0202431
0.0482638 0.0501634
0.0851609 0.0570043
-0.0417624 0.0221854
0.0197009 0.0516567
1.74905e-07 0.0245853
-0.00460347 0.01435
-0.00699273 0.0472631
0.000213748 0.0370633
0.0749536 0.0459301
0.133575 0.037858
0.0867332 0.0457424
0.0337179 0.0549917
0.0594204 0.0388013
0.100097 0.047078
0.0373484 0.0493932
-0.00331175 0.0397273
0.0222151 0.0409157
0.0109517 0.019318
-0.014485 0.0210387
0.0174339 0.0486839
-3.76332e-06 0.0177827
0.0417629 0.0227763
0.0904958 0.0482099
0.09274 0.0574242
0.0941706 0.0471174
-0.0211014 0.0131638
0.0511643 0.0399996
0.0346447 0.0187057
0.112932 0.0440753
-0.00236348 0.0373915
0.0210915 0.0361494
0.090741 0.0473684
-0.0776845 0.0145444
-0.036223 0.0253477
0.0594349 0.0454978
-0.0300728 0.019233
-0.0132965 0.0504848
0.0977799 0.0391034
0.0594396 0.0410179
0.101441 0.0445255
-0.000786289 0.0254444
6.39156e-06 0.0221033
0.0237988 0.0511082
0.059181 0.0454579
0.0189852 0.020013
0.0237988 0.0407422
0.0191276 0.045572
-0.000835048 0.0258718
0.0871098 0.0449551
0.0594396 0.0479406
0.0897923 0.0500295
0.112932 0.0386115
0.0109586 0.017491
0.0424215 0.049402
0.0867334 0.0520915
0.05943 0.0466107
-0.0188599 0.0340697
0.0271109 0.0165981
0.01684 0.0396957
-0.0177216 0.0261725
0.0321632 0.047191
-0.0118768 0.0377226
-0.0467928 0.0216012
0.0594375 0.0451123
0.0412752 0.0183632
0.00250079 0.0138073
0.0871095 0.047857
0.0170933 0.0416504
0.0950805 0.0407467
0.125863 0.0470388
-0.0215356 0.0223811
0.0691072 0.048268
0.0243585 0.0443893
0.0317696 0.0483593
-0.0177216 0.02452
-0.0529722 0.0184426
parameters: [ 8.496  1.55   3.953  1.422  4.918]. error: 74076806435.5.
----------------------------
epoch 0, loss 0.954484
epoch 128, loss 1.05597
epoch 256, loss 1.10755
epoch 384, loss 1.11359
epoch 512, loss 0.963329
epoch 640, loss 1.13304
epoch 768, loss 1.00225
epoch 896, loss 0.833064
epoch 1024, loss 0.85913
epoch 1152, loss 1.05077
epoch 1280, loss 0.788851
epoch 1408, loss 1.24213
epoch 1536, loss 0.960494
epoch 1664, loss 1.15745
epoch 1792, loss 1.2256
epoch 1920, loss 0.95516
epoch 2048, loss 0.922043
epoch 2176, loss 1.07813
epoch 2304, loss 1.04797
epoch 2432, loss 1.01341
epoch 2560, loss 1.08638
epoch 2688, loss 0.862444
epoch 2816, loss 1.01845
epoch 2944, loss 0.969398
epoch 3072, loss 0.92816
epoch 3200, loss 1.06574
epoch 3328, loss 0.998074
epoch 3456, loss 0.918736
epoch 3584, loss 0.993245
epoch 3712, loss 1.00792
epoch 3840, loss 0.888423
epoch 3968, loss 0.769091
epoch 4096, loss 0.957939
epoch 4224, loss 0.846156
epoch 4352, loss 0.980619
epoch 4480, loss 0.825853
epoch 4608, loss 1.06457
epoch 4736, loss 0.956644
epoch 4864, loss 0.906653
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.00378297 0.0310744
-0.0248069 0.0239118
0.0405236 0.0316839
0.0815268 0.042131
0.027823 0.0424613
0.0865811 0.0396582
0.0133907 0.0350932
0.0144707 0.0262692
0.081527 0.031371
0.0626054 0.0314987
0.021525 0.0325545
0.0390991 0.0279559
0.0857789 0.0290041
2.41162e-06 0.0303642
-0.014485 0.0320293
-0.0462076 0.0256413
0.072741 0.0313706
0.0710835 0.0343608
0.0764428 0.033753
0.0648671 0.030435
0.0538297 0.0324408
0.0538297 0.0308644
-0.0234677 0.0317068
0.0416368 0.025016
0.0871095 0.0313051
0.0706156 0.035028
-0.0204101 0.0282298
0.0520969 0.0405468
0.0594396 0.0348705
-0.065961 0.0296663
0.0813321 0.0326635
-0.0215238 0.0290343
0.0605534 0.0302024
-0.0412711 0.0305858
0.072741 0.0366767
0.0412702 0.0333719
-0.0111732 0.024094
0.0122295 0.0257112
0.0327313 0.0342924
0.0594396 0.0313649
-0.0414014 0.0277084
0.0857789 0.0376196
0.0131858 0.027622
0.0853255 0.0351039
0.0278228 0.0446356
0.109566 0.036927
0.0541227 0.0308553
0.0983621 0.032248
0.00863088 0.0257642
-0.0462148 0.0341322
0.05943 0.0306214
1.01848e-06 0.0263036
0.0319944 0.0374382
0.0594473 0.0356142
0.0497673 0.0346794
0.0189868 0.0244514
-0.0144713 0.0251649
0.0527578 0.0304475
0.0330953 0.0287554
0.0710837 0.042434
-0.0117822 0.023996
0.0594324 0.0332892
0.0109781 0.0252105
0.0582236 0.0379389
1.74905e-07 0.0266897
0.01134 0.0250386
0.0594466 0.0342454
0.0734322 0.0304232
0.0267393 0.0251584
0.0855722 0.0313216
0.0920508 0.03714
0.0865811 0.0399984
0.000782366 0.0254195
-0.0131845 0.0266005
0.0122243 0.0342913
0.0237987 0.0499333
-0.0144706 0.0262216
-0.0776845 0.037519
-0.0385433 0.0242247
0.0626054 0.027274
0.0106903 0.0279927
-0.0209824 0.0229218
-0.0207342 0.0250401
0.106619 0.0337174
-0.0271153 0.0286583
-0.036223 0.0284633
0.0271185 0.0317691
0.049499 0.0290121
0.0910569 0.0401062
0.0594398 0.0368318
0.0286789 0.026126
0.0182076 0.0386763
-0.0131858 0.0252446
0.0851613 0.0370905
0.000834798 0.0280718
0.0199309 0.0324068
0.00863092 0.0294222
0.0224432 0.0385294
0.124354 0.0280573
0.00894003 0.0315147
-0.0385433 0.0323247
0.0329332 0.0333352
-0.00893962 0.0309355
0.0803921 0.0294513
0.0199307 0.0353964
-0.00460347 0.0225169
0.0911173 0.0410464
-0.0113401 0.0260486
0.0237987 0.0488129
0.088729 0.0375719
-0.0659495 0.0254069
-0.0365701 0.0302315
-0.0716682 0.025155
-0.0188062 0.0285695
0.0673482 0.0378664
0.0037846 0.0342599
-0.0807974 0.0281321
0.0319945 0.0373096
0.0859467 0.0387802
0.0675205 0.0321342
0.0753057 0.0265165
-0.0462076 0.0291909
-0.10011 0.0254631
0.0661222 0.0320527
0.0416434 0.0328874
0.0791961 0.0295772
0.0248197 0.0220339
-0.0468045 0.0300667
parameters: [ 8.496  1.531  3.953  1.422  4.918]. error: 90960134196.3.
----------------------------
epoch 0, loss 1.21721
epoch 128, loss 1.08777
epoch 256, loss 1.04493
epoch 384, loss 0.906606
epoch 512, loss 1.05563
epoch 640, loss 1.01654
epoch 768, loss 1.08735
epoch 896, loss 0.836925
epoch 1024, loss 0.913724
epoch 1152, loss 0.805445
epoch 1280, loss 1.15257
epoch 1408, loss 0.89488
epoch 1536, loss 0.950817
epoch 1664, loss 0.944141
epoch 1792, loss 1.15349
epoch 1920, loss 0.966622
epoch 2048, loss 0.916565
epoch 2176, loss 0.910238
epoch 2304, loss 1.07554
epoch 2432, loss 0.836606
epoch 2560, loss 1.02549
epoch 2688, loss 1.31773
epoch 2816, loss 0.805425
epoch 2944, loss 0.701094
epoch 3072, loss 0.942498
epoch 3200, loss 0.798532
epoch 3328, loss 1.09343
epoch 3456, loss 0.89026
epoch 3584, loss 0.981571
epoch 3712, loss 0.855846
epoch 3840, loss 0.992411
epoch 3968, loss 1.04773
epoch 4096, loss 1.00457
epoch 4224, loss 1.01609
epoch 4352, loss 0.839152
epoch 4480, loss 0.862371
epoch 4608, loss 0.806161
epoch 4736, loss 0.83652
epoch 4864, loss 0.930205
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0217933 0.0396504
-0.0827278 0.0272264
-0.000795203 0.0304572
0.0867334 0.0490222
-0.014471 0.0229369
-0.049248 0.0191005
0.00863088 0.043496
0.00863029 0.0304587
0.0234681 0.0243779
0.0710837 0.0448514
-1.32922e-07 0.02252
0.0272988 0.0264955
0.0611271 0.0289382
0.106634 0.0283641
-0.0131841 0.0217479
-0.036223 0.0221739
0.0865811 0.046848
0.0596272 0.0273478
0.0412752 0.0281861
0.0335492 0.0496793
-0.0215356 0.0214505
0.0330953 0.0402382
0.0594397 0.0517047
-0.036223 0.0205788
0.106619 0.0301886
0.0133878 0.027215
0.0188019 0.0280977
-0.0234626 0.0291368
0.072741 0.048546
0.0267781 0.0344003
0.0673482 0.0280945
-7.99663e-06 0.017928
0.0594376 0.0292564
-0.0462049 0.0259522
0.0390986 0.0231621
0.0594397 0.0540766
0.0815268 0.0404671
-0.0462148 0.020007
0.0871097 0.0567256
0.032195 0.0378123
0.0776735 0.0245283
0.0247806 0.0346757
-7.68032e-07 0.020151
0.0822309 0.0294671
0.0594397 0.0522268
0.0131849 0.0217636
0.0624794 0.0308376
0.0197009 0.0511021
0.0734439 0.0272341
0.0691068 0.0452246
0.0807987 0.021393
0.0966595 0.0418007
-0.0462076 0.0231679
0.0532535 0.0256185
0.0412725 0.031578
0.0037846 0.0299712
-0.0152583 0.0321997
-0.0662877 0.0159334
0.132184 0.0323283
0.0390991 0.0231639
0.109566 0.0504705
0.0594396 0.0450907
0.0496683 0.0531206
0.00932421 0.0473948
0.0272894 0.0344359
0.0529686 0.0245633
0.0594324 0.0354429
-0.00250418 0.0209857
0.0390986 0.0204236
0.0301505 0.0502376
0.0191276 0.0395881
0.0251233 0.0285852
0.0989677 0.0307443
2.27454e-05 0.0260284
-0.0204001 0.0361328
0.0707072 0.0211835
0.0710835 0.0501203
0.00863088 0.0371274
0.0621334 0.0200349
-0.000786289 0.0209929
-0.0362226 0.027293
0.022215 0.0286363
0.0416434 0.0287952
0.0283785 0.046052
0.0417489 0.0253413
0.0594397 0.0516341
0.130756 0.0382844
0.0373484 0.0330064
-0.00893962 0.0256802
0.0977799 0.0244717
1.01848e-06 0.0227249
-0.10011 0.0260631
0.0920561 0.0286955
0.0882822 0.0378485
-0.00236368 0.0498606
-0.0707062 0.0288924
0.0424215 0.0319914
-0.0204101 0.0323701
0.0865811 0.0488227
0.0385434 0.0209403
0.0177216 0.0312216
0.0340225 0.0206202
0.0210981 0.0286486
0.0941706 0.0384327
0.0152545 0.035412
0.125146 0.0341478
0.0290868 0.0478004
0.130756 0.042474
1.13647e-06 0.0306455
0.022215 0.0360746
0.00459996 0.0217089
-0.0707055 0.0200124
0.118684 0.0281283
0.0800819 0.0470989
0.0220538 0.0569677
0.0621308 0.0270086
0.0642209 0.0308345
0.0867332 0.0489964
0.0882822 0.0409437
-0.00484737 0.023332
0.09274 0.0405615
0.0215366 0.0284163
0.0329331 0.0571036
0.0648671 0.0308438
-0.00894125 0.0192609
0.0211031 0.027462
-1.92327e-05 0.018141
0.0859465 0.0533069
parameters: [ 8.496  1.544  3.953  1.422  4.918]. error: 1194043130.02.
----------------------------
epoch 0, loss 1.14394
epoch 128, loss 1.20113
epoch 256, loss 1.14562
epoch 384, loss 1.02774
epoch 512, loss 1.1005
epoch 640, loss 1.03615
epoch 768, loss 1.07232
epoch 896, loss 1.23233
epoch 1024, loss 0.973089
epoch 1152, loss 0.918164
epoch 1280, loss 0.929814
epoch 1408, loss 1.00347
epoch 1536, loss 1.09362
epoch 1664, loss 1.0656
epoch 1792, loss 1.01256
epoch 1920, loss 0.973547
epoch 2048, loss 1.04647
epoch 2176, loss 0.762014
epoch 2304, loss 1.03697
epoch 2432, loss 0.925006
epoch 2560, loss 1.13649
epoch 2688, loss 0.934968
epoch 2816, loss 0.957466
epoch 2944, loss 0.804001
epoch 3072, loss 1.01647
epoch 3200, loss 1.14646
epoch 3328, loss 0.97458
epoch 3456, loss 1.09024
epoch 3584, loss 1.06278
epoch 3712, loss 1.1136
epoch 3840, loss 0.810098
epoch 3968, loss 1.13852
epoch 4096, loss 0.90655
epoch 4224, loss 0.935787
epoch 4352, loss 1.05655
epoch 4480, loss 0.945882
epoch 4608, loss 1.03583
epoch 4736, loss 0.938676
epoch 4864, loss 1.01418
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0866799 0.0267289
0.080402 0.0280727
0.0673482 0.0308558
0.0594449 0.0359001
0.0122404 0.0311639
0.112932 0.0273598
0.0207331 0.029615
-0.00249422 0.0260705
-0.0621346 0.0260568
-0.0860431 0.0139184
0.0859466 0.0535146
0.0710837 0.0520734
0.0800821 0.0371725
0.0986864 0.0364139
0.0594402 0.0346485
0.0189868 0.0231941
-0.0417422 0.0220303
0.0861486 0.0480146
-0.0161572 0.0325292
0.032902 0.0347574
0.0941706 0.0340223
2.5668e-06 0.0227615
-0.0346352 0.0203864
0.0113515 0.0225283
0.0192074 0.0510329
-0.00733491 0.0349213
0.032902 0.036215
-1.92327e-05 0.0172739
-0.129457 0.022572
0.0134334 0.0295068
-0.129449 0.015765
0.0168394 0.0405553
0.0477958 0.0495451
0.0477958 0.0509051
-0.129445 0.0170798
0.0945151 0.035771
0.0724741 0.0419607
0.0820225 0.0358016
0.0131851 0.0309198
0.0335492 0.0435856
0.0997468 0.0372419
-0.000835377 0.0268556
0.100115 0.0243733
0.00131924 0.0248237
0.0596272 0.0332195
0.080402 0.0290508
0.0594467 0.0303164
0.00080867 0.021764
-0.0144706 0.0267456
0.0861488 0.0363897
0.0861488 0.0399333
-3.91322e-05 0.0307798
-0.00891927 0.0298438
0.121243 0.0337694
0.0188018 0.0380151
0.0243594 0.0412123
1.96444e-05 0.021769
0.0867334 0.0411489
0.0781227 0.0285868
0.0538009 0.027536
0.0210209 0.0360392
0.0937752 0.0396226
-0.0271152 0.0217814
0.029557 0.0263103
0.0215366 0.0283142
0.0319944 0.0479588
0.00596587 0.0358872
0.106645 0.0290766
-0.0734361 0.0307796
-0.0394796 0.025817
0.0966596 0.0329167
0.0927399 0.0500596
0.0897923 0.0499542
0.0870053 0.0366145
0.088729 0.0388301
0.0859865 0.0324975
0.0716773 0.0277123
0.0375434 0.0340192
0.0407515 0.0400082
1.96444e-05 0.0292751
0.0247038 0.0345337
-0.0207316 0.0360318
-0.0131845 0.0221719
0.00894166 0.025399
0.05943 0.0273431
0.0981549 0.0440755
0.0904958 0.0332998
0.0964766 0.0414477
0.0211183 0.0321057
0.0859465 0.0542502
0.0589765 0.036244
-0.0250779 0.0269841
0.121243 0.0470928
0.0764428 0.0343412
0.0224432 0.0429514
0.0271115 0.0228125
0.0462156 0.0321213
-0.0529682 0.0167461
0.0237988 0.04958
0.0267369 0.0276756
0.0631173 0.0233718
-0.0271152 0.0257413
1.01848e-06 0.0239496
0.0362237 0.0219656
0.000782366 0.0291476
4.17498e-06 0.0237843
0.061289 0.0339826
0.0207331 0.0242859
0.029557 0.0296151
-0.0394792 0.0172691
0.0594397 0.0509896
0.0368959 0.026293
0.0650734 0.0241393
0.0859465 0.0496757
-0.0152556 0.0281611
0.126206 0.0325396
0.0701141 0.0341808
0.0281391 0.0498058
0.0727411 0.0445854
0.0113515 0.0224049
-0.0662833 0.0276723
0.0122142 0.0342153
0.0594424 0.0319337
0.0594297 0.0335218
0.0199306 0.0314738
0.0927398 0.0515252
0.0594397 0.0476128
0.10664 0.0305762
parameters: [ 8.496  1.538  3.953  1.422  4.918]. error: 847946065.495.
----------------------------
epoch 0, loss 1.27285
epoch 128, loss 1.02096
epoch 256, loss 1.08056
epoch 384, loss 1.10803
epoch 512, loss 1.12409
epoch 640, loss 0.980179
epoch 768, loss 1.02364
epoch 896, loss 1.15839
epoch 1024, loss 1.12544
epoch 1152, loss 1.02149
epoch 1280, loss 0.99512
epoch 1408, loss 1.07291
epoch 1536, loss 0.991043
epoch 1664, loss 0.962774
epoch 1792, loss 1.08184
epoch 1920, loss 1.14307
epoch 2048, loss 1.06482
epoch 2176, loss 1.15333
epoch 2304, loss 0.803728
epoch 2432, loss 0.817109
epoch 2560, loss 0.851249
epoch 2688, loss 0.974852
epoch 2816, loss 1.3679
epoch 2944, loss 1.12814
epoch 3072, loss 0.889523
epoch 3200, loss 1.1486
epoch 3328, loss 0.980364
epoch 3456, loss 0.851868
epoch 3584, loss 1.15901
epoch 3712, loss 1.07984
epoch 3840, loss 0.981457
epoch 3968, loss 1.00756
epoch 4096, loss 0.976385
epoch 4224, loss 1.19421
epoch 4352, loss 0.894047
epoch 4480, loss 1.00842
epoch 4608, loss 1.10847
epoch 4736, loss 1.11719
epoch 4864, loss 0.849874
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0904958 0.0327013
0.0134225 0.0562505
0.0538009 0.0417744
0.0330948 0.032319
0.0781227 0.0376611
-2.64683e-07 0.0341893
0.0659505 0.0337787
-0.0131858 0.0381896
-0.0412685 0.03086
0.0424369 0.043493
0.109566 0.0358613
0.0753057 0.035922
-3.11149e-07 0.041805
0.0937752 0.0452623
0.0594397 0.0196589
-0.0106876 0.0398642
-0.0188599 0.033745
0.0594424 0.0491517
0.129458 0.0388065
0.081797 0.0297817
0.0710838 0.0377914
0.0417702 0.0259954
0.0209836 0.0367907
0.023648 0.0307082
0.0964305 0.0347564
0.0497674 0.0284895
0.105506 0.0595137
0.0204045 0.0334695
0.000802697 0.038708
0.0384877 0.0432333
0.000793148 0.0498387
-0.0385431 0.0389168
0.0329332 0.0289393
0.0582389 0.0447447
0.0384777 0.0457043
0.0329332 0.0343109
0.0417583 0.0376427
0.0807983 0.039727
0.00931417 0.0365208
-0.0394792 0.0359516
0.0642209 0.0503227
0.0594452 0.0528283
0.102035 0.0425915
0.0594395 0.0402992
-0.0417488 0.0367836
0.028139 0.0404407
0.0370769 0.0370246
0.0950804 0.0342227
0.0394803 0.0300673
0.0436381 0.0378064
-0.0385427 0.0310824
0.00891968 0.0375625
-0.041769 0.0371344
-0.0449599 0.0315508
0.086885 0.0387653
-0.080798 0.0388448
0.021525 0.0364811
0.0495768 0.0281042
0.126086 0.0400761
2.27454e-05 0.0377066
0.0813269 0.0404377
0.0781223 0.0428265
-2.64683e-07 0.0299192
-0.0346352 0.0406139
0.0482639 0.0279561
0.0247829 0.032155
0.0375434 0.0326267
0.0237988 0.0357781
0.0691072 0.0297718
0.0337178 0.0306163
0.0964772 0.0343915
-0.0211014 0.0300984
-0.0118765 0.0400257
0.0859466 0.030971
0.0305919 0.0397905
-3.11621e-07 0.0277317
-0.0215238 0.0367814
0.036794 0.0457813
0.100115 0.0332995
0.0791961 0.0434339
-0.0417425 0.037572
-0.0118768 0.0395368
0.0621308 0.0350213
-2.86114e-05 0.0389478
0.112932 0.0401966
0.0594399 0.0359375
-0.0194897 0.02941
0.0710835 0.0373821
-0.0209708 0.0340585
0.018802 0.0376268
0.0594396 0.032072
0.0237988 0.0357781
0.0871097 0.0408727
0.0661223 0.0409846
0.0468012 0.0383421
0.0867067 0.0331023
0.0271187 0.0226423
0.0122352 0.0421243
0.0326469 0.0435917
0.0346353 0.0302062
-0.0189864 0.0252016
-0.0109566 0.0310192
-0.0385424 0.0366889
0.0599612 0.0356842
0.121243 0.0374646
0.0109781 0.0356327
0.0707072 0.0318488
0.00863032 0.0416926
0.044975 0.0482446
-0.0207367 0.039071
0.0188016 0.0479902
0.0867119 0.0378665
-0.000835392 0.0409633
-0.00549668 0.0424515
0.0624794 0.0516355
0.0461382 0.0405412
0.0871097 0.0411377
0.0327314 0.0356681
0.109566 0.0353631
0.0707062 0.0371516
0.130756 0.0419757
0.0870049 0.0380974
0.0319944 0.0389126
0.0562597 0.0505044
-0.00459356 0.0368125
0.0449751 0.0493115
0.0752422 0.0301974
-0.0210969 0.0312163
parameters: [ 8.496  1.538  4.953  1.422  4.918]. error: 1118945.72947.
----------------------------
epoch 0, loss 1.16966
epoch 128, loss 1.42876
epoch 256, loss 1.38964
epoch 384, loss 1.20227
epoch 512, loss 1.1243
epoch 640, loss 1.32683
epoch 768, loss 0.959503
epoch 896, loss 0.986555
epoch 1024, loss 1.39512
epoch 1152, loss 1.13846
epoch 1280, loss 1.55817
epoch 1408, loss 1.22901
epoch 1536, loss 1.14999
epoch 1664, loss 1.06792
epoch 1792, loss 1.15748
epoch 1920, loss 1.30974
epoch 2048, loss 1.01294
epoch 2176, loss 1.03823
epoch 2304, loss 1.21081
epoch 2432, loss 1.06636
epoch 2560, loss 1.47343
epoch 2688, loss 1.24028
epoch 2816, loss 1.24776
epoch 2944, loss 1.20501
epoch 3072, loss 1.00591
epoch 3200, loss 1.23619
epoch 3328, loss 1.32647
epoch 3456, loss 1.12525
epoch 3584, loss 1.2078
epoch 3712, loss 1.161
epoch 3840, loss 1.26867
epoch 3968, loss 1.3356
epoch 4096, loss 1.30003
epoch 4224, loss 1.47322
epoch 4352, loss 1.30452
epoch 4480, loss 1.15384
epoch 4608, loss 1.16648
epoch 4736, loss 1.51951
epoch 4864, loss 1.32434
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0318695 0.00675464
-1.92327e-05 0.0188387
0.00932421 0.0126818
0.0594398 0.00989961
0.0272894 0.0167936
0.0207193 0.0211311
0.130757 0.00864964
0.0271204 0.0155
0.100097 0.0107474
0.0596272 0.010912
0.0621308 0.0139849
0.0384877 0.00963902
0.102035 0.0101608
0.0604514 0.0199654
0.0594397 0.011787
-0.0365701 0.0222227
0.0271115 0.00718022
0.0989677 0.0176495
0.0394799 0.0153032
-3.91322e-05 0.0175641
-0.0860366 0.0190055
0.0407515 0.00892408
0.0867119 0.0116954
0.0305923 0.0154663
-6.66928e-06 0.0173074
0.0301504 0.0137887
0.0144846 0.0120364
0.0191276 0.0148992
0.028139 0.0221341
0.0207304 0.00962615
0.0594396 0.0122084
0.0321468 0.0198652
-0.0247813 0.0121892
-0.0109566 0.0138811
0.0318695 0.00675629
0.0599029 0.00931357
-0.0271152 0.0109368
0.0191272 0.01023
0.0904958 0.0201505
0.0261394 0.00994478
-0.0417573 0.0105196
0.0538015 0.00752815
0.0964307 0.0191851
-0.0346424 0.00974028
0.0870053 0.0073088
0.0594398 0.0116873
0.081527 0.0200928
0.00131924 0.0130963
0.0941706 0.0138382
0.0495801 0.0135061
0.0131851 0.00818622
0.00131924 0.00850703
0.0691986 0.0240043
0.0594399 0.0164434
0.0482639 0.0137407
0.132178 0.0202291
-0.0144843 0.0120186
0.0224432 0.0181962
0.0594396 0.0120081
-0.0734384 0.027815
-1.32922e-07 0.0153377
-0.0247813 0.00868301
0.0496682 0.00767109
-5.97989e-06 0.0122943
0.0626054 0.0058553
0.0424268 0.0181277
0.123121 0.00911226
0.0859467 0.0192696
-0.0161473 0.0161218
-0.0194973 0.0184842
-0.0323135 0.0110837
-0.0248069 0.0190406
0.0594399 0.0137387
-0.129453 0.0217991
-0.0234677 0.0124409
0.0800819 0.00793588
-0.032317 0.0132323
0.018802 0.0173891
0.0113491 0.0136343
0.0851612 0.0122867
0.0907409 0.0144328
-0.00331175 0.00821064
0.0594273 0.0150334
0.0851609 0.0180631
0.126171 0.0134776
0.0278229 0.00679498
0.0857789 0.0221022
0.059435 0.0153208
0.0594396 0.0132547
0.0781227 0.0100044
-0.0300728 0.0218312
0.0234704 0.013684
0.0871096 0.0083983
0.0594396 0.0117868
0.046209 0.00861555
-0.0113473 0.0254334
0.0305923 0.0183887
-3.17974e-05 0.0123529
-0.00460347 0.0290751
0.0776855 0.0113612
4.07333e-06 0.00889808
0.0375434 0.00966293
0.00931426 0.0105042
-0.0247813 0.0100214
-0.000835063 0.0134287
0.0867332 0.00904934
-0.0394796 0.0194035
0.0407515 0.0148512
0.0461385 0.0154605
0.0290868 0.0141851
-0.0611267 0.00923892
0.0594494 0.0101235
0.0800821 0.0153328
0.0707072 0.0108676
-0.0204122 0.00989624
0.0945153 0.0114995
0.0582651 0.00751701
0.0594398 0.0164842
0.0210981 0.0157898
0.0248169 0.0255108
0.0326439 0.00840086
-0.000801651 0.00982244
0.000782366 0.0199815
0.0268195 0.0206792
0.0322982 0.0157091
0.0305923 0.0139455
0.0716781 0.013667
0.0730676 0.0171734
parameters: [ 8.496  1.538  6.572  1.422  4.918]. error: 2230327.43361.
----------------------------
epoch 0, loss 1.23309
epoch 128, loss 1.36193
epoch 256, loss 1.27878
epoch 384, loss 1.48017
epoch 512, loss 1.18987
epoch 640, loss 1.56053
epoch 768, loss 1.75635
epoch 896, loss 1.34674
epoch 1024, loss 1.34544
epoch 1152, loss 1.49545
epoch 1280, loss 1.38835
epoch 1408, loss 1.55709
epoch 1536, loss 1.48519
epoch 1664, loss 1.32439
epoch 1792, loss 1.49338
epoch 1920, loss 1.37603
epoch 2048, loss 1.6207
epoch 2176, loss 1.62493
epoch 2304, loss 1.13892
epoch 2432, loss 1.46702
epoch 2560, loss 1.63155
epoch 2688, loss 1.40667
epoch 2816, loss 1.34882
epoch 2944, loss 1.18731
epoch 3072, loss 1.12236
epoch 3200, loss 1.36357
epoch 3328, loss 1.47023
epoch 3456, loss 1.32554
epoch 3584, loss 1.20442
epoch 3712, loss 1.14271
epoch 3840, loss 1.30328
epoch 3968, loss 1.16921
epoch 4096, loss 1.40707
epoch 4224, loss 1.10196
epoch 4352, loss 1.28521
epoch 4480, loss 1.34162
epoch 4608, loss 1.17525
epoch 4736, loss 1.28401
epoch 4864, loss 1.24399
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.059435 0.057458
0.0716781 0.0510986
0.0901949 0.027737
-0.0295823 0.0312754
0.121243 0.0206389
2.05834e-07 0.030858
0.0191272 0.0659653
0.0520969 0.0357279
0.0594759 0.0437183
0.0538297 0.0417841
0.0461384 0.0777924
0.0197009 0.0369341
0.046799 0.0499524
0.0508582 0.0437341
0.0220537 0.0565624
0.085161 0.0378723
-0.0144836 0.06583
-0.0188606 0.0533664
3.62028e-05 0.0423442
-0.0131858 0.0601961
0.0594397 0.0389534
0.059407 0.0468187
-0.0109774 0.0378978
0.046209 0.0607727
0.0594398 0.0603711
0.0859467 0.023656
0.00596546 0.0587977
0.0317699 0.0543814
-0.0323135 0.050312
-2.15513e-06 0.0450539
0.0589763 0.0561293
0.0783558 0.0747535
0.0131845 0.0587089
0.0188609 0.0539777
-0.0131855 0.0463727
2.5668e-06 0.0349053
0.0593312 0.0348748
0.102035 0.037421
0.0626002 0.0508169
0.102034 0.0468923
0.0813268 0.0478269
0.0194981 0.0360751
0.100097 0.0606597
-0.0209824 0.0596765
0.0867334 0.0612646
0.00080867 0.0504433
1.74905e-07 0.0241757
0.0604514 0.059037
-0.0827278 0.055019
0.088729 0.039522
0.0318695 0.0205314
-0.0131855 0.059715
0.000835111 0.0524447
-0.0362226 0.0571627
-0.0177194 0.0421411
-3.40966e-05 0.0426115
0.132184 0.0366282
0.0113491 0.0632192
0.10955 0.0705918
0.0594273 0.0408369
0.0286789 0.0244182
0.106634 0.0503322
0.0329021 0.0540987
0.0594397 0.0522132
0.0538297 0.0417841
0.0981543 0.0551002
0.0321468 0.0540493
0.0807977 0.0745215
0.00080867 0.0497415
-0.0494953 0.027501
0.0373475 0.0546913
-0.0495737 0.0524121
0.0482638 0.0283612
-0.0106876 0.0491495
-0.0807974 0.0621534
0.0867199 0.0628756
0.0346452 0.052393
0.0734322 0.0512646
0.0327314 0.0457263
0.0691984 0.0368121
0.0267781 0.0594227
0.0482639 0.0283551
0.0237988 0.00433524
0.0781223 0.0513488
0.0346353 0.0464268
0.0326439 0.0350692
0.0321897 0.0317875
0.0205116 0.0578964
0.088282 0.052694
0.0599031 0.0427852
-3.40966e-05 0.0430957
0.0317696 0.0565303
0.0268142 0.0616565
-0.0707068 0.0571833
0.0977802 0.0571875
0.0599029 0.0561289
0.0582589 0.0453204
0.0327313 0.0457288
-0.0318748 0.046428
0.00596546 0.0481832
-0.000801651 0.0525941
0.0920613 0.0625957
0.0424316 0.0695441
0.0140549 0.0321366
-0.0152556 0.0278007
0.0971043 0.0561756
4.07609e-05 0.05352
0.0538015 0.0645206
0.0384776 0.0558151
0.0321586 0.0470986
0.0594376 0.0517184
-0.0412757 0.0296338
0.076417 0.0445066
0.0594398 0.0080412
-0.0248165 0.041363
0.0199307 0.0565322
-0.00460347 0.0563918
0.0791969 0.0453146
0.0594399 0.0189088
8.39808e-07 0.0242556
0.0268195 0.0688401
0.0911183 0.0709882
0.0764118 0.0554323
0.0168394 0.0453167
0.0716773 0.0511083
0.0800819 0.0431061
0.0594396 0.0777071
0.0945151 0.0728774
parameters: [ 8.496  1.538  4.953  1.422  4.918]. error: 87022240917.6.
----------------------------
epoch 0, loss 1.11713
epoch 128, loss 1.23547
epoch 256, loss 1.26517
epoch 384, loss 1.14357
epoch 512, loss 1.14736
epoch 640, loss 1.05352
epoch 768, loss 1.11628
epoch 896, loss 1.19944
epoch 1024, loss 1.14859
epoch 1152, loss 0.936317
epoch 1280, loss 1.07923
epoch 1408, loss 1.51588
epoch 1536, loss 1.0347
epoch 1664, loss 1.35443
epoch 1792, loss 1.17653
epoch 1920, loss 0.873149
epoch 2048, loss 1.03688
epoch 2176, loss 0.948628
epoch 2304, loss 1.16738
epoch 2432, loss 1.2976
epoch 2560, loss 1.26784
epoch 2688, loss 1.08045
epoch 2816, loss 1.19018
epoch 2944, loss 1.02733
epoch 3072, loss 1.19238
epoch 3200, loss 1.14792
epoch 3328, loss 1.25084
epoch 3456, loss 0.953836
epoch 3584, loss 1.17562
epoch 3712, loss 1.05494
epoch 3840, loss 1.11804
epoch 3968, loss 1.0599
epoch 4096, loss 1.19994
epoch 4224, loss 0.948727
epoch 4352, loss 0.937496
epoch 4480, loss 1.05914
epoch 4608, loss 1.2238
epoch 4736, loss 1.11344
epoch 4864, loss 1.00872
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0871096 0.0511348
0.0818743 0.0174816
-0.00131975 0.0353077
0.0131845 0.0367777
0.0991788 0.021016
-0.0417573 0.05583
0.0599031 0.0442003
0.0907408 0.0150467
0.0594273 0.0395096
0.0496682 0.0185082
0.0093143 0.046802
-0.0271126 0.0400884
0.0278228 0.0411181
0.0866747 0.0401263
0.0594396 0.0344503
0.0594396 0.0224563
-0.000801651 0.0324263
0.0800819 0.0198488
-0.0346352 0.049204
0.0337182 0.031556
-0.0716682 0.0174011
0.0827282 0.0290128
0.029557 0.0476842
0.00891968 0.0263495
0.0188015 0.0246896
0.0467944 0.0434856
0.0204045 0.0541765
-0.0390959 0.03459
0.102034 0.0333179
0.000213748 0.0252836
0.0168394 0.0443681
-3.76332e-06 0.0267904
0.0174339 0.0410294
-0.0295559 0.0365557
0.0412702 0.0159145
0.093775 0.0380539
0.0716773 0.0210133
0.0968257 0.0267467
0.0950805 0.0327205
0.0589763 0.0360799
2.41162e-06 0.037994
0.0661224 0.0386708
0.0650734 0.0399566
-0.0707068 0.0515379
0.0188086 0.0399578
0.00863029 0.0359052
0.0317698 0.0510891
0.0508582 0.0184179
0.036794 0.0189229
0.0449751 0.0355662
0.121243 0.0538502
0.0734439 0.0466041
-2.23337e-05 0.0254078
0.0532578 0.0259116
0.0327313 0.0382164
0.088729 0.0417924
0.086885 0.0328829
0.0373484 0.035247
-0.0716787 0.0137652
-0.0152556 0.0486546
0.0541227 0.0240578
2.27454e-05 0.0306479
0.0122295 0.0334682
0.0205116 0.0499595
0.0496683 0.0219519
0.0251233 0.0153044
-0.0707055 0.0310549
0.0650733 0.0364955
0.0412776 0.0315589
0.0691068 0.046057
0.0197009 0.0312154
0.0461385 0.0308715
-1.48505e-07 0.0319073
-1.1614e-09 0.0217521
0.0594395 0.0286429
8.40829e-06 0.0449652
0.079197 0.0380916
0.00863092 0.0434133
0.0859467 0.0210231
-0.0144843 0.0133221
-0.00733491 0.0326408
0.0439209 0.0429917
0.0739226 0.0374377
0.0599031 0.0482812
0.0870049 0.0599819
0.0707062 0.0314303
0.00460075 0.0187338
0.0191272 0.0427681
0.0927396 0.0258
0.0396776 0.0283567
-0.0416401 0.0496502
0.0329332 0.00771299
0.126222 0.0440032
0.0691987 0.0209046
0.0261393 0.0149629
0.0861486 0.0401781
0.106645 0.0247728
0.121243 0.0394905
0.0594396 0.0470594
0.0424428 0.0199468
0.0417629 0.0424612
0.0397474 0.0458333
-0.0346352 0.030096
0.0122142 0.0412129
0.0594251 0.0175555
0.0449751 0.0356295
0.0710835 0.0184551
-0.014471 0.0208151
-0.00378521 0.0460867
-0.0152606 0.0450083
-0.0385421 0.0118576
0.0329331 0.017197
0.0562597 0.0330545
0.0851511 0.0257014
0.0532535 0.0174363
0.0211185 0.0304367
0.0223972 0.0195471
0.0412725 0.0243698
0.0599031 0.0568623
-0.0631232 0.017533
0.0329021 0.0273889
0.0375382 0.0154092
0.0861487 0.0361156
0.0594452 0.0217312
-0.0118768 0.0414007
0.076422 0.0405699
0.0375382 0.021589
0.0764118 0.0405665
parameters: [ 8.496  1.538  5.572  1.422  4.918]. error: 311543211.639.
----------------------------
epoch 0, loss 1.09304
epoch 128, loss 1.1164
epoch 256, loss 1.12583
epoch 384, loss 1.1898
epoch 512, loss 1.20002
epoch 640, loss 1.28417
epoch 768, loss 1.33754
epoch 896, loss 1.37091
epoch 1024, loss 1.212
epoch 1152, loss 1.28986
epoch 1280, loss 1.3589
epoch 1408, loss 1.26641
epoch 1536, loss 0.977251
epoch 1664, loss 1.09673
epoch 1792, loss 1.03299
epoch 1920, loss 1.26346
epoch 2048, loss 1.14787
epoch 2176, loss 1.07539
epoch 2304, loss 0.945036
epoch 2432, loss 1.04042
epoch 2560, loss 1.08161
epoch 2688, loss 1.12473
epoch 2816, loss 0.950283
epoch 2944, loss 0.991532
epoch 3072, loss 1.22538
epoch 3200, loss 1.15551
epoch 3328, loss 1.06348
epoch 3456, loss 1.15832
epoch 3584, loss 1.09279
epoch 3712, loss 1.39601
epoch 3840, loss 1.14495
epoch 3968, loss 1.1103
epoch 4096, loss 1.31099
epoch 4224, loss 1.30514
epoch 4352, loss 0.98873
epoch 4480, loss 0.987511
epoch 4608, loss 0.967704
epoch 4736, loss 0.909131
epoch 4864, loss 1.20997
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0414033 0.0554071
-0.0109774 0.0291132
0.0205116 0.0245118
0.0220537 0.0449043
-0.0417422 0.056051
-0.0631103 0.0465938
0.0435741 0.0237394
-3.40966e-05 0.0421427
0.0691072 0.0267634
0.0661225 0.0414127
0.0986864 0.0511611
-2.95455e-07 0.0398849
-0.0734361 0.0256354
0.0857794 0.0611103
0.088729 0.029025
0.0941706 0.0257932
-0.0271157 0.0500037
0.135017 0.0499109
0.0271185 0.0439081
0.0855721 0.0390892
0.0977799 0.0229329
0.000796249 0.0430923
0.0106903 0.0296705
0.0857284 0.0362759
0.0594396 0.044665
0.0752422 0.0268984
0.0981549 0.044241
-0.0117822 0.032931
0.0594398 0.0477054
0.000796249 0.0504709
-0.0209708 0.0576099
0.0396778 0.0258014
0.0330948 0.0481801
0.0093143 0.0397444
0.0977802 0.0455205
0.0964772 0.0611382
0.0589127 0.0413422
4.07609e-05 0.039237
-0.0716686 0.036981
0.0562597 0.0345637
8.40829e-06 0.044678
-0.00701261 0.0616621
0.0261392 0.0471868
0.0910567 0.0364837
0.0626054 0.0488014
0.0716781 0.0549594
0.0321467 0.0361707
-0.0385421 0.0480589
0.0131861 0.0428697
0.0414001 0.0621772
0.102034 0.0385439
0.00131924 0.0356368
0.0210212 0.0449691
-0.0414059 0.0595697
0.0497673 0.0275916
0.0318787 0.0615207
0.0977802 0.0338518
-0.053257 0.0347427
0.0340225 0.0270942
-0.065961 0.0436427
-0.0152533 0.0381125
0.0707072 0.0385364
0.0223972 0.040976
-0.0189824 0.0302174
0.0859467 0.0547609
0.0385437 0.0326098
-0.0414014 0.0407285
0.0859467 0.0419322
-0.0177216 0.0307469
0.0989678 0.0295776
0.0248097 0.0481
-0.00131656 0.0400014
3.82209e-05 0.0371483
0.106629 0.0238249
0.0977801 0.0247681
0.0131845 0.0442652
0.0911183 0.0371039
-0.0412685 0.0385458
0.0859865 0.0507985
0.0857794 0.0478451
0.0511643 0.0479403
0.121243 0.0291533
0.0594397 0.040467
-0.0707068 0.0216804
0.0271163 0.0377906
0.0781227 0.0318786
0.0261394 0.0335822
0.0859466 0.0462381
0.0870049 0.0494969
0.0191276 0.0541915
-0.036223 0.0304125
-3.91322e-05 0.0420444
0.0594396 0.0359792
0.12614 0.0535916
0.109566 0.0298145
0.027823 0.0310438
0.0605996 0.0448844
0.109549 0.0477097
0.0477959 0.0673288
0.0109857 0.0266989
0.0188609 0.0449597
2.05834e-07 0.0302505
0.106624 0.0273115
-0.0385424 0.0372798
-0.0662867 0.0399112
-0.0417425 0.0574988
0.0887289 0.0292692
0.101814 0.0310286
0.0813321 0.0477969
0.0396776 0.0333169
0.0857284 0.0466025
0.00931426 0.0315733
0.0734439 0.0289648
0.0989677 0.0335296
0.0468012 0.0311418
0.0495801 0.0409727
-0.0117783 0.0354122
0.124311 0.0397899
0.0117845 0.0440274
0.0589764 0.0559193
0.0901952 0.026104
0.0621334 0.0527462
0.094504 0.0550907
0.104819 0.0560353
-0.0188599 0.0424617
0.0204119 0.028619
0.0727411 0.0611405
-0.0346416 0.0444809
parameters: [ 8.496  1.538  5.953  1.422  4.918]. error: 82802280981.1.
----------------------------
epoch 0, loss 1.12321
epoch 128, loss 1.03097
epoch 256, loss 0.96968
epoch 384, loss 0.971999
epoch 512, loss 1.16215
epoch 640, loss 1.00255
epoch 768, loss 0.940193
epoch 896, loss 0.947942
epoch 1024, loss 0.983502
epoch 1152, loss 1.1341
epoch 1280, loss 1.21194
epoch 1408, loss 1.14582
epoch 1536, loss 1.20549
epoch 1664, loss 0.994532
epoch 1792, loss 1.0761
epoch 1920, loss 0.877278
epoch 2048, loss 0.88566
epoch 2176, loss 0.988243
epoch 2304, loss 1.02034
epoch 2432, loss 1.04323
epoch 2560, loss 1.40724
epoch 2688, loss 0.86959
epoch 2816, loss 1.07417
epoch 2944, loss 1.16963
epoch 3072, loss 1.14751
epoch 3200, loss 0.882902
epoch 3328, loss 1.04726
epoch 3456, loss 1.13883
epoch 3584, loss 0.92655
epoch 3712, loss 0.885269
epoch 3840, loss 0.956454
epoch 3968, loss 0.928722
epoch 4096, loss 1.0908
epoch 4224, loss 1.02071
epoch 4352, loss 1.05612
epoch 4480, loss 0.97326
epoch 4608, loss 0.892361
epoch 4736, loss 1.11515
epoch 4864, loss 1.25369
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.072741 0.0340033
-0.0117788 0.0289777
-0.014471 0.0328122
0.0626002 0.0321137
0.01134 0.0442047
0.0394799 0.0381274
0.00596587 0.0303368
0.0707072 0.031963
0.0538009 0.0367301
-0.0177216 0.0388543
-0.0161572 0.0448391
-0.0860366 0.040306
0.0317699 0.0461254
0.019701 0.0333955
-0.000801651 0.0288717
0.0706156 0.0434152
0.088729 0.041127
0.0594399 0.037958
-0.0385427 0.0377179
0.0859467 0.0348919
0.0594116 0.0365768
0.0781227 0.0445956
0.0594398 0.0417027
-0.0118765 0.033401
0.0477959 0.0496764
1.13647e-06 0.0295672
0.0966596 0.0489635
0.0329021 0.0397315
0.0776855 0.0351131
0.0414025 0.0410809
-1.63011e-07 0.0320033
0.0901952 0.0438287
0.0468064 0.0242618
-0.036223 0.0297016
0.0322983 0.0550465
0.0174335 0.0402677
-0.0734384 0.0392147
-0.0117783 0.0258613
-0.0144706 0.0298975
0.0407513 0.0459896
0.0322983 0.0538606
0.0594396 0.0398065
0.0267369 0.0410435
0.0300757 0.035915
0.0710838 0.0379277
-0.0118765 0.0458815
0.0267369 0.0448633
0.0964307 0.0464016
-0.0716782 0.0360253
-5.97989e-06 0.0333668
0.0807993 0.0329779
-0.0204101 0.0297057
0.0243585 0.0415153
0.106629 0.0426528
0.0414025 0.0389389
-0.0189887 0.0333623
0.0626106 0.0255524
-6.30661e-07 0.0373131
0.0927399 0.0387992
0.0281393 0.0496115
0.0860364 0.0276346
0.0691072 0.0428598
0.0691068 0.0397497
-0.00893962 0.0410774
0.00131949 0.025825
0.0318695 0.0517069
0.0261391 0.0444555
0.0853245 0.0327676
0.0594399 0.0493922
0.0594396 0.0421115
-0.039089 0.0226368
0.105503 0.0365306
0.0724741 0.0516747
-0.0207292 0.0375075
-0.0109489 0.0417346
0.0532557 0.0312563
0.0424529 0.0406558
0.0897923 0.0404097
0.0706156 0.0446367
0.123121 0.0318933
0.0329331 0.0463225
0.0911183 0.0490591
0.0594397 0.0543289
1.96444e-05 0.0274124
0.0927396 0.0341055
0.0710835 0.0457613
-0.0707068 0.0298449
0.0989675 0.0312307
0.0243586 0.0533892
0.0861486 0.0406011
0.0385437 0.0312226
-0.0023637 0.0347126
0.0860364 0.0321438
0.0529686 0.0374929
-0.00131656 0.03974
0.126086 0.0341862
0.0594397 0.0524472
0.0417448 0.0393255
0.032195 0.0281535
0.0204028 0.030562
0.0424268 0.0432983
0.106634 0.0464387
0.0594395 0.0468364
0.049499 0.035686
0.0691072 0.0379969
0.00143008 0.0391558
0.0271163 0.040447
-0.0495796 0.0301256
0.0594398 0.0426616
0.0209836 0.0476033
0.0901948 0.0336009
-0.00737805 0.034645
0.00143008 0.031997
0.0251231 0.0386399
0.0416368 0.0299865
-0.053257 0.0442682
0.0170933 0.0454355
0.021525 0.0373817
0.0968257 0.0485514
0.0871097 0.0266084
-0.0340213 0.0313657
0.0950802 0.0435767
-0.0204001 0.0401103
0.0865812 0.0537532
0.0412752 0.0336089
0.0182077 0.036875
0.0594399 0.0453766
0.0492474 0.0363038
parameters: [ 8.496  1.538  5.459  1.422  4.918]. error: 1091758899.82.
----------------------------
epoch 0, loss 1.38592
epoch 128, loss 1.061
epoch 256, loss 1.48307
epoch 384, loss 1.34335
epoch 512, loss 1.22021
epoch 640, loss 1.40748
epoch 768, loss 1.14988
epoch 896, loss 1.32523
epoch 1024, loss 0.956261
epoch 1152, loss 0.832175
epoch 1280, loss 1.01703
epoch 1408, loss 1.13057
epoch 1536, loss 0.913778
epoch 1664, loss 1.17297
epoch 1792, loss 1.42884
epoch 1920, loss 1.38272
epoch 2048, loss 1.08273
epoch 2176, loss 1.09936
epoch 2304, loss 0.971494
epoch 2432, loss 1.01046
epoch 2560, loss 1.01801
epoch 2688, loss 1.02256
epoch 2816, loss 1.12472
epoch 2944, loss 0.97951
epoch 3072, loss 1.08555
epoch 3200, loss 1.18293
epoch 3328, loss 1.02182
epoch 3456, loss 1.48166
epoch 3584, loss 1.04456
epoch 3712, loss 1.04431
epoch 3840, loss 1.38307
epoch 3968, loss 1.12356
epoch 4096, loss 0.879404
epoch 4224, loss 1.0829
epoch 4352, loss 0.964489
epoch 4480, loss 1.13685
epoch 4608, loss 1.21591
epoch 4736, loss 1.05301
epoch 4864, loss 1.20313
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0860431 0.0485574
0.0191276 0.0539334
0.0631173 0.0491265
-0.00893962 0.0499661
0.0749536 0.0501139
0.000796249 0.0529524
-0.0318771 0.052263
0.0625951 0.0628439
0.022444 0.0515894
0.0589765 0.0533695
0.0791961 0.0547973
0.0412725 0.0531949
-0.0250779 0.0509691
0.0631196 0.0476865
0.0897923 0.0547658
0.0861486 0.0559205
0.0417454 0.0516525
0.0897923 0.0563202
0.0937753 0.0561178
-0.0385427 0.0493385
0.0346452 0.0501164
0.12432 0.0508436
0.0210212 0.0523827
0.0739226 0.0549016
-0.0131848 0.0555809
-0.0716782 0.0514962
0.0594395 0.0531453
-0.0267379 0.0528824
0.0365698 0.0495283
0.0594424 0.054705
0.0594116 0.0537167
0.0329331 0.0523934
0.124311 0.055805
0.0414075 0.0511835
0.0859465 0.0511635
0.0661225 0.0619158
0.0532578 0.0501249
0.0272916 0.0501294
0.0197009 0.0499269
0.0964307 0.0532747
0.0321468 0.0498809
0.0113515 0.053287
0.0217931 0.0479923
0.032158 0.0526304
0.088282 0.0533284
0.0859467 0.0502654
0.0317699 0.0570576
-0.0295676 0.0526076
0.0707062 0.0499219
-6.66928e-06 0.0522017
0.0871095 0.0544303
-0.0113495 0.0510634
0.0188603 0.0490283
0.0966596 0.0506062
0.0859465 0.0544298
0.0997462 0.0555296
-0.0318748 0.049157
0.0927396 0.0526849
-0.0111732 0.049184
0.076417 0.0518588
0.0710838 0.0537586
0.028139 0.0534575
0.0861488 0.0525011
0.0321468 0.0543632
0.0321466 0.0550231
0.0582236 0.0559278
0.0594398 0.0538991
0.0189868 0.0552316
0.0964307 0.0511597
0.0387924 0.0514656
0.106629 0.0505296
-0.00378521 0.0558556
-0.0385433 0.0484132
-0.0144713 0.0502162
0.109566 0.0517428
0.100097 0.051976
-0.000835377 0.0560218
0.0243586 0.0509324
0.057604 0.0521056
0.0626002 0.0521114
0.0330949 0.0515528
-0.00131656 0.052456
0.0631173 0.0530013
0.0594398 0.0497397
0.0966587 0.0527658
0.0197009 0.0518956
0.0927397 0.0526923
0.0861487 0.0529553
-0.0272904 0.0507183
0.0527578 0.0558097
-0.0111692 0.0504838
0.042448 0.0508272
0.121243 0.053619
0.0866799 0.0555469
0.021525 0.0559322
0.0210209 0.0515975
0.0496682 0.0495677
0.027823 0.0513833
-0.0716686 0.0498838
-0.0417792 0.0524468
0.0375381 0.0519159
0.027823 0.0489797
0.0667826 0.0544377
0.0387924 0.0531612
0.0594397 0.0594053
0.0321466 0.0510977
0.0271204 0.0498963
0.0594424 0.0537341
0.0321466 0.0497195
0.0730676 0.0504612
-0.00553123 0.0533003
-0.0394792 0.0489203
0.090741 0.0496114
0.0234704 0.0496334
0.0346353 0.0531547
0.0989675 0.0549036
0.0192074 0.0504819
0.0204119 0.0511679
0.0290868 0.0565617
0.0188603 0.0504926
0.0920961 0.0510192
-0.0385437 0.0493307
-0.00459356 0.0504625
0.0589764 0.0573006
0.0346353 0.052916
-0.0132904 0.0486446
0.0968255 0.0525701
-0.0462148 0.0505412
parameters: [ 8.496  1.538  5.523  1.422  4.918]. error: 360.749128402.
----------------------------
epoch 0, loss 1.24366
epoch 128, loss 1.30292
epoch 256, loss 1.45579
epoch 384, loss 1.16404
epoch 512, loss 1.53603
epoch 640, loss 1.35986
epoch 768, loss 1.42219
epoch 896, loss 1.30562
epoch 1024, loss 1.37848
epoch 1152, loss 1.06716
epoch 1280, loss 1.39754
epoch 1408, loss 1.2818
epoch 1536, loss 1.51669
epoch 1664, loss 1.35703
epoch 1792, loss 1.08846
epoch 1920, loss 1.01279
epoch 2048, loss 1.26776
epoch 2176, loss 1.22425
epoch 2304, loss 1.30914
epoch 2432, loss 0.982011
epoch 2560, loss 1.17061
epoch 2688, loss 1.34187
epoch 2816, loss 1.40751
epoch 2944, loss 1.35836
epoch 3072, loss 1.26647
epoch 3200, loss 1.04196
epoch 3328, loss 1.31824
epoch 3456, loss 1.2189
epoch 3584, loss 1.36358
epoch 3712, loss 1.31387
epoch 3840, loss 1.16297
epoch 3968, loss 1.18906
epoch 4096, loss 1.60322
epoch 4224, loss 1.39383
epoch 4352, loss 1.1762
epoch 4480, loss 1.32666
epoch 4608, loss 1.7382
epoch 4736, loss 1.09422
epoch 4864, loss 1.26724
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0109517 0.0568056
0.0800821 0.0562648
-0.0188023 0.0573179
0.0301504 0.0380525
-0.10011 0.0587567
0.0871098 0.042679
0.0807993 0.0488826
0.000835126 0.0482943
0.0950805 0.0450822
0.0496682 0.0541936
0.0871097 0.0324861
0.0803921 0.0320735
0.0594919 0.0586187
-0.053266 0.0528744
0.0436381 0.0423464
0.0981543 0.0557641
0.0271289 0.0561906
0.0405236 0.0644685
-0.0267425 0.0647377
-0.0716686 0.0589856
-0.0414059 0.059935
0.0335493 0.0592059
0.0807977 0.0601687
0.0867199 0.038718
0.0871095 0.0324894
0.0599612 0.0352681
-0.0414059 0.0505948
-0.0417491 0.0594241
0.0333261 0.0540481
0.0407513 0.0431945
0.0268195 0.0581913
0.0857789 0.0611209
0.0594467 0.0412854
0.12432 0.0572288
0.0222152 0.0351326
0.00131949 0.0600387
0.0853245 0.0566892
-0.0271126 0.0631577
0.123121 0.0502017
0.0594396 0.0592099
0.102035 0.0500776
0.0464005 0.0663434
0.0396777 0.0652544
0.0326469 0.0697215
8.39808e-07 0.0481534
0.0261391 0.041744
9.99101e-07 0.0577976
0.0210212 0.0447974
-1.92327e-05 0.0589342
0.0927397 0.0488874
0.0897923 0.0565248
2.41162e-06 0.0652874
-0.0621274 0.0595931
0.0594466 0.0485093
-0.0188023 0.0472745
0.0144707 0.0611668
0.0191272 0.0592298
-0.0250779 0.0432422
-0.000835063 0.0485327
0.0375381 0.0416888
0.0964307 0.0559763
-0.0207367 0.0473413
0.0871096 0.0498961
0.0131858 0.058688
0.125883 0.0714732
0.0321466 0.025421
0.0333262 0.0467617
0.0492474 0.0534212
0.0204028 0.0458695
0.0144859 0.0494031
0.0131849 0.0438233
-0.0417624 0.0576519
-0.0734426 0.0641691
0.0866799 0.0472862
0.0204028 0.047086
-0.0468001 0.0442835
4.07333e-06 0.0572808
0.0461385 0.0589729
0.09274 0.0455307
0.0188018 0.0444005
0.0322982 0.0404922
0.0907409 0.0441912
0.0631207 0.0658962
0.0594397 0.059958
0.0210915 0.0596839
0.0964307 0.0477695
0.0189828 0.0479726
-0.0529698 0.0569924
0.0749536 0.0542834
0.100116 0.0738715
0.0964772 0.0528045
-0.00236347 0.0499797
0.0857795 0.0477924
0.0532535 0.0451192
0.0247806 0.0655584
0.043921 0.0465769
0.101814 0.0576309
0.0144707 0.0600584
0.130005 0.0462235
0.0412752 0.0654236
0.105506 0.0513344
0.0865811 0.0390416
0.0321466 0.0606094
-0.00236347 0.0367513
0.0131855 0.0406408
0.0194981 0.0657789
-3.6833e-05 0.0439444
0.0211185 0.0486983
0.0710837 0.0457323
0.0117847 0.0577419
0.0211183 0.0415817
0.0385427 0.0649309
0.022444 0.0502008
-0.065961 0.051911
0.0464005 0.0573802
0.0710837 0.0457323
-0.0412757 0.0621589
0.023648 0.0594374
0.100116 0.0583174
0.0365698 0.0495346
-0.0207292 0.0580235
0.0318693 0.0451009
0.0496682 0.0431162
0.0487559 0.0564098
0.0317696 0.0356214
0.044975 0.0502876
0.0529686 0.0694352
0.102034 0.0426098
parameters: [ 8.496  1.538  5.539  1.422  4.918]. error: 8049173650.66.
----------------------------
epoch 0, loss 1.72163
epoch 128, loss 1.74215
epoch 256, loss 1.33288
epoch 384, loss 1.63567
epoch 512, loss 1.74095
epoch 640, loss 1.46566
epoch 768, loss 1.63881
epoch 896, loss 1.18978
epoch 1024, loss 1.16426
epoch 1152, loss 1.39578
epoch 1280, loss 1.50583
epoch 1408, loss 1.28328
epoch 1536, loss 1.51003
epoch 1664, loss 1.29298
epoch 1792, loss 1.27832
epoch 1920, loss 1.52481
epoch 2048, loss 1.44542
epoch 2176, loss 1.49915
epoch 2304, loss 1.36554
epoch 2432, loss 1.51019
epoch 2560, loss 1.46114
epoch 2688, loss 1.71658
epoch 2816, loss 1.37342
epoch 2944, loss 1.62419
epoch 3072, loss 1.28387
epoch 3200, loss 1.50951
epoch 3328, loss 1.23329
epoch 3456, loss 1.64556
epoch 3584, loss 1.15472
epoch 3712, loss 1.6222
epoch 3840, loss 1.46826
epoch 3968, loss 1.4043
epoch 4096, loss 1.59447
epoch 4224, loss 1.19732
epoch 4352, loss 1.61612
epoch 4480, loss 1.47654
epoch 4608, loss 1.62554
epoch 4736, loss 1.55875
epoch 4864, loss 1.26527
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0412702 0.0477324
0.0461385 0.0645164
0.0611271 0.0610194
0.0122295 0.0251538
0.0318695 0.00669687
0.129458 0.032146
-0.0449599 0.0234684
0.038792 0.0285599
0.0594397 0.0176903
0.0117847 -0.00473909
0.0326469 0.0524751
0.0390895 -0.0164303
0.0170834 0.0379367
0.000796249 -0.0112229
0.0177241 0.0138054
0.0458115 0.029134
-0.0111821 0.00793865
-0.0529659 0.0355182
0.0117845 0.0344749
0.0562597 0.00887798
0.0281392 0.0721249
0.0593665 -0.00156991
0.0109781 0.0105729
0.0131851 -0.00159483
0.0907408 0.0703594
0.0210958 0.047172
0.121243 0.0388973
0.0131855 -0.0132434
0.0950803 0.0390133
-0.014485 0.0736474
-0.0267352 -0.00204218
0.0237989 0.061317
-0.0529659 0.023365
0.126206 0.0268528
-0.0194973 0.016732
0.0855721 0.0421702
0.027745 0.0643568
0.0904958 0.0343873
0.0477959 0.0508643
0.0496682 0.00672121
0.0964307 0.0195951
-0.0111692 0.0218405
-0.0417573 0.0298059
0.0730674 0.0217815
-0.0056768 0.0190082
0.0248076 0.00401799
0.0113515 0.0105738
0.100668 0.0360302
0.076422 0.0632375
0.0267779 0.0264424
2.5668e-06 0.0139984
0.0365698 0.0329025
0.0321466 0.0192215
0.0800821 -0.00276335
0.130757 0.0312698
0.0859466 0.000297707
-0.0417792 0.0319023
0.0495763 0.0286158
0.088282 0.00866873
0.0593312 0.0378685
-0.0394796 0.0482148
0.0964772 0.0184442
0.0268195 0.0647155
0.0319945 0.0279646
0.0271187 0.0294643
0.00131924 0.0352962
0.100097 0.037912
-1.17339e-07 0.0389498
0.0857183 0.0174394
0.0867201 0.037476
0.0594396 0.0316464
0.0384676 0.0230982
-0.0529722 0.0242473
0.0396778 0.0106694
0.130756 0.0210932
0.0594398 0.0201903
0.0317696 -0.0046962
-0.000795203 0.02065
-0.0131855 0.00141772
-0.0132965 0.0306258
0.046209 0.0140287
0.0204028 0.0303129
-0.0621343 0.0612348
0.0375434 0.0176178
0.0290868 -0.00757014
-1.48505e-07 0.0326383
-0.00700272 0.0288444
0.0860364 0.0544975
0.0117782 0.0242287
-0.00459356 0.0190374
0.0217931 0.0688848
-0.0621346 0.0410259
0.0199309 0.0103254
0.0188018 0.0777073
0.05943 0.0581949
-0.00236347 0.00467545
0.0764272 0.0654098
0.0724734 0.0173704
-0.0716686 0.031824
0.0220539 0.0527731
0.0317696 0.0299019
0.0739226 0.0438561
0.038792 0.0457367
0.0373484 0.0182069
-0.00737805 0.0117735
0.0286789 0.0103862
0.028139 0.0706413
0.132178 0.0763221
0.0329331 0.0737799
0.0753057 -0.0057026
0.0477956 -0.00871663
0.106634 0.0281663
0.126169 0.0145589
2.96104e-08 0.0326529
0.0497674 0.0342944
0.0897923 0.0669595
0.0340225 0.0285897
0.0281392 0.0586853
0.0941706 0.0196546
0.0321632 -0.000808827
0.0394802 0.0707593
0.0981549 0.0400102
0.0997468 0.0613547
0.0594452 0.0416881
0.0267779 -0.00308585
0.0857794 -0.01109
0.0582389 0.0111487
0.124277 0.0234203
parameters: [ 8.496  1.538  5.507  1.422  4.918]. error: 1.27700122191e+13.
----------------------------
epoch 0, loss 1.42428
epoch 128, loss 1.49551
epoch 256, loss 1.24723
epoch 384, loss 1.42206
epoch 512, loss 1.42177
epoch 640, loss 1.22982
epoch 768, loss 1.41868
epoch 896, loss 1.5216
epoch 1024, loss 1.27903
epoch 1152, loss 1.64345
epoch 1280, loss 1.36962
epoch 1408, loss 1.24739
epoch 1536, loss 1.33821
epoch 1664, loss 1.739
epoch 1792, loss 1.66775
epoch 1920, loss 1.45443
epoch 2048, loss 1.48906
epoch 2176, loss 1.21324
epoch 2304, loss 1.25731
epoch 2432, loss 1.41629
epoch 2560, loss 1.23377
epoch 2688, loss 1.32607
epoch 2816, loss 1.19135
epoch 2944, loss 1.57887
epoch 3072, loss 1.31429
epoch 3200, loss 1.37701
epoch 3328, loss 1.33312
epoch 3456, loss 1.32937
epoch 3584, loss 1.22943
epoch 3712, loss 1.11806
epoch 3840, loss 1.51863
epoch 3968, loss 1.41569
epoch 4096, loss 1.13066
epoch 4224, loss 1.33701
epoch 4352, loss 1.26549
epoch 4480, loss 1.38823
epoch 4608, loss 1.29057
epoch 4736, loss 1.39294
epoch 4864, loss 1.38485
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-1.99995e-06 0.0419824
-0.000835377 0.0682359
-0.000795203 0.0692226
-0.0716782 0.0159795
0.0188596 0.0508029
-0.0234699 0.0516977
0.0436381 0.0542861
-0.0417425 0.0201442
0.100667 0.0650723
0.0594424 0.0529714
0.0261393 0.0337807
0.0989677 0.0495371
0.0188086 0.0633157
0.072741 0.0302236
0.0901953 0.045791
0.100097 0.0146356
0.0113515 0.0541611
0.000835126 0.050337
0.0599612 0.0660402
0.0594482 0.0210568
0.0594399 0.0629209
-0.0414033 0.0228398
-0.129453 0.0255845
0.0327313 0.0790714
0.0594397 0.0728703
0.0319943 0.0807761
0.0589133 0.0493362
0.00894922 0.0587496
-0.0267425 0.0398573
0.12432 0.0454797
0.0589765 0.069088
0.062595 0.0640314
0.01134 0.0698109
0.0910567 0.0751297
0.0966596 0.0443611
0.059435 0.0475173
0.0301505 0.0669546
-0.00891927 0.0319125
0.0224432 0.0665149
0.0594116 0.0473167
0.092056 0.012867
0.0592247 0.0291095
0.0594452 0.0201048
0.0201879 0.0270356
0.0594273 0.0537879
0.090741 0.0191901
0.0868851 0.0541968
0.0781223 0.0643479
0.0897923 0.0629107
0.126169 0.0557493
-0.0529659 0.0185306
0.0321467 0.0800619
0.0131849 0.0621582
-0.0234699 0.0260903
-0.0414033 0.0536041
0.110244 0.0476692
0.0109586 0.0317986
-0.00737805 0.0451369
-0.0234699 0.0528919
0.0710835 0.0362076
0.101814 0.0232732
-0.00553123 0.0569975
0.0764428 0.0602754
0.0424581 0.0272152
-0.0248182 0.0173261
-0.0385437 0.0132334
-0.0532643 0.0130782
0.019701 0.037143
-0.034636 0.0554124
-0.053266 0.0145384
0.0594398 0.0720365
0.0866747 0.0538785
-0.0023637 0.0619151
0.0414025 0.040506
0.0594398 0.0579847
0.0384676 0.0398436
-0.00236368 0.0513651
0.0691984 0.0335569
0.0477958 0.0334342
0.0272988 0.0283721
0.0882822 0.0609944
0.0370769 0.069637
-0.0272995 0.0690995
0.0937753 0.0253837
0.0868849 0.0500074
0.0234704 0.0295902
0.0330953 0.0238839
0.0594398 0.0300785
-0.0416401 0.0658487
0.0394803 0.0134868
-0.0532547 0.0462216
0.0496683 0.0356402
0.0707062 0.0414734
-0.0234699 0.0466315
0.109566 0.0486385
-0.0716682 0.0215666
-0.080798 0.0182427
0.022215 0.0647305
0.028139 0.0475447
0.0318693 0.0718282
-6.0681e-07 0.0424108
0.0604514 0.046001
0.076417 0.0499713
-0.0611177 0.0561421
0.0661224 0.0468804
-0.0113373 0.0623701
0.0496681 0.042455
0.0589133 0.0652461
-0.0300728 0.0205261
0.0594397 0.0328224
-0.0271278 0.0517836
0.0727412 0.0429278
0.00894922 0.0306099
0.0436381 0.0691842
0.0861486 0.0791809
0.0261392 0.0604076
0.0461383 0.0275993
0.0424428 0.0144642
0.0482639 0.0290362
0.0813268 0.0439136
0.101441 0.0194019
0.0207331 0.0399289
-0.000795203 0.063335
0.0511643 0.0179944
0.0417448 0.0343132
0.0199307 0.0531063
0.0122194 0.0589089
0.0538009 0.0583044
parameters: [ 8.496  1.538  5.523  1.422  4.918]. error: 283.426334622.
----------------------------
epoch 0, loss 1.36761
epoch 128, loss 1.49987
epoch 256, loss 1.45673
epoch 384, loss 1.59485
epoch 512, loss 1.43403
epoch 640, loss 1.36539
epoch 768, loss 1.45527
epoch 896, loss 1.29256
epoch 1024, loss 1.48172
epoch 1152, loss 1.32131
epoch 1280, loss 1.36715
epoch 1408, loss 1.56694
epoch 1536, loss 1.3171
epoch 1664, loss 1.34057
epoch 1792, loss 1.37272
epoch 1920, loss 1.53012
epoch 2048, loss 1.38334
epoch 2176, loss 1.55597
epoch 2304, loss 1.29126
epoch 2432, loss 1.26354
epoch 2560, loss 1.14852
epoch 2688, loss 1.47069
epoch 2816, loss 1.35579
epoch 2944, loss 1.4339
epoch 3072, loss 1.76578
epoch 3200, loss 1.37087
epoch 3328, loss 1.62749
epoch 3456, loss 1.5634
epoch 3584, loss 1.25119
epoch 3712, loss 1.51877
epoch 3840, loss 1.4363
epoch 3968, loss 1.33844
epoch 4096, loss 1.30667
epoch 4224, loss 1.27924
epoch 4352, loss 1.33717
epoch 4480, loss 1.44727
epoch 4608, loss 1.3655
epoch 4736, loss 1.45704
epoch 4864, loss 1.3864
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.053257 0.040834
0.0716719 0.0333815
0.0865811 0.0491829
0.0859465 0.088879
0.0857795 0.0426096
0.025123 0.0433984
0.0261394 0.0558208
0.0594324 0.0638791
-0.0113401 0.0589129
-0.0462169 0.0767079
-0.0621343 0.0347612
-3.40966e-05 0.0509952
0.049499 0.0327611
0.0188019 0.0272822
0.0318693 0.0531615
-0.0189887 0.0454759
3.82209e-05 0.0518518
0.0477958 0.0739122
0.0267781 0.0440314
0.0220537 0.0618349
0.038792 0.0308203
0.110244 0.0659997
0.0191272 0.0705315
0.0326469 0.0408858
0.0865811 0.049478
-0.0152556 0.0456831
0.110244 0.0832731
0.0813268 0.0268177
0.0859467 0.071126
0.100097 0.0309326
0.000213748 0.0414474
0.0941706 0.0433633
-0.0611171 0.0304255
0.0859465 0.0917528
0.0594374 0.0425249
0.0417629 0.0427919
0.0295843 0.0408193
0.0416368 0.0818473
0.130756 0.0284142
8.40829e-06 0.0586363
0.0867333 0.0618974
0.0594398 0.0618578
-0.00728242 0.0436047
-0.0131854 0.0518951
0.129458 0.0429469
0.0394803 0.0445646
0.0964305 0.0296003
-0.0662867 0.0536338
0.0189828 0.0610314
-0.036223 0.0421864
0.0188015 0.0385904
0.0868848 0.0999581
0.0271289 0.0529086
-0.0188028 0.0535656
0.0267419 0.0504295
0.0818743 0.0373416
0.0362234 0.0456207
0.0594395 0.0524474
0.0290868 0.0439322
-0.0272977 0.0815409
0.0730676 0.0512971
1.01848e-06 0.0476891
0.059435 0.056585
0.0417583 0.0759638
0.0594467 0.0394099
0.0727412 0.0721914
-0.0716682 0.0450604
0.01684 0.0829686
0.0851612 0.0352845
-3.11621e-07 0.0405567
0.0482638 0.0453451
0.0321466 0.083324
0.0497672 0.0427581
0.0739226 0.0437989
-0.0494977 0.0592086
0.0482639 0.0411436
0.125883 0.0178527
-0.0271111 0.0485243
6.7959e-07 0.0350266
-0.041635 0.0431008
0.0860428 0.0464775
-0.00699273 0.0408344
0.0204119 0.0482283
0.0594379 0.0499582
-0.0267379 0.0385709
0.0594482 0.0305782
-0.0131841 0.0421486
0.0319944 0.0750376
-0.0394796 0.0481198
0.0414051 0.0627082
0.081797 0.0534068
0.0417489 0.0351678
0.0188015 0.0317292
0.042448 0.0581601
0.101814 0.048516
-0.0416423 0.0782986
0.0477957 0.0673331
0.0989677 0.0455469
0.130005 0.0340518
-0.0857226 0.0468678
0.0631207 0.0534361
0.00894166 0.0357942
0.130756 0.0173846
0.0594397 0.0705015
-0.0188606 0.0624789
0.0122243 0.0672373
0.0734366 0.0490979
-0.0295823 0.0487547
0.0589763 0.0555394
0.0691072 0.0485172
-0.10011 0.0446558
3.62028e-05 0.0437303
-0.0529682 0.0367504
0.0322982 0.0468214
0.0710835 0.0870652
0.101441 0.0462136
-0.00142797 0.0707086
0.0927397 0.0798074
-0.0204122 0.0541365
0.0978866 0.0279452
0.0860428 0.0558524
0.109566 0.0753703
0.0220537 0.07837
0.106634 0.0608024
0.0594402 0.0307973
0.125883 0.0270282
-0.0144856 0.0429078
0.0950802 0.0307008
parameters: [ 8.496  1.538  5.523  2.422  4.918]. error: 6030861929.67.
----------------------------
epoch 0, loss 0.976506
epoch 128, loss 1.08041
epoch 256, loss 1.20957
epoch 384, loss 1.28069
epoch 512, loss 1.29658
epoch 640, loss 1.36436
epoch 768, loss 1.32191
epoch 896, loss 0.92159
epoch 1024, loss 0.749112
epoch 1152, loss 0.941512
epoch 1280, loss 0.711979
epoch 1408, loss 1.76503
epoch 1536, loss 1.00926
epoch 1664, loss 1.06778
epoch 1792, loss 0.86808
epoch 1920, loss 0.784086
epoch 2048, loss 1.23102
epoch 2176, loss 1.21018
epoch 2304, loss 0.991513
epoch 2432, loss 0.755956
epoch 2560, loss 1.01766
epoch 2688, loss 0.919528
epoch 2816, loss 0.621363
epoch 2944, loss 0.986954
epoch 3072, loss 1.11582
epoch 3200, loss 1.01931
epoch 3328, loss 0.747815
epoch 3456, loss 1.4433
epoch 3584, loss 1.32404
epoch 3712, loss 0.45165
epoch 3840, loss 1.09099
epoch 3968, loss 0.788288
epoch 4096, loss 0.498453
epoch 4224, loss 0.929979
epoch 4352, loss 1.59474
epoch 4480, loss 1.07654
epoch 4608, loss 1.6938
epoch 4736, loss 0.975286
epoch 4864, loss 0.901307
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0397476 0.0436994
0.0691985 0.0397494
0.01134 0.0268705
0.100097 0.0386574
0.0189868 0.0241745
0.0594324 0.0454886
0.0706156 0.0291121
-1.63011e-07 0.0269102
0.0594398 0.0311078
-0.0109489 0.017748
-7.99663e-06 0.0238967
0.0861488 0.0227115
-0.065961 0.02828
0.0730674 0.0318665
0.0188018 0.0389843
0.0989677 0.0354723
0.0807987 0.0238838
0.0329021 0.0360568
0.101441 0.0346143
-0.0860366 0.0194329
0.076438 0.0383544
-0.0532643 0.0188022
0.0781227 0.0271189
0.0362234 0.0303277
0.0261392 0.0384935
0.0370769 0.0149379
0.0562851 0.0291585
-0.0611177 0.0325949
0.0268195 0.0421445
-6.30661e-07 0.0332175
0.134985 0.0331623
0.109566 0.0280574
0.000802697 0.0377126
0.0319947 0.029441
-0.0161248 0.0316459
-0.0152583 0.0422816
0.104819 0.0400894
0.0532657 0.0229021
0.0362233 0.0213985
0.0594396 0.0433493
0.112932 0.0470315
0.029557 0.0193242
0.0188018 0.0424894
0.125146 0.0372323
0.0424268 0.0383111
0.0882822 0.0269625
0.0496683 0.0325671
0.0327313 0.0188189
-0.0716787 0.0223645
0.0853245 0.0307082
0.0271109 0.0376926
0.0867334 0.0310454
-0.0631103 0.0215024
0.121243 0.0273705
0.038544 0.02029
0.0375434 0.0433584
0.0220537 0.0324062
0.0800819 0.0416204
0.0594398 0.0325968
0.0706155 0.0291074
0.00856342 0.0347993
0.0882823 0.0325036
0.0267779 0.0309432
-0.063123 0.0296382
0.0594398 0.0310551
-0.0362233 0.031936
0.0776855 0.0277466
0.109566 0.0312984
-0.0267379 0.0456837
0.0496682 0.0323589
0.0234754 0.0275786
-0.0857226 0.0327068
0.046799 0.0241308
0.0271109 0.0375571
0.0594376 0.0455091
-0.014471 0.0520078
0.0594325 0.037701
-0.00628321 0.0438116
-0.0394792 0.028028
0.0387924 0.0447266
0.100116 0.0317343
0.0204143 0.0372942
0.0236481 0.0227239
0.0562649 0.0421606
0.0385291 0.0241135
0.025123 0.0479875
0.0234638 0.0226034
0.0582651 0.037609
0.0365698 0.0363005
-0.0807974 0.019834
0.130756 0.0380337
-0.0860431 0.0264388
0.0851609 0.0338252
-0.0267425 0.0371254
-0.0621346 0.0337097
-0.000792737 0.0293183
0.0643397 0.0462219
0.015257 0.0254972
0.109566 0.0205277
0.0174335 0.0326565
0.0764428 0.0421926
0.0497673 0.0343225
0.0337179 0.0443113
0.0910569 0.0192766
0.0871095 0.0332357
0.01684 0.0228648
-0.0117788 0.0231297
0.059435 0.0319555
0.0661223 0.0279076
0.0174335 0.0341213
0.0327314 0.0191103
-0.0462148 0.0276896
0.134995 0.0279506
0.0599612 0.0336897
0.0857788 0.0325096
0.0589764 0.0216487
0.132178 0.0434947
-0.0023636 0.0216067
0.0494968 0.0302826
-0.0188028 0.0275373
0.0691985 0.0402166
0.0991788 0.0379507
0.0734322 0.0307595
0.000834798 0.0405701
0.0964772 0.0287699
0.01684 0.0297265
6.39156e-06 0.0383298
0.0223972 0.0297891
parameters: [ 8.496  1.538  5.523  0.196  4.918]. error: 27849985.0335.
----------------------------
epoch 0, loss 1.24963
epoch 128, loss 1.20366
epoch 256, loss 1.17434
epoch 384, loss 1.1205
epoch 512, loss 1.16395
epoch 640, loss 1.14593
epoch 768, loss 1.17028
epoch 896, loss 1.41542
epoch 1024, loss 1.25637
epoch 1152, loss 0.876633
epoch 1280, loss 1.10157
epoch 1408, loss 1.07042
epoch 1536, loss 1.14722
epoch 1664, loss 1.14115
epoch 1792, loss 1.00807
epoch 1920, loss 1.13719
epoch 2048, loss 1.18818
epoch 2176, loss 0.967792
epoch 2304, loss 1.03385
epoch 2432, loss 1.20659
epoch 2560, loss 1.48421
epoch 2688, loss 1.08773
epoch 2816, loss 1.2947
epoch 2944, loss 1.16983
epoch 3072, loss 1.29409
epoch 3200, loss 1.16757
epoch 3328, loss 1.48401
epoch 3456, loss 1.18461
epoch 3584, loss 1.23556
epoch 3712, loss 1.23195
epoch 3840, loss 1.21203
epoch 3968, loss 1.49391
epoch 4096, loss 1.22303
epoch 4224, loss 1.10758
epoch 4352, loss 1.11437
epoch 4480, loss 1.25335
epoch 4608, loss 1.2196
epoch 4736, loss 1.30577
epoch 4864, loss 1.03159
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0207377 0.0547223
-0.0117783 0.0499019
0.0594398 0.049666
0.0272916 0.0368493
0.0791961 0.0471885
0.0937752 0.0681583
0.118684 0.0433259
-0.0662867 0.0420539
-0.0662833 0.0446045
0.0461384 0.0302569
0.0727409 0.0301845
-0.0532547 0.030081
-0.00236357 0.0484459
0.0594396 0.0428993
0.0594398 0.0542581
0.0907408 0.0218347
-0.0362226 0.0416485
0.0870053 0.0528032
2.5668e-06 0.0282454
0.0211183 0.0527868
0.0860523 0.0537868
-0.0189887 0.0374252
0.0134334 0.0421196
0.0594395 0.0572879
0.0261393 0.0254746
0.0897923 0.0257727
-0.0385437 0.0535185
0.0174339 0.0349744
0.0144846 0.0700358
0.0625951 0.0447162
0.0461385 0.0376479
0.0631173 0.0506829
-0.014471 0.0394585
0.129445 0.0393867
-7.68032e-07 0.0325405
-0.0468045 0.0511052
0.0538015 0.0461161
-0.0611171 0.0328716
0.0791268 0.0568538
7.08095e-06 0.0500558
-0.0144836 0.0541182
0.0201879 0.0408195
-0.0362233 0.0506414
-0.00894944 0.0681045
-0.065961 0.0575047
0.0594396 0.0662529
0.0134225 0.0360623
-0.0362226 0.0510036
0.0791961 0.0362428
-0.0734426 0.0457503
-0.0207292 0.0381153
0.129445 0.038409
-0.0734384 0.0321186
-0.0210947 0.0664214
-0.0318748 0.04246
-0.0417425 0.0465961
-0.0271153 0.0614662
0.0317696 0.0367662
-0.0210947 0.0410013
-0.00142797 0.049369
0.0113515 0.0403825
0.0462066 0.0505147
0.129453 0.0364415
0.0920969 0.0312573
0.0594397 0.0564226
0.0594396 0.0466193
0.0532557 0.0332948
0.0468064 0.0349385
0.0968257 0.0363193
-0.0807993 0.0499065
-0.0326437 0.0431571
0.0461382 0.0273708
-0.0109842 0.0317496
-0.0188023 0.0497139
0.0271115 0.0446574
0.0464005 0.0567339
0.0194912 0.0552799
0.0964766 0.0329536
0.0661224 0.0434001
0.0037846 0.0540157
4.17498e-06 0.0470627
0.0677309 0.0396518
0.0134334 0.049146
0.0594273 0.0508763
-0.129449 0.0486868
-0.00236347 0.0461931
0.0538297 0.0478874
0.0323166 0.0476488
0.0224432 0.0457244
0.0595095 0.0358861
0.0820225 0.0348661
-0.063123 0.0646439
0.0867333 0.0466537
0.00894922 0.0398368
0.0594398 0.037553
0.0477957 0.0491717
-0.0807993 0.0458827
0.0189828 0.049743
0.0625949 0.0382821
-0.00700272 0.0374142
-0.00731719 0.0490251
0.0247829 0.0266958
0.12945 0.0312857
0.0866747 0.0540428
0.0594397 0.0597551
0.0593758 0.0411877
0.0220537 0.0550633
-0.0468001 0.0462786
-2.86114e-05 0.0571853
0.0340225 0.0531521
0.0593133 0.0279146
0.0362234 0.0532881
0.0194981 0.0595117
0.0599612 0.0315589
0.0941706 0.0446759
0.0562851 0.042816
0.0593105 0.0582261
-0.00856301 0.0437726
0.093775 0.0438121
0.0317696 0.0428424
0.0594759 0.0302762
0.0868848 0.0424737
0.0329332 0.0309147
0.0337179 0.0254254
0.0496682 0.0486534
0.0981549 0.0393548
0.0855721 0.0313501
0.0691068 0.0535753
parameters: [ 8.496  1.538  5.523  1.422  4.918]. error: 134913353.586.
----------------------------
epoch 0, loss 1.37437
epoch 128, loss 0.950322
epoch 256, loss 1.40259
epoch 384, loss 1.04115
epoch 512, loss 1.53828
epoch 640, loss 1.03348
epoch 768, loss 1.32358
epoch 896, loss 1.59646
epoch 1024, loss 1.20679
epoch 1152, loss 1.33898
epoch 1280, loss 1.19516
epoch 1408, loss 1.29176
epoch 1536, loss 1.29255
epoch 1664, loss 1.03068
epoch 1792, loss 1.2492
epoch 1920, loss 1.61553
epoch 2048, loss 1.34761
epoch 2176, loss 1.14523
epoch 2304, loss 1.07372
epoch 2432, loss 1.20852
epoch 2560, loss 1.04959
epoch 2688, loss 0.978947
epoch 2816, loss 1.12857
epoch 2944, loss 1.50381
epoch 3072, loss 1.18098
epoch 3200, loss 1.26078
epoch 3328, loss 1.26627
epoch 3456, loss 1.19547
epoch 3584, loss 1.30649
epoch 3712, loss 0.880679
epoch 3840, loss 1.25388
epoch 3968, loss 1.18792
epoch 4096, loss 1.02546
epoch 4224, loss 1.01891
epoch 4352, loss 1.02259
epoch 4480, loss 1.20293
epoch 4608, loss 1.21807
epoch 4736, loss 1.38874
epoch 4864, loss 1.54679
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0177216 0.0274563
0.121243 0.0122679
-0.0412685 0.0239671
0.0194912 0.0083996
-4.71368e-07 0.0284608
0.0764481 0.0136449
0.0109586 0.0177378
0.0412776 0.0277562
0.038792 0.009078
4.57767e-08 0.00722633
2.05834e-07 0.0157104
-0.0111018 0.0113207
0.088282 0.0199428
0.0701141 0.0162078
0.0191276 0.0210635
0.0676335 0.0105122
-0.0857226 0.0221217
-1.99995e-06 0.0180533
0.0950805 0.0173832
0.0594395 0.0145074
0.022215 0.0200993
0.0424316 0.0157615
0.0594376 0.0203452
-0.000835392 0.0184205
0.0977801 0.00881369
-0.0111692 0.013042
0.0140549 0.0219939
0.000802697 0.0196342
0.088282 0.0189352
0.0416484 0.0176085
0.0945151 0.0247007
-0.053257 0.0299622
-0.0323135 0.0151257
0.0691985 0.00843951
0.0247806 0.0308359
-0.0390966 0.0219743
0.0237989 0.0119821
-0.0132904 0.00880959
-0.0462148 0.024751
0.0593133 0.0031705
0.100097 0.00310952
-0.0390898 0.0287493
0.0318766 0.00898505
-0.0194973 0.0270265
-0.0417624 0.0324964
0.0387924 0.0140415
0.0594398 0.0258996
0.028139 0.0261972
0.0424428 0.0213228
0.0508582 0.00545886
0.0321632 0.0116957
0.0626106 0.0044759
0.0691985 0.0151536
0.0220537 0.0115907
0.0207193 0.0117615
0.0449639 0.0161927
-0.0113473 0.0230395
0.0800819 0.00944134
0.0594397 0.0069711
0.0710838 0.0304807
-0.0234744 0.00940064
0.059367 0.016681
-0.0113401 0.029591
0.022215 0.00967519
-0.053257 0.0063106
0.0871096 0.0376758
0.00931417 0.0166665
0.0606162 0.0190849
0.081527 0.0185534
0.00080867 0.00773784
0.0868849 0.0252716
0.0464004 0.00332019
0.0707066 0.0195277
-0.0207367 0.0150712
0.0211183 0.015522
0.0594396 0.0241139
0.0477957 0.00708857
0.027823 0.0108316
0.0983621 0.0206453
2.41162e-06 0.0145569
0.0330542 0.0114855
0.0983621 0.0222453
0.0435639 0.0220613
-0.0271111 0.0216006
-0.0385424 0.0102027
0.0626002 0.0108205
0.0210209 0.0157514
0.112932 0.0145552
0.0589763 0.017412
0.042448 0.0039797
0.0271185 0.024412
-0.00378521 0.0273172
0.0920508 0.0181569
0.0867201 0.0185115
-0.0267379 0.0204366
-0.016115 0.000611813
0.0439209 0.0200197
0.0538297 -0.00415353
0.076438 0.0159356
0.0211182 0.00708748
0.092056 -0.00179316
0.0605534 0.00947126
0.0329332 0.0152946
0.100115 0.0226976
0.0562851 0.0336758
0.0599029 0.0254134
0.0594397 0.0174554
0.0174335 0.010117
0.0464004 0.00126057
0.0593346 0.0114905
0.0397476 0.0140129
0.0815268 0.00859011
0.0327314 0.0240771
0.0322982 0.019038
0.0177241 0.0279085
0.0860587 0.0152947
0.0199306 0.00666983
0.0626054 0.0189351
0.0870053 0.0118392
0.0611263 0.00786596
-0.0529682 0.0082318
0.0867334 0.0308374
-0.080798 0.00930213
0.046209 0.0377976
0.0659505 0.0237391
0.0859467 0.02614
0.0594324 0.00746511
0.0281391 0.02404
parameters: [ 8.496  1.538  5.523  0.804  4.918]. error: 99010858392.4.
----------------------------
epoch 0, loss 0.968458
epoch 128, loss 0.959271
epoch 256, loss 0.999519
epoch 384, loss 0.807023
epoch 512, loss 1.1776
epoch 640, loss 1.04844
epoch 768, loss 1.1549
epoch 896, loss 1.01715
epoch 1024, loss 1.07468
epoch 1152, loss 0.948816
epoch 1280, loss 0.917761
epoch 1408, loss 1.27902
epoch 1536, loss 0.999961
epoch 1664, loss 1.14593
epoch 1792, loss 0.980244
epoch 1920, loss 1.18166
epoch 2048, loss 1.08794
epoch 2176, loss 1.05469
epoch 2304, loss 1.02334
epoch 2432, loss 0.921583
epoch 2560, loss 1.14241
epoch 2688, loss 0.969879
epoch 2816, loss 1.13108
epoch 2944, loss 1.12733
epoch 3072, loss 0.996172
epoch 3200, loss 1.20878
epoch 3328, loss 1.20665
epoch 3456, loss 0.862153
epoch 3584, loss 1.16413
epoch 3712, loss 0.929821
epoch 3840, loss 0.978595
epoch 3968, loss 0.950633
epoch 4096, loss 0.984472
epoch 4224, loss 1.07265
epoch 4352, loss 1.11399
epoch 4480, loss 0.981754
epoch 4608, loss 0.923278
epoch 4736, loss 0.95597
epoch 4864, loss 1.0317
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0204001 0.0182812
0.0594427 0.0429178
0.0416434 0.0404549
0.0624794 0.0227878
0.0710838 0.0370113
-0.0467928 0.040097
-0.0611267 -0.00342616
0.0978866 0.0390827
0.0871096 0.0428147
-0.0109774 0.0415758
0.126206 0.0419528
0.0594397 0.0462429
0.0853255 0.0412288
0.0211183 0.0333106
0.0318766 0.0244246
0.0321897 0.042926
0.0594376 0.0297854
-0.0494953 0.0455168
0.0177241 0.0403252
0.0326469 0.0249888
0.0436381 0.0437259
-0.00553123 0.036872
0.0317696 0.0471404
0.0594398 0.0200396
0.101441 0.0361328
-0.00236343 0.0410602
0.0281391 0.0410309
-0.00553123 0.0407095
0.104819 0.0292847
0.0594397 0.0431432
0.0907408 0.0380593
0.033718 0.0417201
0.0589763 0.0229279
0.0417448 0.0135413
0.0417454 0.0290293
0.0272894 0.0381369
-0.0267425 0.0267414
-0.0532547 0.00807437
-0.0414014 0.0183327
0.0631163 0.0423149
0.0412702 0.0274087
-0.0390898 0.0432646
0.100097 0.0263737
0.0496682 0.043172
-0.0611267 0.03529
-1.48505e-07 0.0413746
0.0209719 0.0246815
0.0818743 0.0343827
0.0144853 0.0183878
0.0529686 0.0346886
0.0871095 0.0470071
0.0243586 0.039593
-0.0417624 0.0221107
0.081797 0.0362036
-0.0111821 0.0372053
0.0462066 0.0354633
0.0271115 0.0252726
0.0594397 0.0257353
0.126222 0.0377874
-0.0204028 0.015338
0.0661222 0.0504448
0.0592543 0.0346503
0.0594376 0.0297854
0.0482638 0.0388983
0.044975 0.0322086
-0.0267398 0.0246238
-0.0318771 0.0244665
0.0234681 0.044467
0.0851612 0.041702
0.0327315 0.0343684
-0.0295746 0.0282528
-0.0023637 0.0424358
0.0532535 0.031255
0.0387924 0.0356225
-0.0365701 0.020375
0.0321467 0.0421926
0.0319944 0.0422021
0.0211031 0.0348475
0.0271289 0.0317543
0.0964772 0.0438515
-0.0117788 0.0242491
0.0407513 0.0449191
-0.0271152 0.0326383
0.0562649 0.040119
0.0997468 0.0121375
0.126171 0.0234182
-0.000792737 0.00914037
-3.91322e-05 0.0288334
0.0122504 0.0344655
0.0818743 0.0222634
-0.0210896 0.0209028
0.0261393 0.0440258
0.0117782 0.0126143
0.0977801 0.0382934
0.110244 0.0348121
-0.0323135 0.0159456
-0.0860366 0.0340466
-0.0621272 0.00476568
-0.00142113 0.0305456
0.0859467 0.024889
-0.0144836 0.0380539
0.0859467 0.0127781
0.076422 0.0273941
0.0397474 0.0431655
0.0317697 0.0470392
0.0458116 0.0256356
-0.0131858 0.0329329
3.82209e-05 0.0393635
0.0396776 0.0442433
0.0201879 0.0434416
0.105471 0.0317161
0.046218 0.00985333
0.0188088 0.0156786
-0.00737805 0.0331389
0.126222 0.0348462
0.0642209 0.0188155
-0.00700262 0.0338578
0.0310444 0.0368188
0.106634 0.0206377
-0.0111018 0.0409601
-0.0662843 0.0391944
0.0385441 0.0330484
-0.080798 0.0406234
0.0791969 0.0264161
-0.0827278 0.0385028
0.0677309 0.0345278
-0.0631232 0.0283724
0.074953 0.0290432
parameters: [ 8.496  1.538  5.523  1.804  4.918]. error: 340407.835634.
----------------------------
epoch 0, loss 1.14
epoch 128, loss 1.35028
epoch 256, loss 1.2036
epoch 384, loss 1.47841
epoch 512, loss 1.17738
epoch 640, loss 1.19272
epoch 768, loss 1.14264
epoch 896, loss 1.18106
epoch 1024, loss 1.38656
epoch 1152, loss 1.28834
epoch 1280, loss 1.4335
epoch 1408, loss 1.24124
epoch 1536, loss 1.20874
epoch 1664, loss 1.26583
epoch 1792, loss 1.17269
epoch 1920, loss 1.21888
epoch 2048, loss 1.37126
epoch 2176, loss 1.35036
epoch 2304, loss 1.28853
epoch 2432, loss 1.19097
epoch 2560, loss 1.388
epoch 2688, loss 1.14683
epoch 2816, loss 1.15748
epoch 2944, loss 1.33744
epoch 3072, loss 1.21612
epoch 3200, loss 1.33802
epoch 3328, loss 1.24295
epoch 3456, loss 1.5917
epoch 3584, loss 1.26987
epoch 3712, loss 1.64416
epoch 3840, loss 1.17114
epoch 3968, loss 1.22001
epoch 4096, loss 1.42726
epoch 4224, loss 1.27364
epoch 4352, loss 1.0956
epoch 4480, loss 1.14766
epoch 4608, loss 1.27179
epoch 4736, loss 1.27525
epoch 4864, loss 1.08356
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0199306 0.0260006
0.0188015 0.0302381
0.0191272 0.0283258
0.0414025 0.0393532
0.0301505 0.0426387
0.124354 0.0359417
0.0424428 0.0332765
0.0964766 0.0423393
-0.0295746 0.0436101
-0.0323135 0.0463865
0.0977799 0.0202037
0.0710837 0.0319218
0.0868849 0.0553172
-0.0827278 0.0487508
0.0594397 0.0408029
0.0278231 0.0413781
0.0496681 0.0392557
0.0707072 0.047959
-0.0267352 0.0401125
-3.17974e-05 0.0559104
0.0224432 0.0331063
-0.0416468 0.0350396
0.0492474 0.043265
0.056285 0.0424742
0.0859865 0.0294571
-0.10012 0.0541284
0.0594325 0.0311826
0.0201879 0.0304073
0.0317698 0.039039
0.102035 0.0404843
-0.0860431 0.0409342
-0.0267379 0.0376385
0.0397474 0.0411443
0.0283783 0.0246176
0.109549 0.021273
0.0234681 0.0410131
0.125873 0.0259993
0.0327313 0.0296648
0.101441 0.032188
-0.041769 0.0419098
0.00131949 0.0383766
0.0911173 0.0268376
-0.0495737 0.0512985
-0.0117817 0.0477969
-0.00249422 0.0442408
0.0394799 0.0384137
0.0952317 0.0350921
0.0368964 0.0353441
0.0599031 0.0404355
1.74905e-07 0.0449346
-0.00378297 0.0511584
0.0594396 0.0504965
-0.0271193 0.0367227
-0.0113495 0.0432136
0.0106903 0.0491788
-0.0394796 0.0364896
0.106629 0.028149
0.0807983 0.0385677
0.0327314 0.0465357
0.0417702 0.0596153
0.092056 0.0263244
0.0868849 0.0410373
0.0662803 0.0382912
0.0910569 0.037636
0.0482638 0.0355052
0.0134334 0.0283161
4.57767e-08 0.0292094
0.0492512 0.0413021
0.01684 0.0454985
0.0887289 0.0384056
2.41162e-06 0.0509879
0.0496681 0.0392557
-0.0295676 0.0367515
0.0691985 0.0387168
0.0492512 0.0377105
-0.0417488 0.0549574
0.0416434 0.0432548
0.0866747 0.0410504
0.0594375 0.0411211
-0.0860447 0.0389104
-0.0323135 0.048082
-0.0417624 0.0329363
0.123121 0.0358676
0.0496683 0.0377585
-0.00236373 0.0613036
0.0407515 0.0410063
0.0859962 0.0292289
0.0783558 0.0397911
0.00932421 0.0297399
-6.66928e-06 0.0457086
-0.0111018 0.0213871
0.0599031 0.0400492
1.01848e-06 0.0369645
0.0170933 0.0186707
-0.0340329 0.0417338
0.097887 0.0192842
0.0857789 0.0347646
0.0327314 0.0348976
-0.0250779 0.0247801
0.0594399 0.0273599
0.106619 0.019302
0.0594398 0.0391855
0.121243 0.0536216
-0.0207367 0.0458065
0.0901949 0.0303603
0.059407 0.0377848
0.0251231 0.0245114
0.0477956 0.0505654
0.0268143 0.0284966
0.0594397 0.0503816
0.081527 0.0423564
-0.0234699 0.0525728
0.0724734 0.037256
-0.0271157 0.0499608
-0.00378297 0.0616013
-0.0417792 0.0332326
0.0910567 0.0432019
0.0271109 0.0607034
-0.0295823 0.0524966
-0.0495796 0.0356195
0.0870053 0.0317463
0.109566 0.0413293
0.0337179 0.0709265
3.62028e-05 0.0603074
0.0251233 0.0278312
0.0582389 0.0339095
0.0661225 0.0472211
0.0861486 0.0347568
parameters: [ 8.496  1.538  5.523  1.614  4.918]. error: 3.25233899887e+12.
----------------------------
epoch 0, loss 1.27038
epoch 128, loss 1.18266
epoch 256, loss 1.1267
epoch 384, loss 1.04394
epoch 512, loss 1.36006
epoch 640, loss 1.07293
epoch 768, loss 1.25605
epoch 896, loss 1.12347
epoch 1024, loss 1.05382
epoch 1152, loss 1.26285
epoch 1280, loss 1.22664
epoch 1408, loss 1.36432
epoch 1536, loss 1.1865
epoch 1664, loss 1.19023
epoch 1792, loss 1.17155
epoch 1920, loss 1.14161
epoch 2048, loss 1.05489
epoch 2176, loss 1.24024
epoch 2304, loss 1.19786
epoch 2432, loss 1.27537
epoch 2560, loss 1.03601
epoch 2688, loss 1.21133
epoch 2816, loss 1.07433
epoch 2944, loss 1.16138
epoch 3072, loss 0.907205
epoch 3200, loss 1.34311
epoch 3328, loss 1.26076
epoch 3456, loss 1.17908
epoch 3584, loss 1.01424
epoch 3712, loss 1.17768
epoch 3840, loss 1.02362
epoch 3968, loss 1.10017
epoch 4096, loss 1.34238
epoch 4224, loss 1.19855
epoch 4352, loss 1.10945
epoch 4480, loss 1.32861
epoch 4608, loss 1.2256
epoch 4736, loss 1.18504
epoch 4864, loss 1.18858
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0594374 0.0401093
0.0989675 0.00726945
0.081527 0.0321584
-3.11149e-07 0.0328852
0.0234681 -0.00268564
-0.00131656 0.0198689
-0.000805986 0.0345502
4.07609e-05 0.0378157
0.0594452 0.0149382
0.0865811 0.0487923
0.0373475 0.0226907
0.0283783 0.0377583
0.0271109 0.0199173
0.0857795 0.0157485
0.100667 0.0539034
-0.0385437 0.0330463
0.0594395 0.0179248
0.109566 0.0317263
-0.00236343 0.0420502
0.0305919 0.0406709
0.121243 0.0262079
-3.76332e-06 0.0176432
0.0122404 0.0511353
0.0857183 0.0117668
0.0978866 0.0111103
0.0707072 0.0241843
-0.0495734 0.017152
0.0188088 0.00970095
0.0385437 0.0302427
-0.00249422 0.0177466
-0.00737805 0.0213229
0.0122243 0.0304789
0.0594395 0.00592305
0.0168394 0.0449544
0.0860428 0.0476299
0.0594379 0.0366773
0.0237989 0.0179493
0.0271185 -0.0184767
0.0727409 0.0123449
0.0496682 0.0062522
0.0247038 0.0279449
0.109566 0.0131232
-0.0131855 0.0182076
0.0277564 0.0230758
0.0497672 0.0389368
0.0417583 0.023896
0.0317696 0.0121026
0.0527577 0.0424436
0.0594396 0.0286714
0.00596546 0.0156258
-4.6184e-07 0.035632
0.0390895 0.0199314
-0.0385421 0.0200618
-0.00893962 0.0028277
0.0582589 0.0170908
0.0964305 0.0240481
0.0511643 0.0111459
-0.0659495 0.016316
0.0370775 -0.0170232
0.0211182 0.0184314
0.0868851 0.0237407
0.00250001 0.0316945
0.0318695 0.033124
0.0865811 0.043745
0.0122243 0.0209184
0.0144853 0.0318925
0.100668 0.0409537
0.088282 0.0321074
0.121243 0.0471765
0.032902 0.0260933
0.0529729 -0.00176164
-0.0247813 0.0154945
-0.0300731 0.00062082
-0.0807986 0.0090129
8.40829e-06 -0.00576899
0.0920508 0.0300145
0.0776735 -0.0131811
-0.0117783 0.00993521
0.132184 0.0286812
0.00459996 0.00364029
0.0384676 0.0320097
-0.0716682 0.0306944
-0.0207292 0.0160216
0.0867333 0.0350077
0.0211182 0.0280627
0.0290868 0.0136753
0.0286789 0.0164985
0.028139 0.0333755
0.0211183 0.0126672
0.0414051 0.0187725
0.125873 0.0391359
0.0707072 0.0164664
0.092056 0.0309942
0.0986864 0.00589589
-0.027288 -0.00339718
-0.000793636 0.0310747
-0.0326437 0.0228129
0.0318693 0.0393169
0.112932 0.0208148
0.0188088 0.023879
-0.00700262 0.0266172
-0.0417624 0.022468
0.0251233 0.0172259
0.0272916 0.019556
-3.11149e-07 0.0254443
-0.013282 0.0524691
0.0594397 0.0334978
0.0866799 0.0451283
-0.041635 0.0185087
0.0362237 0.0210268
0.0781227 0.0340843
0.038792 0.0206238
0.0857794 0.0166284
0.0310444 0.0368893
-0.00236373 0.0188994
0.130756 -0.00362274
-0.034636 0.0257257
-0.0131858 0.030134
0.134995 0.0405003
-0.0621343 0.00321331
-0.0210896 0.00676143
-4.6184e-07 0.0330016
0.0594204 0.0466061
-0.0118764 0.0177292
-0.0295676 0.00825099
-0.00460347 0.0261307
0.0997468 0.0195167
-0.0295746 -0.00072215
parameters: [ 8.496  1.538  5.523  2.04   4.918]. error: 262269.216583.
----------------------------
epoch 0, loss 1.11275
epoch 128, loss 1.03038
epoch 256, loss 1.09458
epoch 384, loss 1.23115
epoch 512, loss 1.14212
epoch 640, loss 1.03886
epoch 768, loss 1.19377
epoch 896, loss 1.44419
epoch 1024, loss 1.24651
epoch 1152, loss 1.18539
epoch 1280, loss 1.0586
epoch 1408, loss 1.13891
epoch 1536, loss 1.15933
epoch 1664, loss 1.25895
epoch 1792, loss 1.21514
epoch 1920, loss 1.09698
epoch 2048, loss 1.22686
epoch 2176, loss 1.16999
epoch 2304, loss 1.38482
epoch 2432, loss 1.19403
epoch 2560, loss 0.894605
epoch 2688, loss 1.11981
epoch 2816, loss 1.29605
epoch 2944, loss 1.20613
epoch 3072, loss 1.27142
epoch 3200, loss 1.06325
epoch 3328, loss 1.21823
epoch 3456, loss 1.25173
epoch 3584, loss 1.12161
epoch 3712, loss 1.29666
epoch 3840, loss 1.10821
epoch 3968, loss 1.27666
epoch 4096, loss 1.18777
epoch 4224, loss 1.20332
epoch 4352, loss 1.19163
epoch 4480, loss 1.26915
epoch 4608, loss 1.23245
epoch 4736, loss 1.13842
epoch 4864, loss 1.20664
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0966587 0.0197153
-0.0860382 0.0136453
-0.0271152 0.01839
0.110244 0.0173718
0.0113515 0.0167453
0.0661222 0.0167226
0.0482639 0.0104513
0.0659505 0.0152766
0.0188596 0.0118108
0.0594399 0.00881042
0.0950802 0.0182807
0.0870049 0.0172466
0.0662931 0.0180304
-0.0417488 0.0117111
0.0477957 0.0109466
-7.68032e-07 0.0242339
-0.0417425 0.0131818
0.0207193 0.0146349
0.0496681 0.00965713
0.0261394 0.0155375
0.0592543 0.0195385
0.060558 0.0181904
0.0497674 0.021338
0.100097 0.0229531
0.0783558 0.0137509
-0.036223 0.0116438
0.0387924 0.0176077
0.043921 0.0165397
0.0144859 0.0164655
0.0327313 0.0168483
0.0706156 0.0106734
0.0462066 0.0178231
0.0436381 0.0112354
0.0261391 0.014308
0.0716773 0.0187733
0.124354 0.0286475
0.0937753 0.0181266
-0.0413988 0.0179975
0.00894003 0.0172468
0.10546 0.0158946
-0.0204028 0.0113959
0.0710837 0.0158844
0.0920613 0.0136551
0.0781227 0.0156597
0.0385431 0.0150496
0.0217933 0.012103
0.0140549 0.0141721
0.0532677 0.0122227
-0.0449599 0.00994098
0.00932522 0.0220858
0.0464004 0.0192568
0.028139 0.0159231
0.086885 0.0172239
0.0986864 0.0125675
0.0813268 0.0149848
-0.0131848 0.0178473
0.0594398 0.0161416
0.129993 0.014098
0.0492474 0.0155144
-0.0494953 0.0120546
0.0861488 0.0148978
0.0867119 0.0159357
0.0210981 0.013104
0.125146 0.0117918
0.0251233 0.0131712
0.0734439 0.014959
0.126171 0.0125822
-0.0385421 0.0179338
-3.40966e-05 0.0222887
0.0317696 0.0106819
-0.0188057 0.0177057
0.0661223 0.0181954
0.0589765 0.0188727
-0.0417491 0.0137492
-0.0131854 0.0173223
0.0860364 0.0209526
0.126206 0.0196519
0.121243 0.0104211
0.086885 0.0155069
0.0650733 0.0204161
-0.0716682 0.0112346
0.0384776 0.023804
-0.00700262 0.0161101
0.0396777 0.0183785
0.0594297 0.021417
0.0497671 0.0165812
0.135017 0.0169996
0.0333261 0.0164884
0.0234638 0.0183854
0.000213351 0.0177261
0.0220537 0.015
-0.0468001 0.0161386
0.0897923 0.0115424
0.0346447 0.0242426
-0.0390959 0.0154873
-0.0416423 0.013865
0.104819 0.00966632
-0.0857226 0.0164942
2.9023e-05 0.0123175
0.0857284 0.0127909
0.0865811 0.0178014
0.0278229 0.0163043
-0.0194973 0.015975
0.0327314 0.0184849
0.00931426 0.0124996
0.0385427 0.0201353
0.0752321 0.0150372
0.0204143 0.0254009
-0.0271278 0.0150226
-0.0117822 0.0202232
0.0152616 0.017071
1.74905e-07 0.0146577
-0.0131848 0.0134697
0.0295843 0.0114076
0.0412752 0.0217454
0.0716773 0.0190981
0.0152616 0.0151439
0.0594376 0.0159374
0.059367 0.0155207
0.0394799 0.0113464
0.0594374 0.0124064
0.0803921 0.0236565
0.0675205 0.0198661
0.090741 0.0149457
0.0367961 0.0246969
-0.0132965 0.0180049
0.0317699 0.0132919
0.0174335 0.020968
parameters: [ 8.496  1.538  5.523  1.922  4.918]. error: 20331108630.2.
----------------------------
epoch 0, loss 1.13507
epoch 128, loss 1.26156
epoch 256, loss 1.20993
epoch 384, loss 1.21708
epoch 512, loss 1.15045
epoch 640, loss 1.33001
epoch 768, loss 1.04356
epoch 896, loss 1.24468
epoch 1024, loss 1.13178
epoch 1152, loss 1.1774
epoch 1280, loss 0.959781
epoch 1408, loss 1.11141
epoch 1536, loss 1.36891
epoch 1664, loss 1.20445
epoch 1792, loss 0.933095
epoch 1920, loss 1.05259
epoch 2048, loss 1.31578
epoch 2176, loss 1.21616
epoch 2304, loss 1.21818
epoch 2432, loss 1.0793
epoch 2560, loss 1.0291
epoch 2688, loss 0.937548
epoch 2816, loss 1.26379
epoch 2944, loss 1.16872
epoch 3072, loss 1.12536
epoch 3200, loss 1.24923
epoch 3328, loss 1.05026
epoch 3456, loss 1.0189
epoch 3584, loss 1.17197
epoch 3712, loss 1.07856
epoch 3840, loss 1.10414
epoch 3968, loss 1.26057
epoch 4096, loss 1.18105
epoch 4224, loss 1.3442
epoch 4352, loss 1.03128
epoch 4480, loss 1.17371
epoch 4608, loss 1.08219
epoch 4736, loss 1.17206
epoch 4864, loss 1.15008
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0730674 0.0354108
-0.0189824 0.0368826
0.0532657 0.0175086
0.0385434 0.0152419
0.0322983 0.0123901
0.0385441 0.0367375
-0.0177216 0.0391095
0.0268195 0.0211832
0.0727412 0.0136284
0.0188018 0.0294091
0.0397474 0.0282519
0.0971047 0.0311359
-0.0362233 0.0398646
0.0911173 0.0124245
0.0337178 0.0218991
0.0604514 0.0211485
0.0207331 0.0383812
0.026809 0.0324432
0.0867335 0.0222571
0.0188597 0.028399
0.0860428 0.0340387
-0.00733491 0.0399215
-0.0631101 0.0183199
0.0966596 0.0184022
0.0346348 0.0417522
0.0417454 0.0410826
0.0971043 0.0312751
0.0964772 0.0218513
0.093775 0.0325137
-0.0144843 0.0326996
-0.0109774 0.0276912
0.0562649 0.0366283
0.0385437 0.020724
0.0952317 0.0352347
-0.0394796 0.0304569
-0.0131848 0.0416139
0.0449618 0.0258144
0.125863 0.0168499
0.0122295 0.0340158
0.0394802 0.0204823
0.0529686 0.0239336
0.0520969 0.0330894
0.105503 0.036065
-0.0204001 0.0256429
-0.0111821 0.0325966
0.0594325 0.0364722
-0.0413988 0.0313942
0.0330542 0.0336019
0.0594398 0.0255269
0.0367961 0.0377494
0.0710836 0.0148814
0.0177216 0.0269347
0.0496681 0.0192066
0.0727412 0.0287867
0.0462066 0.0398879
0.0237987 0.0319523
-0.0417573 0.0316151
0.0234638 0.0305395
0.100097 0.0285445
-0.00460347 0.0390474
0.043921 0.0394853
0.0305919 0.029692
0.0594324 0.0274298
0.0117785 0.0328205
0.0122504 0.0269339
0.0870049 0.032613
0.100667 0.0138438
0.0329021 0.0299555
-0.0189848 0.0401599
0.0776855 0.0104207
-0.0207292 0.0313606
0.0870049 0.0145165
-0.00733491 0.0264786
-0.0271193 0.0328452
0.0330542 0.032158
-0.0118768 0.00195853
0.0205116 0.0401194
0.0375381 0.0350862
0.014471 0.0288148
0.0861486 0.00502541
0.014484 0.0169483
0.0234754 0.0227366
0.0532578 0.0117545
0.0851609 0.00715955
0.0482639 0.0205528
0.0907409 0.0132625
0.079197 0.0384305
0.125146 0.0228065
0.00460075 0.0311568
0.0416368 0.0408949
0.0197009 0.0148972
0.038792 0.0226482
0.0247038 0.0417089
0.0222151 0.0405963
0.0882822 0.0303669
-0.0131848 0.0159592
4.57767e-08 0.0329039
0.0281392 0.0358461
0.0272916 0.0181981
-0.0295676 0.0396504
-0.00484737 0.03581
0.0997468 0.0420874
0.0594396 0.0348625
0.0734394 0.0375021
0.0424316 0.0219876
0.0305919 0.0336124
0.0977799 0.0285904
0.0188015 0.0277839
0.0267369 0.0408486
2.27454e-05 0.0242245
0.0464005 0.0191285
-0.000786289 0.039112
-0.053257 0.0135283
0.0117845 0.0312966
0.0487559 0.0271157
0.0346348 0.0393597
0.0594398 0.0348951
0.0997468 0.0334559
0.0417489 0.0323436
0.0920969 0.0200329
-1.48505e-07 0.0418897
0.0870049 0.0202163
0.0417454 0.0410826
-7.99663e-06 0.0298055
0.0981549 0.0271306
0.0594398 0.0087566
0.0859465 0.0166518
0.0321468 0.00813371
parameters: [ 8.496  1.538  5.523  2.046  4.918]. error: 4.38033220474e+12.
----------------------------
epoch 0, loss 1.34788
epoch 128, loss 1.38744
epoch 256, loss 1.5345
epoch 384, loss 1.08657
epoch 512, loss 1.14352
epoch 640, loss 1.51684
epoch 768, loss 1.19645
epoch 896, loss 1.27975
epoch 1024, loss 1.41073
epoch 1152, loss 1.25147
epoch 1280, loss 1.50547
epoch 1408, loss 1.296
epoch 1536, loss 1.31952
epoch 1664, loss 1.19822
epoch 1792, loss 1.09946
epoch 1920, loss 1.04691
epoch 2048, loss 1.31389
epoch 2176, loss 1.11596
epoch 2304, loss 1.11901
epoch 2432, loss 1.18817
epoch 2560, loss 1.27292
epoch 2688, loss 1.01817
epoch 2816, loss 1.34187
epoch 2944, loss 0.978516
epoch 3072, loss 1.26281
epoch 3200, loss 1.13161
epoch 3328, loss 1.11494
epoch 3456, loss 1.25607
epoch 3584, loss 1.21381
epoch 3712, loss 1.17552
epoch 3840, loss 1.38596
epoch 3968, loss 1.43558
epoch 4096, loss 1.34121
epoch 4224, loss 1.17395
epoch 4352, loss 1.16221
epoch 4480, loss 1.35711
epoch 4608, loss 1.26734
epoch 4736, loss 1.42855
epoch 4864, loss 1.30949
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0113473 0.0509519
0.0414025 0.0213751
0.0594273 0.052778
0.10955 0.0425711
4.57767e-08 0.0548258
0.0346353 0.0554825
0.0813321 0.0282609
-0.041769 0.036574
0.00142251 0.04542
0.046799 0.0435462
0.0983621 0.0476593
0.0189852 0.0497713
-0.0234677 0.023363
0.0152591 0.0512934
0.0937753 0.0232423
0.0538177 0.0599313
0.0594396 0.0371399
-0.0144836 0.0465582
0.0964307 0.0492723
0.000786701 0.0465959
0.043628 0.037544
0.0182077 0.0393048
0.0329021 0.0483514
0.0317699 0.0635716
0.112932 0.0324825
0.046209 0.0382159
0.0860523 0.0525521
0.0277564 0.0394763
-0.0707055 0.0503715
0.0384777 0.0682558
0.0234681 0.0360138
0.0727412 0.0323125
-0.0271278 0.0473837
-0.0467928 0.0459663
0.0964766 0.0331047
0.0910569 0.0426248
0.00250079 0.0383966
0.0800821 0.0380555
0.100097 0.0359947
0.0989678 0.0540481
0.0677309 0.0428611
0.0870053 0.0447147
0.00459996 0.0379643
-0.0776845 0.0319294
0.0261394 0.0196559
0.0234704 0.0359771
-0.0271193 0.0377367
0.032195 0.0491515
0.0396776 0.0511966
0.0286789 0.054533
-0.0132904 0.0478931
0.0412702 0.0424311
0.0599029 0.0568131
-0.0144856 0.0419331
0.05937 0.0525736
9.99101e-07 0.0301893
0.0968256 0.0428497
-0.0177194 0.0538038
-0.0529698 0.0524228
0.0273016 0.0478153
0.0594349 0.037509
-0.0118768 0.0381889
0.112932 0.0427712
0.0122452 0.0497885
0.000834798 0.0380522
0.0871095 0.0489583
0.046218 0.0348413
0.0321468 0.0617977
0.0234638 0.0375139
0.0952318 0.0334288
0.0981543 0.0404501
0.074953 0.0610536
0.0482639 0.0471065
0.130756 0.0380962
-0.0113401 0.0500825
0.10664 0.0429983
0.0803921 0.0479111
0.044975 0.0632494
0.0495763 0.0455044
0.0321468 0.0505997
0.0390986 0.049675
0.0897923 0.0534273
0.0981549 0.0383717
0.0301504 0.046087
0.0482639 0.048966
0.0997468 0.0573349
0.0642209 0.0438064
-0.0529722 0.0412265
-0.0495796 0.0173977
0.059435 0.0581505
0.0477956 0.0134313
-0.0385421 0.0472327
-3.40966e-05 0.0549705
0.0589765 0.0461008
0.029557 0.045746
-0.00249422 0.0484548
0.0882823 0.0375882
0.0594397 0.0335058
0.0594397 0.0544946
-3.66166e-06 0.0697609
0.0594396 0.0377258
0.0247038 0.0599312
0.0497673 0.0485982
0.0648671 0.0537563
-0.0300728 0.0332463
-0.0023637 0.0556368
0.0971047 0.0524742
0.0317699 0.0611411
0.0384776 0.0516123
0.0188609 0.0477639
0.09274 0.0338101
0.0329332 0.0234162
0.0329332 0.0306522
0.0867201 0.05246
0.0878297 0.047943
0.0495768 0.0357742
0.0237989 0.0275886
0.0860587 0.0484435
0.0594919 0.0711809
0.00931426 0.0521312
-0.0529722 0.0348478
0.0271289 0.0454313
0.0820225 0.053177
2.41162e-06 0.0478359
0.0594349 0.0400902
0.0373484 0.036254
0.0496683 0.0385709
0.105503 0.0388868
parameters: [ 8.496  1.538  5.523  1.995  4.918]. error: 1.56982157527e+13.
----------------------------
epoch 0, loss 1.31626
epoch 128, loss 1.05251
epoch 256, loss 0.972801
epoch 384, loss 1.05212
epoch 512, loss 1.17307
epoch 640, loss 1.21115
epoch 768, loss 1.15413
epoch 896, loss 1.29325
epoch 1024, loss 1.27201
epoch 1152, loss 1.20587
epoch 1280, loss 1.02272
epoch 1408, loss 1.00903
epoch 1536, loss 1.14317
epoch 1664, loss 1.29075
epoch 1792, loss 1.10799
epoch 1920, loss 1.17224
epoch 2048, loss 0.908743
epoch 2176, loss 1.20103
epoch 2304, loss 1.26157
epoch 2432, loss 1.30847
epoch 2560, loss 0.940544
epoch 2688, loss 1.20374
epoch 2816, loss 1.1832
epoch 2944, loss 0.970705
epoch 3072, loss 1.1072
epoch 3200, loss 1.12769
epoch 3328, loss 1.15
epoch 3456, loss 0.983005
epoch 3584, loss 1.07915
epoch 3712, loss 1.1739
epoch 3840, loss 1.08809
epoch 3968, loss 1.09107
epoch 4096, loss 1.05785
epoch 4224, loss 1.17539
epoch 4352, loss 1.23121
epoch 4480, loss 1.32137
epoch 4608, loss 1.09262
epoch 4736, loss 1.0458
epoch 4864, loss 1.04105
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0161572 0.0289723
-1.48505e-07 0.0622636
0.129453 0.0448119
0.0857284 0.0584136
0.126222 0.053732
0.0991789 0.0342238
-0.0716682 0.0384258
0.0261392 0.0366487
-0.065961 0.0380542
0.0188023 0.0431313
0.0234638 0.0477198
-0.0295746 0.0301633
0.0209719 0.0424321
-0.0346424 0.0496987
-0.0659495 0.0441566
0.022444 0.0416825
-0.000801651 0.0478255
0.0318695 0.0622157
0.0234681 0.0334985
0.0387924 0.0319892
4.17498e-06 0.0516729
0.0496681 0.0438193
0.0859467 0.041543
0.0594324 0.0598845
0.088729 0.0633472
-0.00894125 0.0446819
-0.0390959 0.0555123
0.0927396 0.0452891
0.0594375 0.0572032
-0.0295823 0.0292433
0.00143008 0.056321
0.0529729 0.0335613
-0.036223 0.0445382
0.0247829 0.0502271
0.13219 0.0403439
-0.0346352 0.0492274
0.028139 0.0508802
0.0594273 0.0497502
0.0286795 0.0481425
-0.10011 0.0308872
0.0594426 0.0484195
0.0594397 0.0599001
0.0764328 0.0327831
-0.0857218 0.056624
0.0511643 0.0356118
0.0941706 0.0587998
-0.0113473 0.0504813
0.0966596 0.0416409
0.0857789 0.0302471
0.0122352 0.0358117
0.0188023 0.0524708
0.0857794 0.0430251
0.0373475 0.0326605
0.0764428 0.0318998
0.0867333 0.054807
0.0599612 0.0539341
0.0122404 0.0552085
-0.000786289 0.0438506
0.059407 0.061225
0.0317697 0.0459461
0.0373475 0.0375358
0.130756 0.0474799
0.10546 0.0387927
0.0605996 0.0494023
-0.0271153 0.0489868
0.00131949 0.0546282
-0.0267425 0.0400004
0.0870053 0.0615752
0.0188088 0.0458007
0.0800821 0.0369494
-0.0492483 0.0317832
0.121243 0.0464504
0.0594396 0.0459345
-0.0295746 0.0449414
-0.0204122 0.0472431
-0.00628221 0.0419582
0.0964772 0.0370065
0.081527 0.0462734
0.0594374 0.0484354
0.044975 0.0417278
0.0911183 0.0512918
0.05943 0.0351791
0.0727412 0.0377567
0.0424581 0.0307947
0.0730675 0.0439345
0.0417484 0.0362601
0.022444 0.0420712
0.121243 0.0415267
0.0385437 0.0498035
0.0625951 0.0461115
-0.0385424 0.0341428
0.0791961 0.0613622
0.0594399 0.0376954
0.100667 0.0299247
0.0271117 0.0559509
0.0783558 0.0564209
0.0910569 0.0523012
0.0318695 0.0611503
0.129916 0.0557812
0.0964772 0.0462736
0.0659505 0.0598308
0.0335493 0.0486969
-0.000807624 0.0566748
-0.0394796 0.0383207
0.092056 0.0420553
0.0168394 0.047676
0.0113515 0.0447627
0.0191276 0.0427699
0.0251231 0.0391744
-0.0529659 0.0296609
-0.0177216 0.0588336
0.0133878 0.0522914
-3.66166e-06 0.0438475
-0.0271126 0.0541653
0.0335492 0.0314607
-0.0117817 0.0581288
8.40829e-06 0.0449987
-0.0131848 0.0469115
0.0661223 0.0506972
-0.0390959 0.0524468
0.046209 0.0468359
0.0497674 0.0448773
0.026809 0.0420583
0.0964307 0.0445758
0.0223968 0.0434273
0.0131849 0.0423054
0.0594397 0.0396068
0.0497674 0.0600132
parameters: [ 8.496  1.538  5.523  2.023  4.918]. error: 170419635.144.
----------------------------
epoch 0, loss 0.941128
epoch 128, loss 1.00251
epoch 256, loss 0.975967
epoch 384, loss 1.04882
epoch 512, loss 1.23632
epoch 640, loss 1.07304
epoch 768, loss 1.02916
epoch 896, loss 0.941834
epoch 1024, loss 0.983816
epoch 1152, loss 0.991976
epoch 1280, loss 1.1129
epoch 1408, loss 0.990745
epoch 1536, loss 0.961869
epoch 1664, loss 1.13282
epoch 1792, loss 1.14743
epoch 1920, loss 0.98873
epoch 2048, loss 0.987536
epoch 2176, loss 0.870567
epoch 2304, loss 0.99004
epoch 2432, loss 1.13854
epoch 2560, loss 1.08098
epoch 2688, loss 1.10607
epoch 2816, loss 1.15027
epoch 2944, loss 1.20476
epoch 3072, loss 0.988565
epoch 3200, loss 0.985104
epoch 3328, loss 1.06738
epoch 3456, loss 0.99788
epoch 3584, loss 1.07073
epoch 3712, loss 1.07846
epoch 3840, loss 1.03815
epoch 3968, loss 0.942495
epoch 4096, loss 1.18076
epoch 4224, loss 0.989239
epoch 4352, loss 0.99156
epoch 4480, loss 1.20979
epoch 4608, loss 0.875016
epoch 4736, loss 1.15389
epoch 4864, loss 0.966712
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0329332 0.0398342
0.0952317 0.043585
0.0326439 0.042098
-0.0113401 0.0417513
0.0188088 0.0274261
0.0776855 0.0320794
0.0592406 0.0393417
0.0594396 0.0267171
-0.0394792 0.04331
0.0594324 0.0290991
-0.0248069 0.0394099
1.01848e-06 0.0339846
-0.053266 0.0262992
0.059435 0.0365286
0.0236481 0.0540534
0.0861487 0.0404933
0.0861487 0.0428128
-0.00142797 0.0314609
-0.0662867 0.0343903
0.0113491 0.0331062
0.0861486 0.0371992
0.0482639 0.0537735
-0.0247813 0.0328394
-0.0295746 0.046918
0.0223968 0.0461637
-0.049248 0.036438
-0.0161248 0.0428047
1.95996e-07 0.0270553
0.0964772 0.0421667
0.0174335 0.0437346
0.0317697 0.0462389
0.0907408 0.0468554
-0.00733491 0.0314969
-0.0113495 0.0321001
-6.15975e-07 0.0403485
0.00863029 0.053005
0.0861486 0.037464
0.0248076 0.0422745
0.0247829 0.0459097
-0.0271157 0.0318382
0.0201879 0.0426283
0.0807993 0.0293594
0.0538297 0.0442304
0.0267779 0.0427049
0.106655 0.0396161
0.0385291 0.0365526
0.036794 0.0358074
-0.0611171 0.030267
-0.00700272 0.036892
0.0621308 0.0420607
0.0986864 0.0391664
-0.0109489 0.0360217
0.0920969 0.0421058
-0.0734361 0.0334949
0.0243594 0.0421264
0.0859466 0.040942
0.0375434 0.0419985
0.0594397 0.0394946
0.0920613 0.0387109
0.0492474 0.0367804
-0.0621272 0.0403375
0.0508582 0.0357559
0.0776735 0.0338337
-0.0413988 0.0348482
0.106624 0.0368744
0.121243 0.0314546
0.101441 0.0386636
-0.0621274 0.0417148
0.0417583 0.0419232
0.0589763 0.0337192
0.0989675 0.0384744
0.0605534 0.0278606
0.0815268 0.0420674
0.0927396 0.0489121
0.0781227 0.0468013
0.0301505 0.0309007
0.032902 0.0435329
0.01684 0.0419272
-0.0365701 0.0391573
0.0529666 0.028487
-0.0827278 0.0363777
0.0191272 0.0459352
0.0981543 0.04683
0.0188088 0.0346357
-0.0367964 0.0397914
0.0131845 0.0351963
0.0611197 0.0443932
0.0621302 0.0364571
-0.129445 0.0341976
0.0868849 0.0461469
0.112932 0.0257973
0.0305923 0.0437347
-0.0462076 0.0436855
0.0861488 0.0392333
0.0208245 0.045064
0.094504 0.03423
0.0857789 0.0403276
0.0749536 0.041106
0.0910569 0.0364795
0.0562649 0.0339768
-0.00131656 0.030236
-0.0414059 0.0464704
-0.0117783 0.0375721
0.143948 0.0442083
0.0152545 0.041714
-0.0272904 0.0461225
0.0950803 0.0538667
0.0283783 0.036044
0.10955 0.0340841
0.072741 0.0446265
0.0417489 0.0421214
0.0199307 0.0353023
0.109566 0.0449102
0.0626002 0.0339876
0.0424428 0.0391274
0.0950802 0.0538492
0.088729 0.0343427
0.102035 0.0371621
0.0122142 0.0469375
0.0191272 0.0434378
0.0781227 0.0441093
0.0113418 0.0363083
0.00894922 0.0353529
0.0170834 0.0465141
-0.0144856 0.0385833
0.0593105 0.0316941
0.0267419 0.0432383
-0.0207367 0.0332093
parameters: [ 8.496  1.538  5.523  2.034  4.918]. error: 70797076792.2.
----------------------------
epoch 0, loss 1.33804
epoch 128, loss 1.22469
epoch 256, loss 1.24193
epoch 384, loss 1.29403
epoch 512, loss 1.34545
epoch 640, loss 1.24788
epoch 768, loss 1.09084
epoch 896, loss 1.18231
epoch 1024, loss 1.22185
epoch 1152, loss 1.17098
epoch 1280, loss 1.22296
epoch 1408, loss 1.30363
epoch 1536, loss 1.28502
epoch 1664, loss 1.43406
epoch 1792, loss 1.09178
epoch 1920, loss 1.30309
epoch 2048, loss 1.27213
epoch 2176, loss 1.211
epoch 2304, loss 1.32423
epoch 2432, loss 1.2813
epoch 2560, loss 1.2327
epoch 2688, loss 1.17256
epoch 2816, loss 1.1901
epoch 2944, loss 1.14042
epoch 3072, loss 1.33057
epoch 3200, loss 1.29689
epoch 3328, loss 1.19517
epoch 3456, loss 1.24809
epoch 3584, loss 1.41444
epoch 3712, loss 1.22088
epoch 3840, loss 1.14324
epoch 3968, loss 0.997614
epoch 4096, loss 1.43153
epoch 4224, loss 1.28702
epoch 4352, loss 1.16119
epoch 4480, loss 1.51439
epoch 4608, loss 1.26924
epoch 4736, loss 1.28662
epoch 4864, loss 1.07001
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0650733 0.0209129
0.0855722 0.0294789
-0.0860382 0.0288484
0.0911173 0.00752197
-7.68032e-07 0.0222758
0.0267393 0.0205771
0.0396778 0.0286525
0.0594397 0.00460079
0.105506 0.00874997
0.0920613 0.0197094
0.0362234 0.0123465
-0.0529722 0.02817
-0.0215356 0.0214679
0.0310444 0.0202502
0.0594027 0.0132388
0.022444 0.0194842
0.0496682 0.00540896
-0.0362233 0.0136994
0.0204028 0.0130651
0.0199309 0.0213931
0.0168394 0.0236948
0.038792 0.0254562
0.0330953 0.0178202
0.0594325 0.0309538
0.00080867 0.0100642
-3.40966e-05 0.0275115
0.0878299 0.0205852
0.0800819 0.0243701
0.0305923 0.0196493
-0.0132904 0.0242773
0.032933 0.0171214
-0.0417491 0.0162518
0.100097 0.0261225
0.0290868 0.0228738
0.027823 0.0184727
-0.039089 0.0169726
-0.0210969 0.0226189
0.132184 0.0220714
0.036223 0.00694222
0.0950802 -0.00132789
0.0387924 0.0216728
0.0234754 0.0278636
0.0106905 0.0236766
0.0594397 0.0184388
0.0859465 0.0204349
0.0271204 0.0213998
0.0122194 0.0112741
0.0907409 0.0132596
-0.0208232 0.0062606
0.121243 -0.0068024
0.0188023 0.00431664
0.0237987 0.00650862
-0.0207342 0.0058595
0.0346452 0.0144352
0.0494968 0.0282102
0.0330953 0.0175044
0.0497672 0.0229834
0.0920961 0.00721762
-0.0267352 0.0131867
0.0594452 0.0243836
0.0776735 -0.000126074
-0.00728242 0.0164166
0.0968257 0.0212668
0.0920508 0.0240778
0.0495801 0.00909598
0.0594398 0.00722011
0.0277564 0.0148116
0.0707062 0.0272246
0.0286789 0.0183334
0.0986864 0.027301
3.40452e-06 0.0253935
0.0859466 0.0174604
2.05834e-07 0.0249934
-0.0118768 0.00828304
-0.0209708 0.0253696
0.0424529 0.0161818
-2.95455e-07 0.0253213
-4.6184e-07 0.0298908
0.0424215 0.0229237
0.0189868 0.0065947
0.000782366 0.0147016
0.0385427 0.0147974
0.0605534 0.0145736
0.062595 0.00719438
0.118684 0.028299
-0.000835392 0.0278965
0.0368964 0.0157465
0.00894922 0.0221513
0.0286795 0.0024795
0.0210981 0.0209194
0.0813269 0.0265949
0.0416418 0.015042
0.0950803 0.0174629
-0.00549668 0.0162487
-0.0662843 0.0233124
0.033718 0.0170848
-0.0131851 0.0300466
0.0706155 0.0211907
0.081527 0.0124337
0.129925 0.0294838
0.0945153 0.0146764
-0.0234677 0.0177023
0.0237989 0.0174594
0.022215 0.00056265
0.00863032 0.00319032
0.0210958 0.010503
0.0373484 0.0213782
0.0707072 0.0163351
0.0375381 0.0163739
0.121243 -0.000841613
0.0191276 0.0264221
0.104819 0.0244742
0.0920969 0.0210896
-0.00968441 0.0147668
0.0991788 0.0159916
0.0631173 0.0220712
0.01134 -0.00116796
-0.0468045 0.00871715
0.0677309 0.0303677
0.0911173 0.0185604
-3.66166e-06 0.0316385
0.0318766 0.0187384
-0.0416401 0.00472048
0.0870053 0.0226355
0.0330948 0.0285028
0.0387924 0.0216728
0.0208321 0.0278601
4.07609e-05 0.0222742
parameters: [ 8.496  1.538  5.523  2.04   4.918]. error: 51377661473.3.
----------------------------
epoch 0, loss 1.27405
epoch 128, loss 1.04735
epoch 256, loss 1.1226
epoch 384, loss 0.999614
epoch 512, loss 1.10259
epoch 640, loss 1.21771
epoch 768, loss 1.25785
epoch 896, loss 1.05594
epoch 1024, loss 0.960486
epoch 1152, loss 1.0775
epoch 1280, loss 1.06691
epoch 1408, loss 1.14026
epoch 1536, loss 1.15682
epoch 1664, loss 1.12679
epoch 1792, loss 0.975039
epoch 1920, loss 1.05189
epoch 2048, loss 1.17384
epoch 2176, loss 1.12232
epoch 2304, loss 1.09282
epoch 2432, loss 1.14024
epoch 2560, loss 1.12949
epoch 2688, loss 1.12684
epoch 2816, loss 1.03439
epoch 2944, loss 1.04525
epoch 3072, loss 0.989929
epoch 3200, loss 1.02219
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.121243 0.0388592
-0.016115 0.0395322
0.0204119 0.0498571
0.0368959 0.0350593
0.0509553 0.0380931
0.0286795 0.0436453
0.0781227 0.0502064
0.0594396 0.0550335
0.0211185 0.0503346
0.022444 0.0400442
-0.0144856 0.0305797
0.032902 0.0466749
0.0661224 0.0586343
0.0582589 0.046711
0.0273016 0.0465051
0.0191276 0.0449176
0.0375434 0.0377025
0.0783558 0.0403954
0.0611204 0.0305677
-0.0390966 0.0399697
0.0170834 0.0481512
0.0791268 0.0423929
0.0920969 0.0551891
-0.0247813 0.0455944
0.0321468 0.0428039
0.0237989 0.0418196
0.13219 0.0365149
0.0950805 0.0484137
0.0611271 0.0361652
0.00894003 0.0369141
0.026809 0.0410974
0.0251234 0.037702
0.0223968 0.0413757
0.105503 0.0499465
0.0691987 0.0519626
0.0194912 0.0380513
0.0449751 0.046077
0.133575 0.0461127
0.0676335 0.0364092
0.0952318 0.0449874
0.0659615 0.0352371
0.09274 0.0439686
0.0594398 0.0504274
0.0594379 0.0432503
0.0593105 0.0428439
0.0604514 0.0398126
0.0414025 0.0333369
0.0464004 0.0437026
-0.00733491 0.0470268
0.036794 0.036346
0.0189892 0.0341678
0.0594398 0.0489276
-0.0188605 0.0321143
0.0594397 0.0566077
0.0204028 0.0466244
-0.0023637 0.0481992
0.0968255 0.0506939
0.126127 0.0477103
0.0305919 0.0486745
0.0859865 0.042735
0.015257 0.0449497
0.0859467 0.0591644
0.00894922 0.0357751
0.0964305 0.0416959
0.0272916 0.0367382
0.046799 0.0375989
0.0968256 0.0539015
0.024704 0.0383305
0.0907408 0.0559719
0.0594399 0.0378798
0.0807977 0.0355505
-0.0707068 0.033483
0.0706156 0.0484552
0.0706156 0.0544208
0.0396777 0.0409656
0.0910569 0.0449621
-0.00236347 0.0428707
0.0384877 0.0464257
0.0397476 0.0418943
0.0871098 0.0420373
0.0261395 0.0556235
-0.0271193 0.0317772
0.0424529 0.0388981
0.0878299 0.0433987
0.0927396 0.0417892
0.0642209 0.0390223
0.0594396 0.0556443
0.038792 0.046423
-0.0468001 0.0348107
0.0223968 0.0435601
-0.0827278 0.0438779
0.0317699 0.0581641
-0.0271126 0.0394513
-0.0111821 0.0327227
-0.0177194 0.0416443
0.0122452 0.036342
-0.0734426 0.0350988
-0.0417488 0.036252
0.0210981 0.0397327
0.0188023 0.0303619
0.056285 0.0447115
0.0907408 0.0466258
-0.0271121 0.0442012
0.0857183 0.0324675
0.0436381 0.041611
-0.0716686 0.0347281
-0.00553123 0.0476549
0.0907409 0.0457883
-0.0113373 0.0462368
0.0611204 0.0414362
0.0152616 0.0512403
-0.0111732 0.0460559
-0.00131975 0.0288994
0.0594397 0.0506426
0.0538015 0.0514722
0.132184 0.0367549
0.0791961 0.0447377
0.0594325 0.0414786
0.0594397 0.0507262
0.00596546 0.0471516
0.0174339 0.0460434
0.0405236 0.0395941
-0.0117822 0.0297222
0.00142251 0.0504212
0.0295774 0.0390792
-0.0204101 0.0468093
0.0271289 0.0293043
0.0867119 0.0374129
parameters: [ 8.047  2.732  5.141  2.658  3.3  ]. error: 75.0477624871.
----------------------------
epoch 0, loss 1.56788
epoch 128, loss 1.41814
epoch 256, loss 1.43146
epoch 384, loss 1.56831
epoch 512, loss 1.32612
epoch 640, loss 1.46972
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0113373 0.0560258
0.0952317 0.0516031
0.0271204 0.0560775
0.000807032 0.0529486
0.027745 0.056637
0.0927398 0.0504174
0.0210212 0.0470737
0.000834782 0.0623053
2.96104e-08 0.0434577
0.0215366 0.0589073
0.0991788 0.0480673
-0.0207316 0.056045
0.0897923 0.0563057
0.00931421 0.0582452
0.0251231 0.0488664
-2.95455e-07 0.0520202
0.0211183 0.0577803
0.0497674 0.0662044
0.059435 0.0458151
0.00459996 0.0490839
0.0370775 0.0611786
0.0621302 0.0460515
0.0211182 0.0567256
-0.0776729 0.0548349
-0.0326437 0.0422517
-6.0681e-07 0.0513493
-0.0132904 0.0538273
0.132184 0.0476636
0.0321632 0.0630678
0.0435639 0.0589096
0.0950802 0.058699
-0.00700272 0.0457608
0.022444 0.0591703
-0.0462076 0.0639418
-0.0152533 0.0559742
0.0855722 0.0495526
0.0859465 0.0575172
0.0321897 0.0588942
-0.00549668 0.0634148
0.093775 0.0533007
0.0927396 0.0631168
0.0223968 0.0477942
0.0321467 0.0581317
0.101441 0.0602866
0.0625949 0.0605105
0.0710837 0.0479123
0.014471 0.0504629
-0.0152556 0.0618694
0.0813321 0.0498284
0.0335493 0.0523165
-0.00459356 0.0466082
0.0144846 0.0485485
0.0594396 0.0551437
0.0676335 0.0534004
0.0217933 0.0473733
0.0496683 0.0512908
0.0458117 0.0552572
0.0594398 0.0586447
0.0599029 0.0657919
0.0599029 0.0557324
-0.0272904 0.0460094
0.0594399 0.0569696
-0.0495737 0.0526546
0.0594397 0.0488017
0.0562649 0.0500091
0.0532677 0.0526592
0.0964307 0.0465407
0.0859466 0.059029
0.0199307 0.0566549
0.0897923 0.0503073
0.100097 0.0433659
-0.00856301 0.0456423
-0.0131858 0.0595514
0.0860428 0.063068
0.00863032 0.0502473
0.0248076 0.0523019
0.0435639 0.0568132
0.0927396 0.05529
0.0243594 0.0503425
0.109566 0.0604365
0.0319945 0.0644297
0.0606162 0.0542772
0.0271117 0.0576981
0.0468064 0.0548969
0.0631196 0.0539011
0.0920561 0.0519843
-0.0449599 0.0466076
0.0362234 0.060917
0.000213748 0.0561616
0.0461383 0.051255
0.0594395 0.0542254
-0.00459356 0.0553404
0.0317696 0.0662141
-0.0106876 0.0572491
0.0477957 0.054344
-0.0385437 0.0481603
0.0223968 0.0524527
-0.00142113 0.0638004
-0.000801651 0.0572122
0.0272894 0.0564242
0.0941706 0.0565062
0.0268143 0.049958
-0.0204101 0.0562528
0.0496681 0.0463085
-0.00131975 0.0611779
0.0237988 0.0500565
-0.0023636 0.0611511
0.0248076 0.0475408
0.0286789 0.0592935
0.0191276 0.0521868
0.0968255 0.0452513
0.0191272 0.0599807
-0.0416468 0.058808
-0.00142113 0.0673352
0.0251233 0.0475305
0.133575 0.0600288
-0.0144713 0.0506851
0.0396778 0.0579272
0.0673482 0.0518241
6.39156e-06 0.0584335
0.0373475 0.0525805
0.126171 0.0565048
-0.0631101 0.045324
0.0188086 0.0605436
0.097887 0.048842
0.121243 0.0573712
-0.0807993 0.0555994
0.0901952 0.0604289
parameters: [ 7.32   4.666  4.523  3.658  0.682]. error: 2.60947922116e+13.
----------------------------
epoch 0, loss 0.905377
epoch 128, loss 1.02667
epoch 256, loss 1.08305
epoch 384, loss 1.08261
epoch 512, loss 0.97506
epoch 640, loss 0.904792
epoch 768, loss 1.0456
epoch 896, loss 1.07554
epoch 1024, loss 1.13408
epoch 1152, loss 0.936638
epoch 1280, loss 1.04915
epoch 1408, loss 1.02976
epoch 1536, loss 1.15004
epoch 1664, loss 1.01004
epoch 1792, loss 1.15085
epoch 1920, loss 0.934393
epoch 2048, loss 1.04721
epoch 2176, loss 1.15179
epoch 2304, loss 1.28877
epoch 2432, loss 1.02415
epoch 2560, loss 1.05036
epoch 2688, loss 0.976464
epoch 2816, loss 1.08206
epoch 2944, loss 1.09451
epoch 3072, loss 1.04137
epoch 3200, loss 1.06434
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0224432 0.0463767
0.0594481 0.0459458
0.100097 0.0392703
-0.0208232 0.0421299
0.0599029 0.044971
0.0207193 0.050935
0.0477957 0.0419802
-0.0109489 0.0469776
0.0494968 0.0440159
0.00932421 0.0428491
-0.0860366 0.0432312
0.0945038 0.0560575
0.0267781 0.0382063
0.0192074 0.0536391
0.0907408 0.041119
0.0223972 0.0446569
0.0152591 0.036661
-0.032317 0.0459643
0.0416418 0.0395006
-7.68032e-07 0.0378966
0.00131949 0.0366351
0.0286789 0.0512108
0.0211183 0.0446873
-0.0117788 0.0401747
0.0417484 0.0461979
0.0417484 0.0443098
0.088729 0.0425071
0.0223968 0.0417237
0.0867067 0.0431734
0.0436381 0.0370294
0.0234638 0.0555041
0.0321468 0.0556891
0.0272894 0.0449758
0.0318766 0.0473872
-3.11149e-07 0.0381623
0.0482638 0.0499034
0.0977799 0.0363656
0.0305919 0.0518469
0.0495763 0.041525
0.0236479 0.0411801
-0.0271157 0.0376367
0.0865812 0.0457178
0.00863032 0.0429671
-0.0023636 0.0378661
0.0966596 0.0490595
0.0424529 0.0415197
0.0648671 0.0456349
0.0337179 0.0507236
0.0467944 0.04297
0.126206 0.0392787
-0.00236357 0.046513
-0.0189887 0.0390076
0.133575 0.0458695
0.0661225 0.0593872
0.0532557 0.0465039
-0.0204122 0.0408274
0.0122352 0.0414373
-0.0161572 0.0445979
-0.0385427 0.0433799
-0.0416468 0.0475256
0.0691068 0.0456842
-0.0416401 0.0476535
2.05834e-07 0.0384495
0.0851609 0.0471472
0.0321466 0.0454608
0.0131848 0.037606
4.07333e-06 0.048546
0.0367961 0.037443
0.0734366 0.0347716
-0.0716686 0.044801
0.0267781 0.0413712
0.0776855 0.0446567
0.0989675 0.0514678
0.0981549 0.0498996
0.0527577 0.0379062
0.0365698 0.0385957
-0.00484737 0.0392137
0.0594376 0.0454065
0.0661222 0.0516575
0.0643397 0.0463449
-0.00378521 0.0384217
0.0223972 0.0474316
-0.0189848 0.0395441
0.0661225 0.0420887
0.0594395 0.0613452
-0.0631103 0.0425222
0.0594397 0.0462428
0.0496681 0.052664
-0.0529698 0.0503534
0.0594397 0.0473838
-1.48505e-07 0.0387027
0.0800821 0.0530402
-0.0106908 0.0433255
-0.0385434 0.043171
0.0594376 0.04572
0.0611204 0.0406514
-4.6184e-07 0.0366059
-0.0394792 0.0553174
0.0594424 0.0369873
0.01684 0.0559265
0.0222151 0.0383574
0.0370769 0.0404421
-0.0468001 0.0388427
0.0210209 0.0476785
0.0093143 0.0403583
0.0813268 0.0507871
-0.0414014 0.0487186
-0.0188057 0.0366398
0.0268142 0.0461737
0.0562649 0.0444612
0.0927396 0.0471739
0.13219 0.0441323
0.0997462 0.0444257
0.0594204 0.0417792
0.032933 0.0410933
0.0281393 0.0412383
0.00378707 0.0417808
0.0384777 0.0474813
0.0396776 0.0468036
-0.0529722 0.0361336
-0.0394792 0.0423595
0.0594376 0.0457143
0.049499 0.0504091
0.0189852 0.0359626
0.0133878 0.0426452
0.0329332 0.042429
0.032933 0.0544792
-0.0827278 0.0413995
parameters: [ 8.047  2.732  5.141  2.658  3.3  ]. error: 150812099452.0.
----------------------------
epoch 0, loss 1.03889
epoch 128, loss 1.17066
epoch 256, loss 1.03947
epoch 384, loss 1.00537
epoch 512, loss 1.14523
epoch 640, loss 1.18681
epoch 768, loss 1.27612
epoch 896, loss 1.08061
epoch 1024, loss 1.03294
epoch 1152, loss 1.03013
epoch 1280, loss 1.14171
epoch 1408, loss 1.04443
epoch 1536, loss 1.04171
epoch 1664, loss 1.08608
epoch 1792, loss 1.02056
epoch 1920, loss 1.10167
epoch 2048, loss 1.20197
epoch 2176, loss 0.971557
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0458117 0.0354492
0.130756 0.0341844
0.0594162 0.0384149
0.0952316 0.0438109
0.0300757 0.0459433
0.0594399 0.0476087
0.0739226 0.0459269
0.0594427 0.0497787
-0.0776845 0.0425015
0.0286795 0.0367659
-0.000786289 0.0508264
0.0538177 0.0471703
0.0204028 0.0373204
0.0295687 0.0464709
0.0301505 0.0390995
0.0416418 0.0466426
0.0330948 0.0413775
0.0642209 0.0471684
-0.0132965 0.0413503
0.0122452 0.0401802
0.0412725 0.0475229
0.0424369 0.0465781
0.093775 0.0422175
0.0397476 0.0417529
0.0477957 0.0423773
0.100115 0.0472513
0.033718 0.0364903
-0.0210969 0.0412358
0.0199307 0.0385463
0.0764481 0.0396892
0.0611271 0.0461543
0.0643397 0.0462567
0.0991789 0.0440302
0.10664 0.0475949
-2.15513e-06 0.0404807
0.0593665 0.0455917
0.0594397 0.0405487
0.0594395 0.0491507
0.0860428 0.0373723
-0.00701261 0.0362039
0.0131849 0.0376447
0.000786701 0.0467876
-0.0295559 0.0385056
0.0182077 0.0392111
0.0117845 0.0456688
0.0710835 0.0444005
-0.0449599 0.047454
-0.065961 0.0393045
0.0277564 0.035797
0.0223972 0.0454942
0.0594452 0.0432848
0.104819 0.0430528
0.0727409 0.0480385
0.028139 0.0405405
-2.15513e-06 0.0383512
0.134985 0.0465243
-0.0118764 0.0366439
0.0910567 0.0461969
0.0991789 0.0442854
-0.0659495 0.0389866
0.0866747 0.04749
0.0589765 0.0427431
-0.00893962 0.0463798
0.0901949 0.0478777
0.0897923 0.0471938
0.0910567 0.0381412
0.0562597 0.0392311
0.0594424 0.0384149
0.0144859 0.0376529
0.085161 0.0478324
0.036223 0.0397534
0.0529729 0.0448724
0.0243594 0.0398506
-0.0271153 0.0346664
0.0290868 0.0486965
0.0730674 0.0477496
0.0261391 0.0448095
0.0220537 0.0413354
0.0691072 0.0409182
8.40829e-06 0.0501288
0.0541227 0.044517
0.0340225 0.0363202
0.0594398 0.0456354
0.135017 0.0493741
0.0321897 0.0469957
0.0920969 0.035295
0.0865811 0.0406875
0.0991788 0.0404665
0.0283785 0.0449892
0.0594398 0.0480713
-0.00968441 0.0480614
0.0482639 0.0458306
-0.0204028 0.0444336
-0.0152606 0.0384882
0.0191276 0.0413808
0.0594374 0.0432889
0.0989675 0.0398379
0.09274 0.045564
0.0321468 0.0330648
-0.0776845 0.0425015
0.0594116 0.0422649
0.0281391 0.0407528
0.110244 0.0454283
0.0594251 0.0401471
-0.0860431 0.0442421
-0.0494977 0.0487143
0.0927397 0.0448926
0.0594466 0.044708
-0.0109566 0.0430577
0.046209 0.0373077
0.0529729 0.0455147
0.0592543 0.0474557
0.0321467 0.0335608
0.0494968 0.0369285
0.0261391 0.0454404
-0.0449623 0.0464238
0.0589127 0.0385201
0.00863029 0.0519789
-0.0113373 0.0367948
0.0937752 0.0391114
0.0981543 0.0432935
0.0716719 0.0379689
0.0407513 0.04967
0.0871097 0.0464256
0.130757 0.0392435
0.0290868 0.037521
-0.0209824 0.0425608
0.0144707 0.0431269
parameters: [ 7.77   3.471  4.905  3.04   2.3  ]. error: 27444099.7366.
----------------------------
epoch 0, loss 1.26238
epoch 128, loss 1.23681
epoch 256, loss 1.12953
epoch 384, loss 1.19117
epoch 512, loss 1.24036
epoch 640, loss 1.13195
epoch 768, loss 1.31079
epoch 896, loss 1.25596
epoch 1024, loss 1.07591
epoch 1152, loss 1.08993
epoch 1280, loss 1.14264
epoch 1408, loss 1.09855
epoch 1536, loss 1.25141
epoch 1664, loss 1.15172
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.00549668 0.0169369
0.0859962 0.0190656
0.0261392 0.0168858
0.0209719 0.0147881
0.0907408 0.0146265
-0.0621274 0.0211637
0.0243586 0.0163905
0.0532578 0.0238026
0.0268143 0.0223537
0.0920508 0.0177719
-0.0827278 0.0169228
-0.0532643 0.0188387
0.0592543 0.018176
0.0527575 0.0185377
0.0904958 0.0138728
-0.0250779 0.0219915
0.0305923 0.0186418
0.0594325 0.0158197
0.0407515 0.017087
0.0211183 0.0147369
0.00080867 0.0164461
0.0188088 0.0190729
0.0321632 0.0146047
0.00931417 0.0184643
0.0675205 0.016184
0.0950803 0.023344
0.0271163 0.0140006
0.0182077 0.0163089
0.0910567 0.0168555
-0.049248 0.0192812
0.0594397 0.0196621
0.0594397 0.0239471
0.0319944 0.0183172
-0.0188057 0.0141454
-0.0807993 0.0154711
-0.053266 0.0212028
0.0868848 0.0158492
0.0648671 0.0134602
0.0327313 0.0132334
0.102034 0.0151928
0.0211031 0.0236157
0.0920961 0.0139525
0.0317699 0.0169834
0.0594919 0.015403
-0.00893962 0.0198482
0.0405236 0.0121545
0.0364958 0.0169022
0.0191272 0.0212936
-0.0111692 0.0135059
0.0322983 0.0137264
0.0134225 0.0142284
0.0594376 0.0155676
-0.0365701 0.0216985
0.0188609 0.0217805
0.0997462 0.0254668
0.036223 0.0161593
0.0859466 0.0228698
0.0327315 0.0155208
0.00250001 0.0171158
0.0037846 0.0152044
0.097887 0.0179257
0.0800821 0.0200919
0.0871098 0.0180539
0.0261391 0.0168352
-0.0707055 0.0139972
-0.000807624 0.0160846
0.0964305 0.0149911
0.0337182 0.0246807
0.0144859 0.0239659
0.0986864 0.025603
0.0364958 0.0185765
0.100668 0.0163168
0.0527575 0.0222377
0.0248169 0.0193951
0.0327313 0.0160361
0.0222151 0.01734
0.0317697 0.0185153
0.0117785 0.0154657
0.100668 0.0219503
0.0991789 0.0201087
-0.0295559 0.0259298
0.0997468 0.016728
-0.0161572 0.016845
0.0594398 0.016092
0.0329331 0.0146226
0.126127 0.0155077
0.0305919 0.017654
0.0589764 0.0142179
-1.99995e-06 0.0142935
0.0991789 0.020316
0.0468012 0.0164496
4.07333e-06 0.0177088
0.0861488 0.0132402
0.100097 0.0163653
-0.0271153 0.0185227
0.0621334 0.0183451
-0.00131656 0.0149004
0.0631173 0.0200439
0.0594376 0.0133378
0.0436381 0.0174613
0.110244 0.0168076
0.0461382 0.017237
1.01848e-06 0.0174088
0.0131845 0.0162921
0.0208321 0.0170403
0.0322983 0.0125667
-0.0109774 0.0162425
-0.0394796 0.0227257
0.0727409 0.0172386
-0.00236347 0.0158584
0.0907408 0.0232453
0.0983621 0.0152036
0.022444 0.017673
0.000802697 0.0164321
0.0251233 0.0234719
-0.0109774 0.0187961
0.0562648 0.0152214
0.0412725 0.0201825
0.102034 0.0131606
0.0532535 0.0154093
0.0594324 0.0133406
0.0414001 0.0144239
6.7959e-07 0.0193284
0.018802 0.0150086
0.0477957 0.0229445
-0.0131858 0.0141928
0.0706155 0.0235491
0.0882823 0.0173627
parameters: [ 7.598  3.927  4.759  3.276  1.682]. error: 1364190374.21.
----------------------------
epoch 0, loss 1.17995
epoch 128, loss 1.28017
epoch 256, loss 1.4091
epoch 384, loss 1.25405
epoch 512, loss 1.05184
epoch 640, loss 1.2844
epoch 768, loss 1.30095
epoch 896, loss 1.22199
epoch 1024, loss 1.18285
epoch 1152, loss 1.19514
epoch 1280, loss 1.37276
epoch 1408, loss 1.15766
epoch 1536, loss 1.1519
epoch 1664, loss 1.09365
epoch 1792, loss 1.23505
epoch 1920, loss 1.31533
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0870049 0.0554968
0.0477956 0.0581913
0.0385427 0.0566897
0.0871096 0.0543939
0.0251231 0.0505961
0.000835111 0.055631
-0.0161248 0.0567061
0.0487559 0.0581235
3.36628e-05 0.0596099
0.0326439 0.0515374
0.0691985 0.0569283
0.0272916 0.056299
0.0857284 0.0530598
-0.027288 0.0535027
0.00080867 0.0528725
0.014484 0.0506646
2.9023e-05 0.0554163
0.0329332 0.0520292
0.0209719 0.0479404
0.0734322 0.053575
0.0223972 0.0535099
-0.0413988 0.0606454
-0.0207342 0.0553139
-0.00460347 0.0515484
0.0326469 0.0526093
0.0867067 0.0520209
0.0283785 0.0565756
0.0594396 0.0558181
0.0370769 0.0571747
0.0807983 0.0566817
0.0496682 0.0516976
-0.0367964 0.0580016
0.000834798 0.0575376
0.109566 0.0567355
-0.0234699 0.0517931
0.080382 0.0552415
0.076438 0.0577667
0.13219 0.0510422
0.059435 0.0513796
0.0952317 0.0572724
0.0920508 0.0564504
0.130757 0.0570759
0.0997468 0.0538713
0.0907408 0.0504852
0.0871096 0.0499117
0.100097 0.0568556
0.101441 0.0511597
0.0117847 0.0589041
0.124311 0.0548665
0.0122243 0.055647
0.0407513 0.0575417
0.0861486 0.0536197
0.061289 0.0622578
0.0776855 0.058334
0.0210981 0.0539553
-0.00894125 0.0535332
0.0599029 0.0589363
0.0496682 0.0530229
0.0144707 0.0549306
0.0870049 0.0575005
0.129916 0.0572104
0.0662934 0.0575906
0.0991788 0.0517407
-0.0248182 0.0499972
0.0365698 0.0532807
0.0223968 0.0513416
0.0210212 0.0527235
0.0362237 0.0543128
0.0589133 0.0518171
0.000213748 0.0517472
0.0853255 0.0489697
0.0211183 0.0576906
-0.0734312 0.0525995
0.0511643 0.0536394
0.102034 0.0575226
-0.000781954 0.0588884
0.0317699 0.0531278
-0.0189887 0.0560578
0.0243594 0.0483765
9.99101e-07 0.0516363
0.0117845 0.0569305
-0.0734426 0.058074
0.0385441 0.0529535
0.0594396 0.0488103
-0.0385434 0.0511308
0.0605996 0.0565978
0.0407513 0.05685
-0.00856301 0.0548196
0.0497671 0.0530116
-0.0385434 0.0539824
0.0495768 0.053677
0.0941706 0.0541363
0.0937753 0.0545488
-0.0210969 0.0493898
0.0901953 0.0548622
0.106634 0.0573071
0.021525 0.056297
0.088282 0.0567939
-0.0109842 0.0513632
-1.1614e-09 0.0503159
0.0131848 0.0547482
0.0594375 0.0527869
0.0867335 0.0587138
0.0991788 0.0512519
0.0594395 0.0600444
0.0594397 0.0521904
-0.0529698 0.0544836
0.0594395 0.0506154
0.00863032 0.0578032
0.0300757 0.0522695
0.0461383 0.050443
0.0624794 0.0512134
0.0605996 0.0561687
-0.0417488 0.0552569
0.0594397 0.0525566
-0.0346416 0.0580241
-0.000795203 0.0540571
-0.0413988 0.0541654
0.0487559 0.0576407
0.0594324 0.0576407
-0.00378521 0.0537801
0.0538015 0.0523225
0.0707072 0.0528363
-0.000786289 0.0574401
0.10664 0.0560028
-0.0177216 0.0569602
0.0907408 0.0518868
-0.0106876 0.0560027
parameters: [ 7.687  3.691  4.835  3.154  2.002]. error: 6072456962.78.
----------------------------
epoch 0, loss 1.13783
epoch 128, loss 1.06117
epoch 256, loss 1.23035
epoch 384, loss 1.30611
epoch 512, loss 1.43159
epoch 640, loss 1.26071
epoch 768, loss 0.972704
epoch 896, loss 1.20565
epoch 1024, loss 1.37938
epoch 1152, loss 1.37769
epoch 1280, loss 1.09223
epoch 1408, loss 1.17542
epoch 1536, loss 1.24761
epoch 1664, loss 1.16777
epoch 1792, loss 1.21187
epoch 1920, loss 1.1323
epoch 2048, loss 1.16986
epoch 2176, loss 1.27672
epoch 2304, loss 1.34447
epoch 2432, loss 1.20352
epoch 2560, loss 1.37244
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.00596587 0.0210034
0.076417 0.0179121
0.0201879 0.0245207
-0.0385437 0.0204029
0.0482639 0.0209319
0.027823 0.0222544
0.125873 0.0241141
-0.0860431 0.0206299
-0.0621346 0.0185248
-0.00460347 0.0196345
0.0390895 0.0180819
-0.0367964 0.0140791
0.0541426 0.0178972
0.121243 0.0172031
0.072741 0.0255953
-0.00894944 0.0268228
0.00142251 0.0216319
0.0964307 0.0132477
0.0716773 0.016214
0.0527575 0.0232946
-0.0056768 0.0129125
0.100115 0.0200613
0.059407 0.0157519
0.0594398 0.0217873
0.0529666 0.0144815
0.123121 0.0205986
0.00378707 0.017746
0.0414001 0.0222958
-0.0215356 0.0216054
0.110244 0.0176082
0.0318695 0.0190578
-0.0144843 0.0150547
0.0594482 0.0185949
0.0706156 0.0212302
0.0131848 0.0185797
0.0927399 0.023956
0.0730674 0.0219429
0.0248169 0.0218838
0.105506 0.0164862
0.0317697 0.0215161
-0.0234677 0.0253408
0.0945038 0.0245885
-0.0529698 0.0217286
-0.063123 0.0151994
0.0532578 0.0153693
0.0710835 0.0131363
0.0691068 0.0151331
0.0248076 0.0192036
-0.000835392 0.0125362
0.0857183 0.0242554
0.0329332 0.023577
0.0217933 0.0167502
0.0594396 0.0178326
0.0964772 0.0267893
0.0724734 0.0201927
0.05937 0.0208062
0.0764272 0.0172058
0.0385431 0.0210551
0.0791268 0.0177483
0.0144853 0.01877
0.0964766 0.0186465
0.0968257 0.0204435
0.00856342 0.0230259
0.081527 0.015355
-0.0326435 0.0198224
0.0594426 0.0227004
0.110244 0.0174312
0.0417484 0.0220646
0.0267779 0.0215378
0.0375381 0.012333
-0.0467978 0.0199748
0.0605534 0.0157526
0.0937753 0.0238515
0.0177241 0.014854
0.0791961 0.0223835
0.12614 0.0149916
0.0321632 0.0174704
0.0373475 0.0150866
0.0384777 0.0146456
-0.0417624 0.0184628
0.0964305 0.0157214
-0.027288 0.0279926
0.0594473 0.0173777
0.0859467 0.0150485
-0.0215356 0.0168184
0.0319943 0.0178359
0.0327313 0.0205041
0.0855722 0.0175523
0.0234638 0.0244919
0.0318695 0.0198238
0.0496683 0.0249437
0.097887 0.0133912
0.0594397 0.0258395
-0.0495737 0.0244387
0.00931426 0.0190499
-0.0207342 0.0196046
-0.014471 0.0127287
0.0261394 0.0194819
0.0859467 0.0186609
0.0589127 0.0198876
0.059367 0.0125207
0.056285 0.0163252
0.0122194 0.015758
0.0424369 0.0170872
0.0691985 0.026918
-0.0467928 0.0186974
0.0868851 0.0165221
0.0624794 0.0161081
-0.0111732 0.021386
-0.0340213 0.0174352
0.022444 0.028078
0.0414051 0.0125898
-0.0271111 0.0221136
0.0611271 0.0163151
-1.48505e-07 0.0226901
0.090741 0.0270206
0.0865811 0.0198353
0.0527577 0.0232262
0.0273016 0.0213857
0.0251233 0.016278
-0.0385437 0.0193705
1.96444e-05 0.015026
0.0662931 0.0200393
0.0594251 0.0231296
-0.0188606 0.0230314
-0.0189887 0.0169043
2.96104e-08 0.0149766
0.0131849 0.0198038
parameters: [ 7.876  3.189  4.995  2.894  2.682]. error: 1.81936046938e+12.
----------------------------
epoch 0, loss 1.29084
epoch 128, loss 1.16108
epoch 256, loss 1.25153
epoch 384, loss 1.19067
epoch 512, loss 1.12172
epoch 640, loss 1.17112
epoch 768, loss 1.06404
epoch 896, loss 1.02274
epoch 1024, loss 1.11541
epoch 1152, loss 0.974429
epoch 1280, loss 1.22574
epoch 1408, loss 1.05858
epoch 1536, loss 1.12108
epoch 1664, loss 1.14284
epoch 1792, loss 1.08767
epoch 1920, loss 1.19069
epoch 2048, loss 0.995394
epoch 2176, loss 1.15191
epoch 2304, loss 0.984592
epoch 2432, loss 1.06242
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
0.0477956 0.0452461
-0.00700262 0.0457297
-0.0209708 0.0487184
0.0322982 0.0601913
0.0594424 0.0484318
0.0509553 0.0376366
0.0327313 0.0441033
0.00931417 0.055274
0.121243 0.0489116
0.0977799 0.0472249
-0.0346424 0.0476917
0.022444 0.0509022
0.000807032 0.0440598
0.0866747 0.0493561
0.0861487 0.0544949
0.000834798 0.0468065
0.056285 0.0474634
-0.000835392 0.0452589
-2.99285e-06 0.0403378
0.0109517 0.0410067
-0.00728242 0.050894
-0.0131841 0.0403843
0.125157 0.0368601
0.0396776 0.0387378
-0.0204122 0.0399045
0.110244 0.0513219
0.0815268 0.0449866
0.0878299 0.0407121
-0.000795203 0.0458553
-0.0117788 0.0459
0.0904958 0.0536264
0.132178 0.0492168
0.0251234 0.051994
0.0247829 0.0397017
0.086885 0.0539713
0.0385437 0.0412922
0.0461385 0.0504262
0.0188016 0.0509909
0.0582236 0.0474886
0.0595095 0.0457038
0.0871096 0.0511233
0.0964305 0.0416656
0.0867332 0.0471813
0.0661225 0.0612101
0.0207377 0.0394291
0.0752955 0.0404411
-0.0340329 0.0372123
0.0991788 0.042169
0.0878299 0.042515
0.097887 0.0533027
0.038792 0.0401392
0.0319947 0.0571641
0.0631196 0.0362793
-0.053266 0.0396392
-0.0109489 0.0390515
0.000213748 0.0526461
-3.91322e-05 0.0529725
0.100097 0.0495922
0.130756 0.0619341
0.0188603 0.0420196
0.0191276 0.0432941
0.0197009 0.0442433
0.022444 0.0472024
0.134985 0.0378389
0.00931417 0.0441802
0.0209719 0.0400316
0.0867199 0.0538937
0.0611263 0.0449181
0.0117782 0.047797
-0.0207342 0.0471324
0.023648 0.0428345
-0.041769 0.0399266
0.022215 0.0528565
-0.0272995 0.0446734
0.0397476 0.058121
0.129916 0.0516869
-0.00737805 0.0425599
0.0866799 0.0515902
-0.0271121 0.0397692
0.0861488 0.0559752
0.086885 0.0476325
0.0211031 0.0356719
0.024704 0.049821
0.0520969 0.0528072
0.032933 0.0510655
-0.0385433 0.042724
-0.065961 0.0430652
0.0300757 0.0401569
-0.0144856 0.0476527
0.0204119 0.0392542
0.0861486 0.0544643
0.0458117 0.0527279
0.0901952 0.0479383
0.0271185 0.0394829
-0.00735889 0.0512504
-0.0210947 0.0448139
0.00460075 0.0421294
0.080382 0.0536414
0.0271204 0.044503
0.0691987 0.0396443
0.022215 0.0485761
0.0237986 0.0470201
0.0764481 0.0374254
0.0268195 0.0404487
-0.0414033 0.0400252
0.000807032 0.049107
0.0267779 0.0429798
0.0851613 0.0430229
0.00596546 0.0389773
0.0520969 0.0523451
0.0247038 0.0522903
0.0707066 0.0436117
0.0407515 0.0546093
-0.0131841 0.046591
0.0248097 0.0406118
0.0753057 0.040055
0.129458 0.0464381
-0.00726943 0.0419781
0.0405236 0.0513846
0.0781227 0.047664
0.0327313 0.0516823
0.0870049 0.044564
0.0295687 0.0413358
0.0295843 0.0457647
0.0594473 0.0550568
0.0318693 0.043585
0.0477956 0.0399633
0.000786701 0.0409829
parameters: [ 7.81   3.363  4.94   2.984  2.446]. error: 9520.70661829.
----------------------------
epoch 0, loss 1.06816
epoch 128, loss 1.2315
epoch 256, loss 1.17918
epoch 384, loss 1.35221
epoch 512, loss 1.03861
epoch 640, loss 1.19908
epoch 768, loss 1.21027
epoch 896, loss 1.08882
epoch 1024, loss 1.03103
epoch 1152, loss 1.06953
epoch 1280, loss 1.15232
epoch 1408, loss 0.981025
epoch 1536, loss 1.08497
epoch 1664, loss 1.09436
epoch 1792, loss 1.30043
epoch 1920, loss 1.27655
epoch 2048, loss 1.11251
epoch 2176, loss 1.22758
epoch 2304, loss 1.07246
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
-0.0417422 0.0540256
-0.0109842 0.0448037
0.0604514 0.055197
0.0373484 0.0489303
0.00891968 0.0474123
0.0910569 0.0389886
0.0385441 0.0441888
0.0140549 0.0459082
0.0283783 0.03915
0.0188603 0.0503723
0.0727412 0.0535396
0.0706156 0.0484637
-0.0272904 0.0498361
-0.0300731 0.0443535
0.0920613 0.0447697
0.0819546 0.0478438
0.0414075 0.0478577
0.0734366 0.0508799
0.0853255 0.0454394
0.0691068 0.0463499
0.0991789 0.0452977
0.0224432 0.0406859
0.0318787 0.0473179
0.014484 0.0418772
0.134985 0.0443453
0.0527575 0.0530147
0.0482639 0.0487888
0.0207331 0.0492126
0.0210209 0.0402128
0.0791268 0.0491882
0.0604514 0.0480495
0.036794 0.0463371
0.109566 0.0446308
0.081797 0.042903
0.0691072 0.0508873
-6.15975e-07 0.0448404
0.0131845 0.0405689
0.0907408 0.0480396
0.0991788 0.0492796
0.0594396 0.0437748
0.0861486 0.0478636
0.0589765 0.0498773
0.0144707 0.0414653
0.0211183 0.052528
0.0424428 0.040395
0.0815268 0.0474235
0.0192074 0.0480641
-0.0414059 0.0451085
-0.0215238 0.0428848
-0.0611177 0.0442452
0.0170933 0.0445659
2.27454e-05 0.0479476
0.090741 0.0527633
0.0321467 0.0446982
0.033718 0.0477786
0.121243 0.04318
0.0594396 0.0446722
-0.0152556 0.0529261
0.0920508 0.0494144
0.0691072 0.0470112
0.0599612 0.0485914
0.0319943 0.0452929
-0.0860382 0.0521503
-0.0412685 0.0500872
-0.0144843 0.0395882
-0.0346416 0.0471583
3.82209e-05 0.0468558
0.0871095 0.0484163
0.0594397 0.0504493
0.0199307 0.0514214
-0.00460347 0.0468219
0.0764272 0.0456146
0.0611197 0.0454213
0.0691072 0.0471843
0.0593105 0.046215
-0.0346424 0.044647
0.0730674 0.0446728
0.0384877 0.0433287
0.0458117 0.0477856
-3.11149e-07 0.0453683
-0.00142797 0.0432855
-0.0631103 0.0471479
0.0412702 0.0493105
6.7959e-07 0.0432666
-0.0109566 0.0475043
0.0981543 0.0417448
0.0593105 0.0450375
0.0368964 0.0429257
0.121243 0.0474223
0.0594251 0.0533027
-0.0117788 0.044515
0.0416368 0.0488937
0.00932421 0.0406771
0.0321468 0.0477014
0.0594349 0.0468805
0.0907408 0.0492124
0.0820225 0.0436605
0.0330542 0.0529695
0.0122404 0.0400924
-0.00378297 0.049357
-0.0362233 0.0498837
0.0407513 0.0451321
0.081527 0.0527253
0.0867119 0.0466902
0.0390895 0.0537286
0.0385427 0.050624
0.104819 0.0454495
0.000834798 0.0432325
0.130757 0.0506851
0.014471 0.0542114
0.0468012 0.0519176
0.0592247 0.0500602
0.0417484 0.055101
0.0468064 0.0476108
0.0319945 0.0537858
0.0727411 0.0513967
0.0851613 0.0481357
0.0194981 0.050537
-0.0495796 0.0467078
0.101441 0.0527634
-0.016115 0.041087
-0.0412711 0.0469377
0.0870053 0.053593
-0.0117817 0.0538179
0.0593105 0.0422707
0.0417484 0.0481239
0.105503 0.0474286
0.0278231 0.046532
parameters: [ 7.8    3.39   4.931  2.998  2.409]. error: 10227364074.3.
----------------------------
epoch 0, loss 1.14616
epoch 128, loss 1.04515
epoch 256, loss 1.08088
epoch 384, loss 1.12881
epoch 512, loss 1.16864
epoch 640, loss 1.15394
epoch 768, loss 1.0108
epoch 896, loss 1.10886
epoch 1024, loss 1.04356
epoch 1152, loss 1.10394
epoch 1280, loss 0.982249
epoch 1408, loss 0.903706
epoch 1536, loss 1.1194
epoch 1664, loss 1.13173
epoch 1792, loss 1.04832
epoch 1920, loss 1.03461
epoch 2048, loss 1.06264
epoch 2176, loss 1.00264
epoch 2304, loss 0.979286
epoch 2432, loss 1.05654
MlPbSOpt.train: training finished. evaluation on last items: 
 actual | predicted
3.5013e-07 0.0278751
0.088729 0.0254981
0.0861486 0.0248692
0.0861486 0.0267255
0.126169 0.02371
0.0871097 0.0260983
0.0594481 0.0298138
0.0857284 0.0301344
0.00863032 0.0243961
0.00142251 0.0251564
0.0706155 0.0296299
0.0977799 0.0262296
0.0594273 0.0264261
0.076438 0.0294671
2.05834e-07 0.026195
0.134995 0.0344721
0.10546 0.028678
0.0927396 0.0300578
-0.0234677 0.0263691
0.0487559 0.0300405
-6.30661e-07 0.0300808
-0.00131656 0.0256204
0.102034 0.0226695
0.0871096 0.024035
-0.0215356 0.0239146
-0.000835063 0.0273569
0.032933 0.0292143
0.0477958 0.0304144
0.0730674 0.0248019
0.0594398 0.0288172
-0.0412711 0.0271979
0.000802697 0.0271721
0.0631196 0.0266626
0.038544 0.0314614
0.0871098 0.0264725
-0.0248092 0.0296446
-0.0707068 0.0232458
0.0596272 0.031338
0.0781223 0.0270626
0.0243594 0.0290336
0.0204028 0.0228379
0.0661224 0.0216544
0.0941706 0.0262609
0.0384877 0.0296746
0.0950805 0.0316847
0.112932 0.0311812
0.0405236 0.0260217
0.0329332 0.0299256
0.0996717 0.032681
0.0367961 0.0265991
0.0594397 0.0247207
0.0860364 0.027136
0.0267779 0.0212493
0.0807983 0.0289186
-0.036223 0.0233403
0.0541426 0.0316802
0.0867201 0.0256703
0.100097 0.0268712
0.0691068 0.0250013
0.000793148 0.0245677
0.0739227 0.0285222
-1.99995e-06 0.0287222
0.057604 0.0300642
-0.0529698 0.0307663
-0.0117817 0.0304987
-3.11621e-07 0.0309457
0.0860428 0.0249207
-0.0295559 0.0262649
-2.23337e-05 0.0303126
-0.0247793 0.0223428
0.0691985 0.0273563
0.0927399 0.0334728
0.0605534 0.025902
-0.0362233 0.0310825
0.0243594 0.0294706
0.0417583 0.0258624
-0.00699273 0.0301982
0.0144853 0.0302561
-0.0207292 0.0208348
0.081527 0.030383
0.0920561 0.0304829
0.0267779 0.0268718
2.27454e-05 0.0286284
0.0661223 0.020642
0.0594027 0.0234412
0.0977799 0.0301275
0.0144707 0.0319211
0.0611271 0.0253654
0.0109857 0.0207939
-0.0414033 0.0306565
0.0594396 0.0305119
0.0791961 0.0245613
0.0594397 0.026951
0.0589765 0.0248537
0.059367 0.0269729
0.0625949 0.0257753
-0.0734312 0.0261189
-0.027288 0.0324604
0.0321586 0.0262186
0.0857794 0.0272061
0.0710837 0.0318586
0.0177216 0.0275993
0.0375381 0.0299402
0.0764272 0.0314681
-0.0111692 0.0286452
0.0861488 0.027073
0.0424529 0.0310269
0.0710837 0.0262157
0.101441 0.0287273
0.0592543 0.0292369
0.0458115 0.020469
0.0661224 0.02576
-0.0318748 0.0278976
0.0594397 0.0280096
2.9023e-05 0.0324071
0.0271204 0.0309865
-0.0323135 0.0299798
-0.0131855 0.0284916
0.0093143 0.0261118
0.059367 0.0312808
0.0417448 0.0284801
0.0582236 0.0280593
0.0188016 0.0278959
0.126127 0.0326264
0.0594396 0.0293116
0.125146 0.0293982
-0.00735889 0.0236232
0.00894922 0.0298336
parameters: [ 7.817  3.345  4.945  2.975  2.47 ]. error: 57776858667.7.
   direc: array([[ 0.   ,  0.   ,  0.   ,  0.   ,  1.   ],
       [ 0.   ,  1.   ,  0.   ,  0.   ,  0.   ],
       [ 0.   ,  0.   ,  1.   ,  0.   ,  0.   ],
       [ 0.   ,  0.   ,  0.   ,  1.   ,  0.   ],
       [-0.449, -1.195, -0.382,  0.618, -1.618]])
     fun: 9520.7066182945728
 message: 'Optimization terminated successfully.'
    nfev: 219
     nit: 3
  status: 0
 success: True
       x: array([ 7.81 , -3.363,  4.94 ,  2.984,  2.446])
finished! :)

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import scipy.stats\n",
    "\n",
    "# Machine learning\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "import sklearn.preprocessing, sklearn.base, sklearn.utils, sklearn.model_selection, sklearn.gaussian_process\n",
    "\n",
    "import optunity\n",
    "\n",
    "# Various Python tricks and libraries\n",
    "import requests\n",
    "import time\n",
    "import functools\n",
    "import operator\n",
    "import collections\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import dill as pickle\n",
    "import IPython\n",
    "import gc\n",
    "import math\n",
    "\n",
    "# Parallel\n",
    "import joblib\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(ytrue, ypred): # sklearn.metrics.r2_score in tensorflow. 1 output only. \n",
    "\n",
    "    ytrue_mean = tf.reduce_mean(ytrue, name=\"ytrue_mean\")\n",
    "    r2_score = tf.subtract(1., tf.truediv(tf.reduce_mean((ytrue - ypred) ** 2), tf.reduce_mean((ytrue - ytrue_mean) ** 2)), name=\"r2_score\")\n",
    "    return r2_score\n",
    "\n",
    "class TqdmProgBar(keras.callbacks.Callback):\n",
    "    '''features:\n",
    "    1. tqdm ETA bar\n",
    "    2. logs[field] plotted for each field in fields\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_epochs, fields, interval=10): \n",
    "        self.n_epochs = n_epochs\n",
    "        self.fields = fields\n",
    "        self.interval = interval\n",
    "        \n",
    "        self.fields_history = dict((field, []) for field in fields)\n",
    "        self.fig, self.ax = plt.subplots(1, 1)\n",
    "        \n",
    "    def on_train_begin(self, logs):\n",
    "        self.pbar = tqdm_notebook(total=self.n_epochs, leave=False)\n",
    "        \n",
    "    def on_train_end(self, logs):\n",
    "        self.pbar.close()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % self.interval == 0:\n",
    "            self.pbar.update(self.interval)\n",
    "\n",
    "            for field in self.fields:\n",
    "                self.fields_history[field].append(logs[field])\n",
    "\n",
    "            self.ax.clear()\n",
    "            for field in self.fields:\n",
    "                self.ax.plot(self.fields_history[field], label=\"%s=%.2f\" %(field, self.fields_history[field][-1]))\n",
    "            self.ax.legend(loc='best')\n",
    "            self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_csline(l):\n",
    "    '''converts a list to a comma-separated line'''\n",
    "    return ',\\t'.join(map(str, l)) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minibatch(object):\n",
    "    '''\n",
    "    Makes batches from dataframes.\n",
    "    Executes n_epochs before raising StopIteration and dying.\n",
    "    Allows setting aside a test set.\n",
    "    Progress bar.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, df, minibatch_size, n_epochs, test_split, tqdm=None):\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "        N = len(df)\n",
    "        test_size = int(N * test_split)\n",
    "        test_index = np.random.choice(N, test_size, replace=False)\n",
    "        training_index = list(set(range(N)) - set(test_index))\n",
    "        self.test_df = df.iloc[test_index]\n",
    "        self.training_df = df.iloc[training_index]\n",
    "        self.df = df\n",
    "\n",
    "        self.i = 0\n",
    "        self.iMAX = n_epochs * len(self.training_df) / minibatch_size\n",
    "        \n",
    "        if tqdm:\n",
    "            self.tqdm = tqdm_notebook(total=self.iMAX, leave=False)\n",
    "        \n",
    "    def minibatch(self):\n",
    "        if self.i > self.iMAX:\n",
    "            self.i = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.i += 1\n",
    "            \n",
    "        if getattr(self, 'tqdm', None):\n",
    "            self.tqdm.update(1)\n",
    "        \n",
    "        index = np.random.choice(range(len(self.training_df)), self.minibatch_size, replace=False)\n",
    "        return self.training_df.iloc[index, :-1].values, self.training_df.iloc[index, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    def training_set(self):\n",
    "        return self.training_df.iloc[:, :-1].values, self.training_df.iloc[:, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    def test_set(self):\n",
    "        return self.test_df.iloc[:, :-1].values, self.test_df.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log():\n",
    "    jobs = pd.read_csv(filepath_or_buffer='ml_queue.log', sep='\\s+', header='infer', skiprows=[1], na_values=['UNLIMITED','Unknown','kijana','root']).dropna()\n",
    "\n",
    "    jobs.loc[:, 'Submit'] = pd.to_datetime(jobs.loc[:, 'Submit'].copy(), errors='coerce')\n",
    "    jobs.loc[:, 'Start'] = pd.to_datetime(jobs.loc[:, 'Start'].copy(), errors='coerce')\n",
    "    jobs.loc[:, 'End'] = pd.to_datetime(jobs.loc[:, 'End'].copy(), errors='coerce')\n",
    "\n",
    "    jobs.loc[:, 'NNodes'] = pd.to_numeric(jobs.loc[:, 'NNodes'].copy(), errors='coerce', downcast='integer')\n",
    "\n",
    "    jobs.loc[:, 'Timelimit'] = jobs.loc[:, 'Timelimit'].copy().str.replace('-','day ')\n",
    "    jobs.loc[:, 'Timelimit'] = pd.to_timedelta(jobs.loc[:, 'Timelimit'].copy(), errors='coerce')\n",
    "    \n",
    "    jobs.loc[:, 'Waited'] = (jobs.Start - jobs.Submit).values / pd.Timedelta('1h')\n",
    "\n",
    "#     jobs = jobs[(np.abs(scipy.stats.zscore(jobs.loc[:,'Wait'].values / pd.Timedelta('1h'))) <3)]\n",
    "    jobs = jobs.dropna()\n",
    "    \n",
    "    jobs = jobs.sort_values(by = 'Submit')\n",
    "    \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = read_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(tuple_):\n",
    "    \n",
    "    jobs, index, row = tuple_\n",
    "    \n",
    "    tj = thisjob = row\n",
    "    now = tj.Submit\n",
    "    \n",
    "    rj = relatedjobs = jobs.loc[np.logical_and.reduce([jobs.index != index, jobs.Submit <= now, jobs.End > now])] # excludes thisjob, includes jobs submitted simultaneously but ranked earlier\n",
    "    \n",
    "    # 无关人等滚开\n",
    "    if now < jobs.End.min() or index < 100 or index == 10826:\n",
    "        return None\n",
    "        \n",
    "    # 零、有空位，就不用等。\n",
    "    if rj.NNodes.sum() + tj.NNodes <= 29 and np.sum(rj.User == tj.User)<10 and rj.loc[rj.User == tj.User].NNodes.sum() + tj.NNodes<16:\n",
    "        assert tj.Waited <= 0.3\n",
    "        \n",
    "    # 一、各种descriptor\n",
    "    analysis = pd.DataFrame([], index=[index])\n",
    "    \n",
    "    analysis.loc[:, 'NNodes'] = tj.NNodes\n",
    "    analysis.loc[:, 'ETA']    = tj.Timelimit\n",
    "    \n",
    "    analysis.loc[:, 'Total_squeue_nnodes'] = rj.NNodes.sum()\n",
    "    \n",
    "    analysis.loc[:, 'Total_squeue_eta']    = rj.Timelimit.sum()\n",
    "    \n",
    "    analysis.loc[:, 'Self_job_in_squeue_percentage'] = 0 if len(rj)==0 else float(np.sum(rj.User == tj.User)) / len(rj)\n",
    "    \n",
    "    analysis.loc[:, 'Waited'] = tj.Waited\n",
    "    \n",
    "    return analysis\n",
    "    \n",
    "    \n",
    "L = multiprocessing.Pool(processes=20).map(sample, [(jobs, index, row.copy()) for index, row in jobs.iterrows()])\n",
    "analyses = pd.concat(L) # None is automatically ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_analyses = pd.DataFrame(sklearn.preprocessing.scale(analyses.apply(pd.to_numeric).values), index=analyses.index, columns=analyses.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_regress(activation, dropout, momentum, log2_minibatch_size, n_epochs, minuslog10_learning_rate, optimizer, n_layers,\n",
    "               units1, units2=None, units3=None, units4=None, units5=None, units6=None,\n",
    "               normalize_analyses=normalize_analyses):\n",
    "    \n",
    "    minibatch_size = int(2. ** log2_minibatch_size)\n",
    "    n_epochs = int(n_epochs)\n",
    "    learning_rate = 10. ** -minuslog10_learning_rate\n",
    "    n_layers = int(n_layers)\n",
    "    \n",
    "    # graph\n",
    "    tf.reset_default_graph()\n",
    "    training = tf.placeholder(name=\"training\", dtype=tf.bool)\n",
    "    h = X = tf.placeholder(name=\"X\", dtype=tf.float32, shape=[None, 5])\n",
    "    y = tf.placeholder(name=\"y\", dtype=tf.float32, shape=[None, 1])\n",
    "    \n",
    "    for units in [units1, units2, units3, units4, units5, units6]:\n",
    "        if units:\n",
    "            units = int(units)\n",
    "            h = tf.layers.dense(h, units)\n",
    "            h = tf.layers.batch_normalization(h, momentum=momentum, training=training)\n",
    "            h = getattr(tf.nn, activation)(h)\n",
    "            h = tf.layers.dropout(h, dropout, training=training)\n",
    "            \n",
    "    yhat = tf.layers.dense(h, units=1, name=\"yhat\")\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(yhat - y), keepdims=False)\n",
    "    training_op = getattr(tf.train, optimizer)(learning_rate).minimize(loss)\n",
    "    \n",
    "    # sess\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    m = Minibatch(normalize_analyses, minibatch_size, n_epochs, test_split=0.2)\n",
    "    while True:\n",
    "        try:\n",
    "            _X, _y = m.minibatch()\n",
    "            sess.run(training_op, feed_dict={X: _X, y: _y, training: True})\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "    # evaluate on training set\n",
    "    _X, _y = m.training_set()\n",
    "    _yhat = sess.run(yhat, feed_dict={X: _X, y: _y, training: False})\n",
    "    training_r2 = -10 if np.isnan(_yhat).any() else sklearn.metrics.r2_score(_y.reshape(-1), _yhat.reshape(-1))\n",
    "    \n",
    "    # evaluate on test set\n",
    "    _X, _y = m.test_set()\n",
    "    _yhat = sess.run(yhat, feed_dict={X: _X, y: _y, training: False})\n",
    "    test_r2 = -10 if np.isnan(_yhat).any() else sklearn.metrics.r2_score(_y.reshape(-1), _yhat.reshape(-1))\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    # dump output\n",
    "    with open('pso.csv', 'a') as f:\n",
    "        f.write(list_to_csline([time.strftime(\"%Y-%m-%d %H:%M:%S\"), activation, dropout, momentum, minibatch_size, n_epochs, learning_rate, optimizer, n_layers, units1, units2, units3, units4, units5, units6, training_r2, test_r2]))\n",
    "    \n",
    "    return test_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optunity optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'activation': { # tf.nn.X\n",
    "        'tanh': None,\n",
    "        'elu': None,\n",
    "        'relu': None\n",
    "    },\n",
    "    'dropout': [0, 1],\n",
    "    'momentum': [0.5, 1],\n",
    "    'log2_minibatch_size': [7, 13],\n",
    "    'n_epochs': [200, 400],\n",
    "    'minuslog10_learning_rate': [0, 5],\n",
    "    'optimizer': { # tf.train.X\n",
    "        'AdamOptimizer': None,\n",
    "        'GradientDescentOptimizer': None,\n",
    "        'RMSPropOptimizer': None\n",
    "    }\n",
    "}\n",
    "\n",
    "max_units = 16\n",
    "max_layers = 6\n",
    "# we start from 1\n",
    "search_space['n_layers'] = {\n",
    "    str(n_layers): {\n",
    "        'units'+str(layer): [1, max_units] \n",
    "        for layer in range(1, n_layers + 1) \n",
    "    } \n",
    "    for n_layers in range(1, max_layers + 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('pso.csv', 'w') as f:\n",
    "    f.write(list_to_csline(['datetime', 'activation', 'dropout', 'momentum', 'minibatch_size', 'n_epochs', 'learning_rate', 'optimizer', 'n_layers', 'units1', 'units2', 'units3', 'units4', 'units5', 'units6', 'training_r2', 'test_r2']))\n",
    "    \n",
    "result = optunity.maximize_structured(try_regres, search_space=search_space, num_evals=10240)\n",
    "\n",
    "with open('pso.result.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是否限于步数未调试三层+？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_units = 16\n",
    "max_layers = 6\n",
    "\n",
    "search_space_2 = {}\n",
    "# we start from 1\n",
    "search_space_2['n_layers'] = {\n",
    "    str(n_layers): {\n",
    "        'units'+str(layer): [1, max_units] \n",
    "        for layer in range(1, n_layers + 1) \n",
    "    } \n",
    "    for n_layers in range(1, max_layers + 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_regress_2(units1, units2=None, units3=None, units4=None, units5=None, units6=None, *args, **kwargs):\n",
    "    \n",
    "    L = [units1, units2, units3, units4, units5, units6]\n",
    "    f = lambda x: (x - 12.)**2. if x else 0\n",
    "    return sum(map(f, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = optunity.maximize_structured(try_regress_2, search_space=search_space_2, num_evals=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“变量越少越好”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "组1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = {\n",
    "    'n_layers': { str(n_layers): {\n",
    "        'units' + str(ilayer): [0, 16] for ilayer in range(n_layers)\n",
    "    } for n_layers in range(1, 6+1) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return (math.ceil(x) - 12.) ** 2. if x is not None else 0\n",
    "\n",
    "def try_regress_1(n_layers, units0=None, units1=None, units2=None, units3=None, units4=None, units5=None):\n",
    "    L = [units0, units1, units2, units3, units4, units5]\n",
    "    \n",
    "    return sum(map(f1, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for _ in range(20): # 20 trials average\n",
    "    result = optunity.maximize_structured(try_regress_1, search_space=search1, num_evals=50)\n",
    "    L.append(result[1].optimum)\n",
    "print np.mean(L), np.std(L) / np.sqrt(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完毕。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "组2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optunity = reload(optunity)\n",
    "optunity.api = reload(optunity.api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search2 = {\n",
    "    'n_layers': { str(n_layers): {\n",
    "        'units' + str(ilayer): {\n",
    "            str(i): None for i in range(1, 3+1)\n",
    "        } for ilayer in range(n_layers)\n",
    "    } for n_layers in range(1, 3+1) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned ON\n"
     ]
    }
   ],
   "source": [
    "pprint(search2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return (float(x) - 12.) ** 2. if x is not None else 0\n",
    "\n",
    "def try_regress_2(units0=None, units1=None, units2=None, units3=None, units4=None, units5=None, *args, **kwargs):\n",
    "    L = [units0, units1, units2, units3, units4, units5]\n",
    "    \n",
    "    return sum(map(f2, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.12 (default, Dec  4 2017, 14:50:18) \n",
      "Type \"copyright\", \"credits\" or \"license\" for more information.\n",
      "\n",
      "IPython 5.5.0 -- An enhanced Interactive Python.\n",
      "?         -> Introduction and overview of IPython's features.\n",
      "%quickref -> Quick reference.\n",
      "help      -> Python's own help system.\n",
      "object?   -> Details about 'object', use 'object??' for extra details.\n",
      "\n",
      "In [1]: call_dict\n",
      "Out[1]: \n",
      "{'args': {'': ['units0', 'units0', 'units1', 'units1', 'units2', 'units1'],\n",
      "  '2': [None, None, None, None, None],\n",
      "  '3': [None, None, None, None, None],\n",
      "  'n_layers': ['1', '3', '2', '1', '3', '2', '3', '2'],\n",
      "  'units0': ['2', '3', '1', '1', '3', '3', '2', '2'],\n",
      "  'units1': [None, '2', '1', None, '2', '1', '1', '2'],\n",
      "  'units2': [None, '2', None, None, '1', None, '1', None]},\n",
      " 'values': [100.0, 281.0, 242.0, 121.0, 302.0, 202.0, 342.0, 200.0]}\n",
      "\n",
      "In [2]: index\n",
      "Out[2]: 6\n",
      "\n",
      "In [3]: call_dict['args'].items()\n",
      "Out[3]: \n",
      "[('units1', [None, '2', '1', None, '2', '1', '1', '2']),\n",
      " ('units0', ['2', '3', '1', '1', '3', '3', '2', '2']),\n",
      " ('3', [None, None, None, None, None]),\n",
      " ('units2', [None, '2', None, None, '1', None, '1', None]),\n",
      " ('', ['units0', 'units0', 'units1', 'units1', 'units2', 'units1']),\n",
      " ('2', [None, None, None, None, None]),\n",
      " ('n_layers', ['1', '3', '2', '1', '3', '2', '3', '2'])]\n",
      "\n",
      "In [4]: \n",
      "\n",
      "In [4]: k\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/home/xzhang1/src/optunity/optunity/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k' is not defined\n",
      "\n",
      "In [5]: v\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/home/xzhang1/src/optunity/optunity/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v' is not defined\n",
      "\n",
      "In [6]: {k: v[index] for k, v in call_dict[\"args\"].items()}\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/home/xzhang1/src/optunity/optunity/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcall_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;32m/home/xzhang1/src/optunity/optunity/api.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((k, v))\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcall_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'index' is not defined\n",
      "\n",
      "In [7]: index\n",
      "Out[7]: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L = []\n",
    "for _ in range(20): # 20 trials average\n",
    "    result = optunity.maximize_structured(try_regress_2, search_space=search2, num_evals=10)\n",
    "    L.append(result[1].optimum)\n",
    "print np.mean(L), np.std(L) / np.sqrt(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "组3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    return (float(x) - 12.) ** 2. if x is not None else 0\n",
    "\n",
    "def try_regress_2(units0=None, units1=None, units2=None, units3=None, units4=None, units5=None):\n",
    "    L = [units0, units1, units2, units3, units4, units5]\n",
    "    \n",
    "    return sum(map(f2, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_units = 16\n",
    "max_layers = 6\n",
    "\n",
    "search_space_3 = {\n",
    "    'units' + str(ilayer): [-max_units, max_units + 1] for ilayer in range(max_layers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_regress_3(units0, units1, units2, units3, units4, units5):\n",
    "    \n",
    "    # 0及以下按0算\n",
    "    L = [units0, units1, units2, units3, units4, units5]\n",
    "    L = [int(round(_)) if int(round(_))>0 else 0 for _ in L]\n",
    "    \n",
    "    # 不允许中间空层\n",
    "    sgn_L = np.sign(L)\n",
    "    if (np.diff(sgn_L) > 0).any():\n",
    "        return -10\n",
    "    \n",
    "    # 原值\n",
    "    f = lambda x: (x - 12.)**2. if x!=0 else 0\n",
    "    return sum(map(f, L))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = optunity.maximize_structured(try_regress_3, search_space=search_space_3, num_evals=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我觉得可以i)跑particle swarm optimization，看趋势确定可以执行ii)greedy algorithm。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook curates a list of helper functions. When a project is archived, reusable functions go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats, scipy.interpolate, scipy.spatial\n",
    "\n",
    "# matplotlib\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# plotly\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# Machine learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "import sklearn.preprocessing, sklearn.base, sklearn.utils, sklearn.model_selection, sklearn.gaussian_process, sklearn.linear_model\n",
    "import optunity\n",
    "import statsmodels.nonparametric.smoothers_lowess\n",
    "\n",
    "# Various Python tricks and libraries\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import functools\n",
    "import operator\n",
    "import collections\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import dill as pickle\n",
    "import IPython\n",
    "import gc\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Parallel\n",
    "# import joblib\n",
    "# import multiprocessing\n",
    "import pathos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPipeline():\n",
    "    '''\n",
    "    Same as sklearn's pipeline except:\n",
    "    - does not require 2 inputs and outputs. \n",
    "      allows e.g. 3 inputs and outputs, as long as *args match.\n",
    "    - requires only fit() and transform()\n",
    "    - returns state list\n",
    "    - not too inefficient\n",
    "    \n",
    "    Supports *args but not **kwargs.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, ETs):\n",
    "        self.ETs = Estimators_Transformers = ETs\n",
    "      \n",
    "    def fit(self, *args):\n",
    "        '''Note: does not take keyworded input.'''\n",
    "        for ET in self.ETs:\n",
    "            args = ET.fit(*args).transform(*args)\n",
    "        return self\n",
    "\n",
    "    def transform(self, *args):\n",
    "        for ET in self.ETs:\n",
    "            args = ET.transform(*args)\n",
    "        return args\n",
    "    \n",
    "    def transforms(self, *args):\n",
    "        ET_s = []\n",
    "        for ET in self.ETs:\n",
    "            args = ET.transform(*args)\n",
    "            ET_s.append(args)\n",
    "        return ET_s\n",
    "    \n",
    "    def fit_transform(self, *args):\n",
    "        for ET in self.ETs:\n",
    "            args = ET.fit(*args).transform(*args)\n",
    "        return args\n",
    "    \n",
    "    def fit_transforms(self, *args):\n",
    "        ET_s = []\n",
    "        for ET in self.ETs:\n",
    "            args = ET.fit(*args).transform(*args)\n",
    "            ET_s.append(args)\n",
    "        return ET_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnStandardScaler():\n",
    "    '''\n",
    "    StandardScaler, but for RNN-style input, i.e. None * Irregular * Nfeatures.\n",
    "    \n",
    "    Unlike LabelEncoder, sometimes StandardScaler'ing the original DataFrame is not possible.\n",
    "    \n",
    "    Supports neither *args nor **kwargs.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, processes=20):\n",
    "        self.processes = processes\n",
    "        self.scaler = sklearn.preprocessing.StandardScaler()\n",
    "        \n",
    "    def fit(self, X):\n",
    "        assert isinstance(X[0][0][0], float)\n",
    "        \n",
    "        X_ = np.concatenate(X, axis=0)\n",
    "        assert not np.isnan(X_).any()\n",
    "        self.scaler.fit(X_)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        assert isinstance(X[0][0][0], float)\n",
    "        \n",
    "        if self.processes:\n",
    "            return pathos.multiprocessing.ProcessPool(nodes=self.processes).map(lambda _: list(self.scaler.transform(_)), X)\n",
    "        else:\n",
    "            return map(lambda _: list(self.scaler.transform(_)), X)\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualizing ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Iterator(df, *args, **kwargs):\n",
    "    '''\n",
    "    Wrapper around RnnIterator. Takes a Dataframe.\n",
    "    \n",
    "    I am a class.\n",
    "    '''\n",
    "    data = [df.iloc[:, :-1].values, df.iloc[:, -1].values.reshape(-1, 1)]\n",
    "    return RnnIterator(data, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnIterator(object):\n",
    "    '''\n",
    "    Takes [(None, ...), (None, ...), (None, ...)] format data.\n",
    "    Train-test split by train_index, test_index, train_split and/or test_split.\n",
    "    Make batches.\n",
    "    Executes n_epochs before raising StopIteration.\n",
    "    Progress bar.\n",
    "    \n",
    "    Note: \n",
    "    \"Train data\" is preferred to \"training data\".\n",
    "    \n",
    "    Variables:\n",
    "    train_data, test_data\n",
    "    n_minibatches, i_minibatch\n",
    "    n_epochs, @property i_epoch\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, data, minibatch_size, n_epochs, train_index=None, test_index=None, train_split=None, test_split=None, tqdm=True):\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        N = len(data[0])\n",
    "        all_index = range(N)\n",
    "        \n",
    "        # data shape consistency check\n",
    "        data = [np.float32(_) for _ in data]\n",
    "        assert len(np.unique([len(_) for _ in data])) == 1\n",
    "        assert data[-1].ndim >= 2 # y is (None, 1), not (None)\n",
    "        \n",
    "        # determine train_index and test_index\n",
    "        # given index\n",
    "        if train_index and test_index:\n",
    "            pass\n",
    "        elif train_index and not test_index:\n",
    "            test_index = list(set(all_index) - set(train_index))\n",
    "        elif test_index and not train_index:\n",
    "            train_index = list(set(all_index) - set(test_index))\n",
    "        # given split percentage\n",
    "        elif train_split and test_split:\n",
    "            train_index = np.random.choice(all_index, int(N * train_split), replace=False)\n",
    "            remaining_index = list(set(all_index) - set(train_index))\n",
    "            test_index = np.random.choice(remaining_index, int(N * test_split), replace=False)\n",
    "        elif train_split and not test_split:\n",
    "            train_index = np.random.choice(all_index, int(N * train_split), replace=False)\n",
    "            test_index = list(set(all_index) - set(train_index))\n",
    "        elif test_split and not train_split:\n",
    "            test_index = np.random.choice(all_index, int(N * test_split), replace=False)   \n",
    "            train_index = list(set(all_index) - set(test_index))\n",
    "        else:\n",
    "            raise Exception(\"Either specify index, or specify split.\") \n",
    "                            \n",
    "        # generate train_data\n",
    "        self.train_data = [_[train_index, ...] for _ in data]\n",
    "        self.test_data = [_[test_index, ...] for _ in data]\n",
    "\n",
    "        # minibatch counter\n",
    "        self.i_minibatch = 0\n",
    "        self.n_minibatches = n_epochs * len(self.train_data[0]) / minibatch_size\n",
    "        \n",
    "        if tqdm:\n",
    "            self.tqdm = tqdm_notebook(total=self.n_minibatches, leave=False)\n",
    "        \n",
    "    def minibatch(self):\n",
    "        if self.i_minibatch > self.n_minibatches:\n",
    "            self.i_minibatch = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.i_minibatch += 1\n",
    "            \n",
    "        if getattr(self, 'tqdm', None):\n",
    "            self.tqdm.update(1)\n",
    "        \n",
    "        index = np.random.choice(range(len(self.train_data[0])), self.minibatch_size, replace=False)\n",
    "        return [_[index, ...] for _ in self.train_data]\n",
    "    \n",
    "    @property\n",
    "    def i_epoch(self):\n",
    "        # the number of epochs\n",
    "        return float(self.i_minibatch) * self.n_epochs / self.n_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterYhatLive(object):\n",
    "    '''\n",
    "    On-the-run r2 monitoring.\n",
    "    \n",
    "    Plot (i_epoch, r2).\n",
    "    Plot (y, yhat).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, smoothen):\n",
    "        self.fig, (self.ax_decay, self.ax_corr) = plt.subplots(1, 2, figsize=(14, 4.2))\n",
    "        self.ax_corr.set_aspect('equal', adjustable='datalim')\n",
    "        self.i_epochs, self.line_decay_train, self.line_decay_test = [], [], []\n",
    "        self.smoothen = smoothen\n",
    "        \n",
    "    def update(self, i_epoch, y_train, yhat_train, y_test, yhat_test):\n",
    "        self.i_epochs.append(i_epoch)\n",
    "        self.line_decay_train.append(sklearn.metrics.r2_score(y_train, yhat_train))\n",
    "        self.line_decay_test.append(sklearn.metrics.r2_score(y_test, yhat_test))\n",
    "        # smoothen\n",
    "        if self.smoothen:\n",
    "            x = range(len(self.line_decay_train))\n",
    "            smoothline_decay_train = statsmodels.nonparametric.smoothers_lowess.lowess(self.line_decay_train, x, is_sorted=True, frac=0.25, it=1, return_sorted=False)\n",
    "            smoothline_decay_test = statsmodels.nonparametric.smoothers_lowess.lowess(self.line_decay_test, x, is_sorted=True, frac=0.25, it=1, return_sorted=False)\n",
    "        else:\n",
    "            smoothline_decay_train = self.line_decay_train\n",
    "            smoothline_decay_test = self.line_decay_test\n",
    "        #\n",
    "        label_train = '$r^2_{train}$=%.2f'%self.line_decay_train[-1]\n",
    "        label_test = '$r^2_{test}$=%.2f'%self.line_decay_test[-1]\n",
    "        #\n",
    "        self.ax_decay.clear()\n",
    "        self.ax_decay.plot(self.i_epochs, smoothline_decay_train, label=label_train)\n",
    "        self.ax_decay.plot(self.i_epochs, smoothline_decay_test, label=label_test)\n",
    "        self.ax_decay.legend(loc='best')\n",
    "        #\n",
    "        self.ax_corr.clear()\n",
    "        self.ax_corr.set_aspect('equal', adjustable='datalim')\n",
    "        self.ax_corr.scatter(y_train, yhat_train, color='green', s=1, alpha=0.2, label='train')\n",
    "        self.ax_corr.scatter(y_test, yhat_test, color='red', s=1, alpha=0.2, label='test')\n",
    "        self.ax_corr.legend(loc='best')\n",
    "        #\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaGraph(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bs = OrderedDict()\n",
    "        \n",
    "    def addv(self, b, label): # b is a BetterYhatLive instance\n",
    "        if label in self.bs:\n",
    "            raise Exception(\"label %s already in self.bs\" %label)\n",
    "        self.bs[label] = np.float32(zip(b.i_epochs, b.line_decay_train, b.line_decay_test))\n",
    "        \n",
    "#     def draw(self):\n",
    "#         traces = [\n",
    "#             go.Scatter(x = b[:, 1], y = b[:, 2], mode = 'markers', name = label)\n",
    "#             for label, b in l.bs.iteritems()\n",
    "#         ]\n",
    "#         py.iplot(traces, filename='threshold_errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBetterYhatLive(keras.callbacks.Callback):\n",
    "    '''\n",
    "    The Keras version of BetterYhatLive. Known as TqdmProgBar.\n",
    "    \n",
    "    features:\n",
    "    1. tqdm ETA bar\n",
    "    2. logs[field] plotted for each field in fields\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_epochs, fields): \n",
    "        self.n_epochs = n_epochs\n",
    "        self.fields = fields\n",
    "        \n",
    "        self.fields_history = dict((field, []) for field in fields)\n",
    "        self.fig, self.ax = plt.subplots(1, 1)\n",
    "        \n",
    "    def on_train_begin(self, logs):\n",
    "        self.pbar = tqdm_notebook(total=self.n_epochs, leave=False)\n",
    "        \n",
    "    def on_train_end(self, logs):\n",
    "        self.pbar.close()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs, log_interval = 40):\n",
    "        if epoch % log_interval == 0:\n",
    "            self.pbar.update(log_interval)\n",
    "\n",
    "            for field in self.fields:\n",
    "                self.fields_history[field].append(logs[field])\n",
    "\n",
    "            self.ax.clear()\n",
    "            for field in self.fields:\n",
    "                self.ax.plot(self.fields_history[field], label=\"%s=%.2f\" %(field, self.fields_history[field][-1]))\n",
    "            self.ax.legend(loc='best')\n",
    "            self.fig.canvas.draw()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(ytrue, ypred): # sklearn.metrics.r2_score in tensorflow. 1 output only. \n",
    "\n",
    "    ytrue_mean = tf.reduce_mean(ytrue, name=\"ytrue_mean\")\n",
    "    r2_score = tf.subtract(1., tf.truediv(tf.reduce_mean((ytrue - ypred) ** 2), tf.reduce_mean((ytrue - ytrue_mean) ** 2)), name=\"r2_score\")\n",
    "    return r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iterator(object):\n",
    "    '''\n",
    "    Replaced with a wrapper around RnnIterator.\n",
    "    \n",
    "    Allows manually setting aside a test set, or automatically randomly selecting one.\n",
    "    Makes batches from a dataframe, where first N-1 columns are features and last column is label.\n",
    "    Executes n_epochs before raising StopIteration.\n",
    "    Progress bar.\n",
    "    \n",
    "    Note:  \n",
    "    \"train data\" is preferred to \"training data\".\n",
    "    \"index\" refer to \"iloc\", not \"loc\" in dataframe.\n",
    "\n",
    "    Variables:\n",
    "    train_data, test_data\n",
    "    n_minibatches, i_minibatch\n",
    "    n_epochs, i_epoch\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, df, minibatch_size, n_epochs, train_index=None, test_index=None, train_split=None, test_split=None, tqdm=True):\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        N = len(df)\n",
    "        all_index = range(N)\n",
    "        \n",
    "        # determine train_index and test_index\n",
    "        # given index\n",
    "        if train_index and test_index:\n",
    "            pass\n",
    "        elif train_index and not test_index:\n",
    "            test_index = list(set(all_index) - set(train_index))\n",
    "        elif test_index and not train_index:\n",
    "            train_index = list(set(all_index) - set(test_index))\n",
    "        # given split percentage\n",
    "        elif train_split and test_split:\n",
    "            train_index = np.random.choice(all_index, int(N * train_split), replace=False)\n",
    "            remaining_index = list(set(all_index) - set(train_index))\n",
    "            test_index = np.random.choice(remaining_index, int(N * test_split), replace=False)\n",
    "        elif train_split and not test_split:\n",
    "            train_index = np.random.choice(all_index, int(N * train_split), replace=False)\n",
    "            test_index = list(set(all_index) - set(train_index))\n",
    "        elif test_split and not train_split:\n",
    "            test_index = np.random.choice(all_index, int(N * test_split), replace=False)   \n",
    "            train_index = list(set(all_index) - set(test_index))\n",
    "        else:\n",
    "            raise Exception(\"Either specify index, or specify split.\") \n",
    "                            \n",
    "        # generate train_df\n",
    "        self.train_df = df.iloc[train_index]\n",
    "        self.test_df = df.iloc[test_index]\n",
    "        self.df = df\n",
    "\n",
    "        # minibatch counter\n",
    "        self.i_minibatch = 0\n",
    "        self.n_minibatches = n_epochs * len(self.train_df) / minibatch_size\n",
    "        \n",
    "        if tqdm:\n",
    "            self.tqdm = tqdm_notebook(total=self.n_minibatches, leave=False)\n",
    "        \n",
    "    def minibatch(self):\n",
    "        if self.i_minibatch > self.n_minibatches:\n",
    "            self.i_minibatch = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.i_minibatch += 1\n",
    "            \n",
    "        if getattr(self, 'tqdm', None):\n",
    "            self.tqdm.update(1)\n",
    "        \n",
    "        index = np.random.choice(range(len(self.train_df)), self.minibatch_size, replace=False)\n",
    "        return self.train_df.iloc[index, :-1].values, self.train_df.iloc[index, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    @property\n",
    "    def train_data(self):\n",
    "        return self.train_df.iloc[:, :-1].values, self.train_df.iloc[:, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    @property\n",
    "    def test_data(self):\n",
    "        return self.test_df.iloc[:, :-1].values, self.test_df.iloc[:, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    @property\n",
    "    def i_epoch(self):\n",
    "        # the number of epochs\n",
    "        return float(self.i_minibatch) * self.n_epochs / self.n_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# iterable to list (low performance, please avoid!)\n",
    "\n",
    "def is_iterable(L):\n",
    "    return hasattr(L, '__iter__')\n",
    "\n",
    "def to_iterable(L):\n",
    "    if isinstance(L, (np.ndarray, np.generic)):\n",
    "        return L.tolist()\n",
    "    if isinstance(L, pd.DataFrame):\n",
    "        return L.values.tolist()\n",
    "    if is_iterable(L):\n",
    "        return recursive_map(lambda x:x, L)\n",
    "    raise ValueError\n",
    "    \n",
    "def flatten(L):\n",
    "    return reduce(operator.add, map(lambda l: flatten(l) if is_iterable(l) else [l], L))\n",
    "\n",
    "def recursive_map(func, L):\n",
    "    return map(lambda l: recursive_map(func, l) if is_iterable(l) else func(l), L)\n",
    "\n",
    "def get_index(item, L, index_unexpected=-1, random_unexpected=0.): # first occurence or -1\n",
    "    return index_unexpected if np.random.rand() < random_unexpected or item not in L else np.argmax(np.array(L)==item)\n",
    "\n",
    "def get_value(index, L, value_unexpected_index=None, index_unexpected=-1): # L[index] or None\n",
    "    return value_unexpected_index if index == index_unexpected else L[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low performance, please avoid!\n",
    "\n",
    "class LabelEncoder(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin): \n",
    "    # sklearn.preprocessing.LabelEncoder with irregular 2D array, unexpected index or value, and random -1 return.\n",
    "    \n",
    "    def __init__(self, index_unexpected=-1, random_unexpected=0., value_unexpected_index=None):\n",
    "        self.index_unexpected = index_unexpected\n",
    "        self.random_unexpected = random_unexpected\n",
    "        self.value_unexpected_index = value_unexpected_index\n",
    "        \n",
    "    def fit(self, y):\n",
    "        y_flattened = flatten(y)\n",
    "        self.classes_ = np.unique(y_flattened)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        func = functools.partial(get_index, \n",
    "                                 L=self.classes_, \n",
    "                                 index_unexpected=self.index_unexpected, \n",
    "                                 random_unexpected=self.random_unexpected)\n",
    "        return recursive_map(\n",
    "            func=lambda item: get_index(\n",
    "                item=item, \n",
    "                L=self.classes_, \n",
    "                index_unexpected=self.index_unexpected, \n",
    "                random_unexpected=self.random_unexpected),\n",
    "            L=y)\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        return recursive_map(\n",
    "            func=lambda index: get_value(\n",
    "                index=index, \n",
    "                L=self.classes_, \n",
    "                index_unexpected=self.index_unexpected, \n",
    "                random_unexpected=self.random_unexpected),\n",
    "            L=y)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats, scipy.interpolate, scipy.spatial\n",
    "\n",
    "# matplotlib\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# plotly\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Machine learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "import sklearn.preprocessing, sklearn.base, sklearn.utils, sklearn.model_selection, sklearn.gaussian_process, sklearn.linear_model\n",
    "import optunity\n",
    "import statsmodels.nonparametric.smoothers_lowess\n",
    "\n",
    "# Various Python tricks and libraries\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import functools\n",
    "import operator\n",
    "import collections\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import dill as pickle\n",
    "import IPython\n",
    "import gc\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Parallel\n",
    "import joblib\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minibatch(object):\n",
    "    '''\n",
    "    Makes batches from dataframes.\n",
    "    Executes n_epochs before raising StopIteration.\n",
    "    Allows manually setting aside a test set, or automatically randomly selecting one.\n",
    "    Progress bar.\n",
    "    Note: \"train set\" is preferred to \"training set\", e.g. train_index, X_train\n",
    "\n",
    "    Variables:\n",
    "    n_minibatches, i_minibatch\n",
    "    n_epochs, @property i_epoch\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, df, minibatch_size, n_epochs, train_index=None, test_index=None, train_split=None, test_split=None, tqdm=True):\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        N = len(df)\n",
    "        all_index = range(N)\n",
    "        \n",
    "        # determine train_index and test_index\n",
    "        # given index\n",
    "        if train_index and test_index:\n",
    "            pass\n",
    "        elif train_index and not test_index:\n",
    "            test_index = list(set(all_index) - set(train_index))\n",
    "        elif test_index and not train_index:\n",
    "            train_index = list(set(all_index) - set(test_index))\n",
    "        # given split percentage\n",
    "        elif train_split and test_split:\n",
    "            train_index = np.random.choice(all_index, int(N * train_split), replace=False)\n",
    "            remaining_index = list(set(all_index) - set(train_index))\n",
    "            test_index = np.random.choice(remaining_index, int(N * test_split), replace=False)\n",
    "        elif train_split and not test_split:\n",
    "            train_index = np.random.choice(all_index, int(N * train_split), replace=False)\n",
    "            test_index = list(set(all_index) - set(train_index))\n",
    "        elif test_split and not train_split:\n",
    "            test_index = np.random.choice(all_index, int(N * test_split), replace=False)   \n",
    "            train_index = list(set(all_index) - set(test_index))\n",
    "        else:\n",
    "            raise Exception(\"Either specify index, or specify split.\")\n",
    "                            \n",
    "        # generate train_df\n",
    "        self.train_df = df.iloc[train_index]\n",
    "        self.test_df = df.iloc[test_index]\n",
    "        self.df = df\n",
    "\n",
    "        # minibatch counter\n",
    "        self.i_minibatch = 0\n",
    "        self.n_minibatches = n_epochs * len(self.train_df) / minibatch_size\n",
    "        \n",
    "        if tqdm:\n",
    "            self.tqdm = tqdm_notebook(total=self.n_minibatches, leave=False)\n",
    "        \n",
    "    def minibatch(self):\n",
    "        if self.i_minibatch > self.n_minibatches:\n",
    "            self.i_minibatch = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.i_minibatch += 1\n",
    "            \n",
    "        if getattr(self, 'tqdm', None):\n",
    "            self.tqdm.update(1)\n",
    "        \n",
    "        index = np.random.choice(range(len(self.train_df)), self.minibatch_size, replace=False)\n",
    "        return self.train_df.iloc[index, :-1].values, self.train_df.iloc[index, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    def train_set(self):\n",
    "        return self.train_df.iloc[:, :-1].values, self.train_df.iloc[:, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    def test_set(self):\n",
    "        return self.test_df.iloc[:, :-1].values, self.test_df.iloc[:, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    @property\n",
    "    def i_epoch(self):\n",
    "        # the number of epochs\n",
    "        return float(self.i_minibatch) * self.n_epochs / self.n_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterYhatLive(object):\n",
    "    '''\n",
    "    Plot (i_epoch, r2).\n",
    "    Plot (y, yhat).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, smoothen):\n",
    "        self.fig, (self.ax_decay, self.ax_corr) = plt.subplots(1, 2, figsize=(14, 4.2))\n",
    "        self.ax_corr.set_aspect('equal', adjustable='datalim')\n",
    "        self.i_epochs, self.line_decay_train, self.line_decay_test = [], [], []\n",
    "        self.smoothen = smoothen\n",
    "        \n",
    "    def update(self, i_epoch, y_train, yhat_train, y_test, yhat_test):\n",
    "        self.i_epochs.append(i_epoch)\n",
    "        self.line_decay_train.append(sklearn.metrics.r2_score(y_train, yhat_train))\n",
    "        self.line_decay_test.append(sklearn.metrics.r2_score(y_test, yhat_test))\n",
    "        # smoothen\n",
    "        if self.smoothen:\n",
    "            x = range(len(self.line_decay_train))\n",
    "            smoothline_decay_train = statsmodels.nonparametric.smoothers_lowess.lowess(self.line_decay_train, x, is_sorted=True, frac=0.25, it=1, return_sorted=False)\n",
    "            smoothline_decay_test = statsmodels.nonparametric.smoothers_lowess.lowess(self.line_decay_test, x, is_sorted=True, frac=0.25, it=1, return_sorted=False)\n",
    "        else:\n",
    "            smoothline_decay_train = self.line_decay_train\n",
    "            smoothline_decay_test = self.line_decay_test\n",
    "        #\n",
    "        label_train = '$r^2_{train}$=%.2f'%self.line_decay_train[-1]\n",
    "        label_test = '$r^2_{test}$=%.2f'%self.line_decay_test[-1]\n",
    "        #\n",
    "        self.ax_decay.clear()\n",
    "        self.ax_decay.plot(self.i_epochs, smoothline_decay_train, label=label_train)\n",
    "        self.ax_decay.plot(self.i_epochs, smoothline_decay_test, label=label_test)\n",
    "        self.ax_decay.legend(loc='best')\n",
    "        #\n",
    "        self.ax_corr.clear()\n",
    "        self.ax_corr.set_aspect('equal', adjustable='datalim')\n",
    "        self.ax_corr.scatter(y_train, yhat_train, color='green', s=1, alpha=0.2, label='train')\n",
    "        self.ax_corr.scatter(y_test, yhat_test, color='red', s=1, alpha=0.2, label='test')\n",
    "        self.ax_corr.legend(loc='best')\n",
    "        #\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaGraph(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bs = OrderedDict()\n",
    "        \n",
    "    def add(self, b, label): # b is a BetterYhatLive instance\n",
    "        self.bs[label] = np.float32(zip(b.i_epochs, b.line_decay_train, b.line_decay_test))\n",
    "        \n",
    "    def draw(self):\n",
    "        traces = [\n",
    "            go.Scatter(\n",
    "                x = b[:, 1],\n",
    "                y = b[:, 2],\n",
    "                mode = 'markers',\n",
    "                name = label\n",
    "            )\n",
    "            for label, b in self.bs.iteritems()\n",
    "        ]\n",
    "        py.iplot(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate for plotting a dataframe. Do not attempt a function.\n",
    "# traces = [\n",
    "#     go.Scatter(x = df.index, y = df.loc[:, column], mode = 'markers+lines', name = column) \n",
    "#     for column in df.columns\n",
    "# ]\n",
    "# py.iplot(traces, filename='threshold_errors')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and tame the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读logfile，使之compatible with逻辑定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log():\n",
    "    jobs = pd.read_csv(filepath_or_buffer='ml_queue.nanaimo.20170101-20180704.log', sep='\\s+', header='infer', skiprows=[1], na_values=['UNLIMITED','Unknown']).dropna()\n",
    "\n",
    "    jobs.loc[:, 'nodes'] = pd.to_numeric(jobs.loc[:, 'NNodes'].copy(), errors='coerce', downcast='integer')\n",
    "    \n",
    "    # job_name not implemented\n",
    "\n",
    "    jobs.loc[:, 'Timelimit'] = jobs.loc[:, 'Timelimit'].copy().str.replace('-','day ')\n",
    "    jobs.loc[:, 'eta'] = pd.to_timedelta(jobs.loc[:, 'Timelimit'].copy(), errors='coerce')\n",
    "\n",
    "    jobs.loc[:, 'submit'] = pd.to_datetime(jobs.loc[:, 'Submit'].copy(), errors='coerce')\n",
    "    \n",
    "    jobs.loc[:, 'begin'] = pd.to_datetime(jobs.loc[:, 'Start'].copy(), errors='coerce')\n",
    "    \n",
    "    jobs.loc[:, 'end'] = pd.to_datetime(jobs.loc[:, 'End'].copy(), errors='coerce')\n",
    "    \n",
    "    jobs.loc[:, 'user'] = jobs.loc[:, 'User'].copy() # uid not implemented\n",
    "    \n",
    "    # subfile not implemented\n",
    "    \n",
    "    # scancelled not implemented\n",
    "    \n",
    "    jobs.loc[:, 'wait'] = (jobs.begin - jobs.submit).values / pd.Timedelta('1h')\n",
    "\n",
    "    jobs = jobs.dropna()\n",
    "    \n",
    "    jobs = jobs.drop(columns=['User', 'NNodes', 'Timelimit', 'Submit', 'Start', 'End'])\n",
    "    \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = read_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>nodes</th>\n",
       "      <th>eta</th>\n",
       "      <th>submit</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>user</th>\n",
       "      <th>wait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CANCELLED+</td>\n",
       "      <td>1</td>\n",
       "      <td>2 days</td>\n",
       "      <td>2017-08-01 09:42:07</td>\n",
       "      <td>2017-08-01 09:42:08</td>\n",
       "      <td>2017-08-02 23:03:56</td>\n",
       "      <td>nicola</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CANCELLED+</td>\n",
       "      <td>1</td>\n",
       "      <td>2 days</td>\n",
       "      <td>2017-08-01 09:43:03</td>\n",
       "      <td>2017-08-01 09:43:04</td>\n",
       "      <td>2017-08-02 23:03:57</td>\n",
       "      <td>nicola</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CANCELLED+</td>\n",
       "      <td>1</td>\n",
       "      <td>2 days</td>\n",
       "      <td>2017-08-01 09:43:51</td>\n",
       "      <td>2017-08-01 09:43:51</td>\n",
       "      <td>2017-08-02 23:03:59</td>\n",
       "      <td>nicola</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CANCELLED+</td>\n",
       "      <td>1</td>\n",
       "      <td>2 days</td>\n",
       "      <td>2017-08-01 09:44:35</td>\n",
       "      <td>2017-08-01 09:44:35</td>\n",
       "      <td>2017-08-02 23:04:01</td>\n",
       "      <td>nicola</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CANCELLED+</td>\n",
       "      <td>1</td>\n",
       "      <td>2 days</td>\n",
       "      <td>2017-08-01 09:45:28</td>\n",
       "      <td>2017-08-01 09:45:28</td>\n",
       "      <td>2017-08-02 23:04:04</td>\n",
       "      <td>nicola</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  nodes    eta              submit               begin  \\\n",
       "0  CANCELLED+      1 2 days 2017-08-01 09:42:07 2017-08-01 09:42:08   \n",
       "1  CANCELLED+      1 2 days 2017-08-01 09:43:03 2017-08-01 09:43:04   \n",
       "2  CANCELLED+      1 2 days 2017-08-01 09:43:51 2017-08-01 09:43:51   \n",
       "3  CANCELLED+      1 2 days 2017-08-01 09:44:35 2017-08-01 09:44:35   \n",
       "4  CANCELLED+      1 2 days 2017-08-01 09:45:28 2017-08-01 09:45:28   \n",
       "\n",
       "                  end    user      wait  \n",
       "0 2017-08-02 23:03:56  nicola  0.000278  \n",
       "1 2017-08-02 23:03:57  nicola  0.000278  \n",
       "2 2017-08-02 23:03:59  nicola  0.000000  \n",
       "3 2017-08-02 23:04:01  nicola  0.000000  \n",
       "4 2017-08-02 23:04:04  nicola  0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可以直接开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i实现为原始的index。注意删了一些job后，i不是连续的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "认为在提交的同时，发生的事情不知道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始100个job认为不可分析Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQ(i):\n",
    "    # S: in service. Q: wait queue.\n",
    "    \n",
    "    if i < 100:\n",
    "        raise Exception\n",
    "    \n",
    "    j = jobs.loc[i]\n",
    "    \n",
    "    S = jobs.loc[\n",
    "        np.logical_and.reduce([\n",
    "            jobs.begin < j.submit, \n",
    "            jobs.end >= j.submit\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    Q = jobs.loc[\n",
    "        np.logical_and.reduce([\n",
    "            jobs.submit < j.submit, \n",
    "            jobs.begin >= j.submit\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    return S, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_not_wait(i, C):\n",
    "    S, Q = SQ(i)\n",
    "    return S.nodes.sum() + Q.nodes.sum() + jobs.loc[i].nodes <= C\n",
    "\n",
    "def did_not_wait(i, threshold):\n",
    "    return jobs.loc[i].wait < threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer Threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array_threshold_errors = []\n",
    "C = 32\n",
    "\n",
    "for threshold in tqdm_notebook([0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1]):\n",
    "\n",
    "    valid_indices = [_ for _ in jobs.index if _>100]\n",
    "    \n",
    "    array_should_not_wait = multiprocessing.Pool(processes=20).map(\n",
    "        functools.partial(should_not_wait, C=C),\n",
    "        valid_indices\n",
    "    )\n",
    "    jobs.loc[valid_indices, 'should_not_wait'] = array_should_not_wait\n",
    "\n",
    "    jobs.loc[valid_indices, 'did_not_wait'] = jobs.loc[valid_indices, 'wait'] < threshold\n",
    "    \n",
    "    should_not_wait_but_waited = np.sum(np.int_(np.logical_and(\n",
    "        jobs.loc[valid_indices, 'should_not_wait'],\n",
    "        np.logical_not(jobs.loc[valid_indices, 'did_not_wait'])\n",
    "    )))\n",
    "    \n",
    "    should_wait_but_did_not = np.sum(np.int_(np.logical_and(\n",
    "        np.logical_not(jobs.loc[valid_indices, 'should_not_wait']),\n",
    "        jobs.loc[valid_indices, 'did_not_wait']\n",
    "    )))\n",
    "    \n",
    "    should_not_wait_did_not_wait = np.sum(np.int_(np.logical_and(\n",
    "        jobs.loc[valid_indices, 'should_not_wait'],\n",
    "        jobs.loc[valid_indices, 'did_not_wait']\n",
    "    )))\n",
    "    \n",
    "    array_threshold_errors.append([threshold, should_not_wait_but_waited, should_wait_but_did_not, should_not_wait_did_not_wait])\n",
    "    \n",
    "threshold_errors = pd.DataFrame(array_threshold_errors, columns=['threshold', 'should_not_wait_but_waited', 'should_wait_but_did_not', 'should_not_wait_did_not_wait'])\n",
    "threshold_errors = threshold_errors.set_index('threshold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~xzhang1/90.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = threshold_errors\n",
    "traces = [\n",
    "    go.Scatter(x = df.index, y = df.loc[:, column], mode = 'markers+lines', name = column) \n",
    "    for column in df.columns\n",
    "]\n",
    "py.iplot(traces, filename='ml_queue_0803.sec2.fig1')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ed46272b2a407f9064fa8c6c4fd29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "array_C_errors = []\n",
    "threshold = 0.0008\n",
    "\n",
    "for C in tnrange(28, 38):\n",
    "\n",
    "    valid_indices = [_ for _ in jobs.index if _>100]\n",
    "    \n",
    "    array_should_not_wait = multiprocessing.Pool(processes=20).map(\n",
    "        functools.partial(should_not_wait, C=C),\n",
    "        valid_indices\n",
    "    )\n",
    "    jobs.loc[valid_indices, 'should_not_wait'] = array_should_not_wait\n",
    "\n",
    "    jobs.loc[valid_indices, 'did_not_wait'] = jobs.loc[valid_indices, 'wait'] < threshold\n",
    "    \n",
    "    should_not_wait_but_waited = np.sum(np.int_(np.logical_and(\n",
    "        jobs.loc[valid_indices, 'should_not_wait'],\n",
    "        np.logical_not(jobs.loc[valid_indices, 'did_not_wait'])\n",
    "    )))\n",
    "    \n",
    "    should_wait_but_did_not = np.sum(np.int_(np.logical_and(\n",
    "        np.logical_not(jobs.loc[valid_indices, 'should_not_wait']),\n",
    "        jobs.loc[valid_indices, 'did_not_wait']\n",
    "    )))\n",
    "    \n",
    "    should_not_wait_did_not_wait = np.sum(np.int_(np.logical_and(\n",
    "        jobs.loc[valid_indices, 'should_not_wait'],\n",
    "        jobs.loc[valid_indices, 'did_not_wait']\n",
    "    )))\n",
    "    \n",
    "    array_C_errors.append([C, should_not_wait_but_waited, should_wait_but_did_not, should_not_wait_did_not_wait])\n",
    "    \n",
    "C_errors = pd.DataFrame(array_C_errors, columns=['C', 'should_not_wait_but_waited', 'should_wait_but_did_not', 'should_not_wait_did_not_wait'])\n",
    "C_errors = C_errors.set_index('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~xzhang1/92.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = C_errors\n",
    "traces = [\n",
    "    go.Scatter(x = df.index, y = df.loc[:, column], mode = 'markers+lines', name = column) \n",
    "    for column in df.columns\n",
    "]\n",
    "py.iplot(traces, filename='ml_queue_0803.sec2.fig2')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the two types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 32\n",
    "threshold = 0.0008\n",
    "exceptional_cases = [193, 128]\n",
    "\n",
    "for i in tqdm_notebook(valid_indices):\n",
    "    \n",
    "    if i in exceptional_cases:\n",
    "        continue\n",
    "    \n",
    "    if should_not_wait(i, C) != did_not_wait(i, threshold):\n",
    "        S, Q = SQ(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_not_wait(i, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "did_not_wait(i, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs.loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.nodes.sum() + Q.nodes.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I've seen above, most exceptions are confounding - in particular not scancelled - so this is good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log():\n",
    "    jobs = pd.read_csv(filepath_or_buffer='ml_queue.nanaimo.20170101-20180704.log', sep='\\s+', header='infer', skiprows=[1], na_values=['UNLIMITED','Unknown']).dropna()\n",
    "\n",
    "    jobs.loc[:, 'Submit'] = pd.to_datetime(jobs.loc[:, 'Submit'].copy(), errors='coerce')\n",
    "    jobs.loc[:, 'Start'] = pd.to_datetime(jobs.loc[:, 'Start'].copy(), errors='coerce')\n",
    "    jobs.loc[:, 'End'] = pd.to_datetime(jobs.loc[:, 'End'].copy(), errors='coerce')\n",
    "\n",
    "    jobs.loc[:, 'NNodes'] = pd.to_numeric(jobs.loc[:, 'NNodes'].copy(), errors='coerce', downcast='integer')\n",
    "\n",
    "    jobs.loc[:, 'Timelimit'] = jobs.loc[:, 'Timelimit'].copy().str.replace('-','day ')\n",
    "    jobs.loc[:, 'Timelimit'] = pd.to_timedelta(jobs.loc[:, 'Timelimit'].copy(), errors='coerce')\n",
    "    \n",
    "    jobs.loc[:, 'Waited'] = (jobs.Start - jobs.Submit).values / pd.Timedelta('1h')\n",
    "\n",
    "    jobs = jobs.dropna()\n",
    "    \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = read_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample(tuple_):\n",
    "    \n",
    "    jobs, index, row = tuple_\n",
    "    \n",
    "    tj = thisjob = row\n",
    "    now = tj.Submit\n",
    "    \n",
    "    rj = relatedjobs = jobs.loc[np.logical_and.reduce([jobs.index != index, jobs.Submit <= now, jobs.End > now])] # excludes thisjob, includes jobs submitted simultaneously but ranked earlier\n",
    "    \n",
    "    # 无关人等滚开\n",
    "    if now < jobs.End.min() or index < 100:\n",
    "        return None\n",
    "        \n",
    "    # 零、有空位，就不用等。\n",
    "    ## 不必掐头，这样结果看起来好些\n",
    "    # if rj.NNodes.sum() + tj.NNodes <= 36:\n",
    "    #     # 几乎总是tj.Waited < 0.005\n",
    "    #     return None\n",
    "        \n",
    "    # 一、各种descriptor\n",
    "    analysis = pd.DataFrame([], index=[index])\n",
    "    \n",
    "    analysis.loc[:, 'NNodes'] = tj.NNodes\n",
    "    analysis.loc[:, 'ETA']    = tj.Timelimit\n",
    "    \n",
    "    analysis.loc[:, 'Total_squeue_nnodes'] = rj.NNodes.sum()\n",
    "    \n",
    "    analysis.loc[:, 'Total_squeue_eta']    = rj.Timelimit.sum()\n",
    "    \n",
    "    analysis.loc[:, 'Self_job_in_squeue_percentage'] = 0 if len(rj)==0 else float(np.sum(rj.User == tj.User)) / len(rj)\n",
    "    \n",
    "    analysis.loc[:, 'Waited'] = tj.Waited\n",
    "    \n",
    "    return analysis\n",
    "    \n",
    "    \n",
    "L = multiprocessing.Pool(processes=20).map(sample, [(jobs, index, row.copy()) for index, row in tqdm(jobs.iterrows(), total=len(jobs))])\n",
    "analyses = pd.concat(L) # None is automatically ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去尾\n",
    "analyses = analyses[analyses.Waited < analyses.Waited.quantile(q=0.97)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalize_analyses = pd.DataFrame(\n",
    "    sklearn.preprocessing.scale(\n",
    "        analyses.apply(pd.to_numeric).values\n",
    "    ), \n",
    "    index=analyses.index, \n",
    "    columns=analyses.columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minibatch_size = 256\n",
    "n_epochs = 256\n",
    "ns_units = [25, 25]\n",
    "lr = 1E-3\n",
    "\n",
    "# Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(name=\"X\", dtype=tf.float32, shape=[None, 5])\n",
    "\n",
    "h = X\n",
    "for n_units in ns_units:\n",
    "    h = tf.layers.dense(h, units=n_units, activation=tf.nn.elu)\n",
    "h = tf.layers.dense(h, units=1, activation=None)\n",
    "yhat = h\n",
    "\n",
    "y = tf.placeholder(name=\"y\", dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(yhat - y), keepdims=False)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Session\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "m = Minibatch(normalize_analyses, minibatch_size, n_epochs, train_index=range(10000), test_index=range(10000, 20000))\n",
    "b = BetterYhatLive(smoothen=False)\n",
    "while True:\n",
    "    try:\n",
    "        _X, _y = m.minibatch()\n",
    "        sess.run(train_op, feed_dict = {X: _X, y: _y})\n",
    "        \n",
    "        if m.i_minibatch % 20 == 0:\n",
    "            X_train, y_train = m.train_set()\n",
    "            yhat_train = sess.run(yhat, feed_dict = {X: X_train})\n",
    "            X_test, y_test = m.test_set()\n",
    "            yhat_test = sess.run(yhat, feed_dict = {X: X_test}) \n",
    "            b.update(m.i_epoch, y_train, yhat_train, y_test, yhat_test)\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l = LambdaGraph()\n",
    "l.add(b, label='25-25 $\\\\mu$')\n",
    "l.add(b, label='25-25 $\\\\xi$')\n",
    "l.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 36\n",
    "\n",
    "valid_indices = [_ for _ in jobs.index if _>100]\n",
    "\n",
    "# for i in tqdm_notebook(valid_indices):\n",
    "#     jobs.loc[i, 'should_not_wait'] = should_not_wait(i, C)\n",
    "#     jobs.loc[i, 'did_not_wait'] = did_not_wait(i)\n",
    "\n",
    "array_should_not_wait = multiprocessing.Pool(processes=20).map(\n",
    "    functools.partial(should_not_wait, C=C),\n",
    "    valid_indices\n",
    ")\n",
    "jobs.loc[valid_indices, 'should_not_wait'] = array_should_not_wait\n",
    "\n",
    "jobs.loc[valid_indices, 'did_not_wait'] = jobs.loc[valid_indices, 'wait'] < 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.loc[101:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(jobs.loc[valid_indices, 'should_not_wait'] & ~jobs.loc[valid_indices, 'did_not_wait']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~jobs.loc[valid_indices, 'should_not_wait'] & jobs.loc[valid_indices, 'did_not_wait']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(jobs.loc[valid_indices, 'should_not_wait'] & jobs.loc[valid_indices, 'did_not_wait']).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

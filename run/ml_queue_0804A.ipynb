{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats, scipy.interpolate, scipy.spatial\n",
    "\n",
    "# matplotlib\n",
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# plotly\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Machine learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "import sklearn.preprocessing, sklearn.base, sklearn.utils, sklearn.model_selection, sklearn.gaussian_process, sklearn.linear_model\n",
    "import optunity\n",
    "import statsmodels.nonparametric.smoothers_lowess\n",
    "\n",
    "# Various Python tricks and libraries\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import functools\n",
    "import operator\n",
    "import collections\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import dill as pickle\n",
    "import IPython\n",
    "import gc\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Parallel\n",
    "import joblib\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minibatch(object):\n",
    "    '''\n",
    "    Makes batches from dataframes.\n",
    "    Executes n_epochs before raising StopIteration.\n",
    "    Allows manually setting aside a test set, or automatically randomly selecting one.\n",
    "    Progress bar.\n",
    "    \n",
    "    Note: \n",
    "    \"train set\" is preferred to \"training set\", e.g. train_index, X_train\n",
    "    \"idnex\" refer to \"iloc\", not \"loc\" in dataframe.\n",
    "\n",
    "    Variables:\n",
    "    n_minibatches, i_minibatch\n",
    "    n_epochs, @property i_epoch\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, df, minibatch_size, n_epochs, train_index=None, test_index=None, train_split=None, test_split=None, tqdm=True):\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        N = len(df)\n",
    "        all_index = range(N)\n",
    "        \n",
    "        # determine train_index and test_index\n",
    "        # given index\n",
    "        if train_index and test_index:\n",
    "            pass\n",
    "        elif train_index and not test_index:\n",
    "            test_index = list(set(all_index) - set(train_index))\n",
    "        elif test_index and not train_index:\n",
    "            train_index = list(set(all_index) - set(test_index))\n",
    "        # given split percentage\n",
    "        elif train_split and test_split:\n",
    "            train_index = np.random.choice(all_index, int(N * train_split), replace=False)\n",
    "            remaining_index = list(set(all_index) - set(train_index))\n",
    "            test_index = np.random.choice(remaining_index, int(N * test_split), replace=False)\n",
    "        elif train_split and not test_split:\n",
    "            train_index = np.random.choice(all_index, int(N * train_split), replace=False)\n",
    "            test_index = list(set(all_index) - set(train_index))\n",
    "        elif test_split and not train_split:\n",
    "            test_index = np.random.choice(all_index, int(N * test_split), replace=False)   \n",
    "            train_index = list(set(all_index) - set(test_index))\n",
    "        else:\n",
    "            raise Exception(\"Either specify index, or specify split.\")\n",
    "                            \n",
    "        # generate train_df\n",
    "        self.train_df = df.iloc[train_index]\n",
    "        self.test_df = df.iloc[test_index]\n",
    "        self.df = df\n",
    "\n",
    "        # minibatch counter\n",
    "        self.i_minibatch = 0\n",
    "        self.n_minibatches = n_epochs * len(self.train_df) / minibatch_size\n",
    "        \n",
    "        if tqdm:\n",
    "            self.tqdm = tqdm_notebook(total=self.n_minibatches, leave=False)\n",
    "        \n",
    "    def minibatch(self):\n",
    "        if self.i_minibatch > self.n_minibatches:\n",
    "            self.i_minibatch = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.i_minibatch += 1\n",
    "            \n",
    "        if getattr(self, 'tqdm', None):\n",
    "            self.tqdm.update(1)\n",
    "        \n",
    "        index = np.random.choice(range(len(self.train_df)), self.minibatch_size, replace=False)\n",
    "        return self.train_df.iloc[index, :-1].values, self.train_df.iloc[index, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    def train_set(self):\n",
    "        return self.train_df.iloc[:, :-1].values, self.train_df.iloc[:, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    def test_set(self):\n",
    "        return self.test_df.iloc[:, :-1].values, self.test_df.iloc[:, -1].values.reshape(-1, 1)\n",
    "    \n",
    "    @property\n",
    "    def i_epoch(self):\n",
    "        # the number of epochs\n",
    "        return float(self.i_minibatch) * self.n_epochs / self.n_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterYhatLive(object):\n",
    "    '''\n",
    "    Plot (i_epoch, r2).\n",
    "    Plot (y, yhat).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, smoothen):\n",
    "        self.fig, (self.ax_decay, self.ax_corr) = plt.subplots(1, 2, figsize=(14, 4.2))\n",
    "        self.ax_corr.set_aspect('equal', adjustable='datalim')\n",
    "        self.i_epochs, self.line_decay_train, self.line_decay_test = [], [], []\n",
    "        self.smoothen = smoothen\n",
    "        \n",
    "    def update(self, i_epoch, y_train, yhat_train, y_test, yhat_test):\n",
    "        self.i_epochs.append(i_epoch)\n",
    "        self.line_decay_train.append(sklearn.metrics.r2_score(y_train, yhat_train))\n",
    "        self.line_decay_test.append(sklearn.metrics.r2_score(y_test, yhat_test))\n",
    "        # smoothen\n",
    "        if self.smoothen:\n",
    "            x = range(len(self.line_decay_train))\n",
    "            smoothline_decay_train = statsmodels.nonparametric.smoothers_lowess.lowess(self.line_decay_train, x, is_sorted=True, frac=0.25, it=1, return_sorted=False)\n",
    "            smoothline_decay_test = statsmodels.nonparametric.smoothers_lowess.lowess(self.line_decay_test, x, is_sorted=True, frac=0.25, it=1, return_sorted=False)\n",
    "        else:\n",
    "            smoothline_decay_train = self.line_decay_train\n",
    "            smoothline_decay_test = self.line_decay_test\n",
    "        #\n",
    "        label_train = '$r^2_{train}$=%.2f'%self.line_decay_train[-1]\n",
    "        label_test = '$r^2_{test}$=%.2f'%self.line_decay_test[-1]\n",
    "        #\n",
    "        self.ax_decay.clear()\n",
    "        self.ax_decay.plot(self.i_epochs, smoothline_decay_train, label=label_train)\n",
    "        self.ax_decay.plot(self.i_epochs, smoothline_decay_test, label=label_test)\n",
    "        self.ax_decay.legend(loc='best')\n",
    "        #\n",
    "        self.ax_corr.clear()\n",
    "        self.ax_corr.set_aspect('equal', adjustable='datalim')\n",
    "        self.ax_corr.scatter(y_train, yhat_train, color='green', s=1, alpha=0.2, label='train')\n",
    "        self.ax_corr.scatter(y_test, yhat_test, color='red', s=1, alpha=0.2, label='test')\n",
    "        self.ax_corr.legend(loc='best')\n",
    "        #\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaGraph(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bs = OrderedDict()\n",
    "        \n",
    "    def add(self, b, label): # b is a BetterYhatLive instance\n",
    "        self.bs[label] = np.float32(zip(b.i_epochs, b.line_decay_train, b.line_decay_test))\n",
    "        \n",
    "    def draw(self):\n",
    "        traces = [\n",
    "            go.Scatter(\n",
    "                x = b[:, 1],\n",
    "                y = b[:, 2],\n",
    "                mode = 'markers',\n",
    "                name = label\n",
    "            )\n",
    "            for label, b in self.bs.iteritems()\n",
    "        ]\n",
    "        py.iplot(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate for plotting a dataframe. Do not attempt a function.\n",
    "data = [\n",
    "    go.Scatter(x = df.index, y = df.loc[:, column], mode = 'markers+lines', name = column) \n",
    "    for column in df.columns\n",
    "]\n",
    "py.iplot(data, filename='threshold_errors')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and tame the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读logfile，使之compatible with逻辑定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log():\n",
    "    jobs = pd.read_csv(filepath_or_buffer='ml_queue.nanaimo.20170101-20180704.log', sep='\\s+', header='infer', skiprows=[1], na_values=['UNLIMITED','Unknown']).dropna()\n",
    "\n",
    "    jobs.loc[:, 'nodes'] = pd.to_numeric(jobs.loc[:, 'NNodes'].copy(), errors='coerce', downcast='integer')\n",
    "    \n",
    "    # job_name not implemented\n",
    "\n",
    "    jobs.loc[:, 'Timelimit'] = jobs.loc[:, 'Timelimit'].copy().str.replace('-','day ')\n",
    "    jobs.loc[:, 'eta'] = pd.to_timedelta(jobs.loc[:, 'Timelimit'].copy(), errors='coerce') / pd.Timedelta('1day')\n",
    "\n",
    "    jobs.loc[:, 'submit'] = pd.to_datetime(jobs.loc[:, 'Submit'].copy(), errors='coerce')\n",
    "    \n",
    "    jobs.loc[:, 'begin'] = pd.to_datetime(jobs.loc[:, 'Start'].copy(), errors='coerce')\n",
    "    \n",
    "    jobs.loc[:, 'end'] = pd.to_datetime(jobs.loc[:, 'End'].copy(), errors='coerce')\n",
    "    \n",
    "    jobs.loc[:, 'user'] = jobs.loc[:, 'User'].copy() # uid not implemented\n",
    "    \n",
    "    # subfile not implemented\n",
    "    \n",
    "    jobs.loc[:, 'state'] = jobs.loc[:, 'State'].copy() # scancelled not implemented\n",
    "    \n",
    "    jobs.loc[:, 'wait'] = (jobs.begin - jobs.submit).values / pd.Timedelta('1h')\n",
    "\n",
    "    jobs = jobs.dropna()\n",
    "    \n",
    "    jobs = jobs.drop(columns=['User', 'NNodes', 'Timelimit', 'Submit', 'Start', 'End', 'State'])\n",
    "    \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = read_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备去掉直接开始，准备去掉CANCELLED+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SQ(i):\n",
    "    # S: in service. Q: wait queue.\n",
    "    \n",
    "    if i < 100:\n",
    "        raise Exception\n",
    "    \n",
    "    j = jobs.loc[i]\n",
    "    \n",
    "    S = jobs.loc[\n",
    "        np.logical_and.reduce([\n",
    "            jobs.begin < j.submit, \n",
    "            jobs.end >= j.submit\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    Q = jobs.loc[\n",
    "        np.logical_and.reduce([\n",
    "            jobs.submit < j.submit, \n",
    "            jobs.begin >= j.submit\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    return S, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_not_wait(i, C):\n",
    "    S, Q = SQ(i)\n",
    "    return S.nodes.sum() + Q.nodes.sum() + jobs.loc[i].nodes <= C\n",
    "\n",
    "C = 32\n",
    "\n",
    "valid_indices = [_ for _ in jobs.index if _>100]\n",
    "\n",
    "array_should_not_wait = multiprocessing.Pool(processes=20).map(\n",
    "    functools.partial(should_not_wait, C=C),\n",
    "    valid_indices\n",
    ")\n",
    "jobs.loc[valid_indices, 'should_not_wait'] = array_should_not_wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(i):\n",
    "    \n",
    "    if i < 100:\n",
    "        raise Exception\n",
    "        \n",
    "    if jobs.loc[i, 'should_not_wait']:\n",
    "        return None\n",
    "    \n",
    "    if jobs.loc[i, 'state'] == 'CANCELLED+':\n",
    "        return None\n",
    "    \n",
    "    S, Q = SQ(i)\n",
    "\n",
    "    return [\n",
    "        i,\n",
    "        jobs.loc[i, 'nodes'],\n",
    "        S.nodes.sum(),\n",
    "        Q.nodes.sum(),\n",
    "        jobs.loc[i, 'eta'],\n",
    "        S.eta.sum(),\n",
    "        Q.eta.sum(),\n",
    "        jobs.loc[i, 'wait']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = [_ for _ in jobs.index if _>100]\n",
    "\n",
    "array_samples = multiprocessing.Pool(processes=20).map(sample, valid_indices)\n",
    "\n",
    "array_samples = [_ for _ in array_samples if _ is not None]\n",
    "\n",
    "samples = pd.DataFrame(array_samples, columns=['i', 'nodes_i', 'sum_Si nodes_j', 'sum_Qi nodes_j', 'eta_i', 'sum_Si eta_j', 'sum_Qi eta_j', 'wait_i'])\n",
    "samples = samples.set_index('i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 假设检验：$\\hat{\\text{wait}_i}$与$\\text{nodes}_i, \\sum_{j\\in S/Q_i}\\text{nodes}_j, \\text{eta}_i, \\sum_{j\\in S/Q_i}\\text{eta}_j$线性相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13160"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_r2s = []\n",
    "\n",
    "for lb in np.arange(1000, 13000, 500):\n",
    "    for ub in np.arange(lb + 1000, 13000, 500):\n",
    "        \n",
    "        m = Minibatch(samples, minibatch_size=32, n_epochs=1, train_index=range(lb), test_index=range(lb, ub), tqdm=False)\n",
    "\n",
    "        regr = sklearn.linear_model.LinearRegression()\n",
    "        X_train, y_train = m.train_set()\n",
    "        regr.fit(X_train, y_train)\n",
    "        r2_train = regr.score(X_train, y_train)\n",
    "\n",
    "        X_test, y_test = m.test_set()\n",
    "        r2_test = regr.score(X_test, y_test)\n",
    "        \n",
    "        array_r2s.append([lb, ub, r2_train, r2_test])\n",
    "        \n",
    "r2s = pd.DataFrame(array_r2s, columns=['train_bound', 'test_bound', 'r2_train', 'r2_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~xzhang1/94.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = r2s\n",
    "\n",
    "data = [\n",
    "    go.Scatter3d(x = df.train_bound, y = df.test_bound, z=df.r2_train, mode='markers', name = '$r2_\\\\text{train}$'),\n",
    "    go.Scatter3d(x = df.train_bound, y = df.test_bound, z=df.r2_test, mode='markers', name = '$r2_\\\\text{test}$') \n",
    "]\n",
    "py.iplot(data, filename='ml_queue_0804A.sec4.fig1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
